{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TP 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "N_FFT = 400\n",
        "HOP_LENGTH = 160\n",
        "N_MELS = 80\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MFCC Extraction (Partie 1 - MLP pur)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_mfcc(audio_path, sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mfcc=13):\n",
        "    \"\"\"\n",
        "    Extrait les coefficients MFCC d'un fichier audio.\n",
        "    \"\"\"\n",
        "\n",
        "    import soundfile as sf\n",
        "\n",
        "    waveform_np, sr = sf.read(audio_path)\n",
        "\n",
        "    waveform = torch.from_numpy(waveform_np).float()\n",
        "    if waveform.dim() == 1:\n",
        "        waveform = waveform.unsqueeze(0)\n",
        "    else:\n",
        "        waveform = waveform.transpose(0, 1)\n",
        "    \n",
        "    if sr != sample_rate:\n",
        "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    mfcc_transform = torchaudio.transforms.MFCC(\n",
        "        sample_rate=sample_rate,\n",
        "        n_mfcc=n_mfcc,\n",
        "        melkwargs={\n",
        "            'n_fft': n_fft,\n",
        "            'hop_length': hop_length,\n",
        "            'n_mels': 128\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    mfcc = mfcc_transform(waveform)\n",
        "    mfcc = mfcc.squeeze(0).transpose(0, 1)\n",
        "    \n",
        "    mean = mfcc.mean(dim=0, keepdim=True)\n",
        "    std = mfcc.std(dim=0, keepdim=True)\n",
        "    mfcc = (mfcc - mean) / (std + 1e-8)\n",
        "    \n",
        "    return mfcc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. MelSpectrogram Extraction (Partie 2 - CNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_melspectrogram(audio_path, sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS):\n",
        "    import soundfile as sf\n",
        "\n",
        "    waveform_np, sr = sf.read(audio_path)\n",
        "    waveform = torch.from_numpy(waveform_np).float()\n",
        "    if waveform.dim() == 1:\n",
        "        waveform = waveform.unsqueeze(0)\n",
        "    else:\n",
        "        waveform = waveform.transpose(0, 1)\n",
        "    \n",
        "    if sr != sample_rate:\n",
        "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
        "        waveform = resampler(waveform)\n",
        "    \n",
        "    melspectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels\n",
        "    )\n",
        "    \n",
        "    melspectrogram = melspectrogram_transform(waveform)\n",
        "    melspectrogram = melspectrogram.squeeze(0).transpose(0, 1)\n",
        "    \n",
        "    melspectrogram = torch.log10(melspectrogram + 1e-8)\n",
        "    \n",
        "    mean = melspectrogram.mean(dim=0, keepdim=True)\n",
        "    std = melspectrogram.std(dim=0, keepdim=True)\n",
        "    melspectrogram = (melspectrogram - mean) / (std + 1e-8)\n",
        "    \n",
        "    return melspectrogram\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Character Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharEncoder:\n",
        "    def __init__(self):\n",
        "        self.char_to_idx = {'<blank>': 0}\n",
        "        self.idx_to_char = {0: '<blank>'}\n",
        "        self.next_idx = 1\n",
        "    \n",
        "    def build_vocab(self, texts):\n",
        "        for text in texts:\n",
        "            for char in text:\n",
        "                if char not in self.char_to_idx:\n",
        "                    self.char_to_idx[char] = self.next_idx\n",
        "                    self.idx_to_char[self.next_idx] = char\n",
        "                    self.next_idx += 1\n",
        "    \n",
        "    def encode(self, text):\n",
        "        return [self.char_to_idx.get(char, 0) for char in text]\n",
        "    \n",
        "    def decode(self, indices):\n",
        "        return ''.join([self.idx_to_char.get(idx, '') for idx in indices if idx != 0])\n",
        "    \n",
        "    def get_vocab_size(self):\n",
        "        return len(self.char_to_idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Partie 1 - Architecture MLP pure (sans CNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleMLPASR(nn.Module):\n",
        "    \"\"\"\n",
        "    Architecture MLP simple pour ASR avec MFCC et CTC.\n",
        "    Pas de convolutions, juste des couches fully-connected.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        vocab_size,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT,\n",
        "    ):\n",
        "        super(SimpleMLPASR, self).__init__()\n",
        "        \n",
        "        mlp_layers = []\n",
        "        \n",
        "        mlp_layers.append(nn.Linear(input_size, hidden_size))\n",
        "        mlp_layers.append(nn.ReLU())\n",
        "        mlp_layers.append(nn.Dropout(dropout))\n",
        "        \n",
        "        for _ in range(num_layers - 1):\n",
        "            mlp_layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "            mlp_layers.append(nn.ReLU())\n",
        "            mlp_layers.append(nn.Dropout(dropout))\n",
        "        \n",
        "        mlp_layers.append(nn.Linear(hidden_size, vocab_size))\n",
        "        self.mlp = nn.Sequential(*mlp_layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, features = x.size()\n",
        "        x = x.contiguous().view(-1, features)\n",
        "        x = self.mlp(x)\n",
        "        x = x.view(batch_size, seq_len, -1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Partie 2 - Architecture CNN + MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNMLPASR(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        vocab_size,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT,\n",
        "        conv_channels=[64, 128, 256],\n",
        "        kernel_sizes=[3, 3, 3],\n",
        "    ):\n",
        "        super(CNNMLPASR, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        in_channels = input_size\n",
        "\n",
        "        for out_channels, kernel_size in zip(conv_channels, kernel_sizes):\n",
        "            self.conv_layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(\n",
        "                        in_channels, out_channels, kernel_size, padding=kernel_size // 2\n",
        "                    ),\n",
        "                    nn.BatchNorm1d(out_channels),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(dropout),\n",
        "                )\n",
        "            )\n",
        "            in_channels = out_channels\n",
        "\n",
        "        conv_output_size = conv_channels[-1]\n",
        "        mlp_layers = []\n",
        "        mlp_layers.append(nn.Linear(conv_output_size, hidden_size))\n",
        "        mlp_layers.append(nn.ReLU())\n",
        "        mlp_layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        for _ in range(num_layers - 1):\n",
        "            mlp_layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "            mlp_layers.append(nn.ReLU())\n",
        "            mlp_layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        mlp_layers.append(nn.Linear(hidden_size, vocab_size))\n",
        "        self.mlp = nn.Sequential(*mlp_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size, seq_len, features = x.size()\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "        batch_size, seq_len, features = x.size()\n",
        "        x = x.contiguous().view(-1, features)\n",
        "        x = self.mlp(x)\n",
        "        x = x.view(batch_size, seq_len, -1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Dataset LibriSpeech (Alternative au dataset français)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LibriSpeechDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, librispeech_dir, encoder, subset='train-clean-100', sample_rate=SAMPLE_RATE, n_mfcc=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            librispeech_dir: Chemin vers le dossier LibriSpeech (ex: 'data/LibriSpeech')\n",
        "            encoder: CharEncoder pour encoder les transcriptions\n",
        "            subset: 'train-clean-100', 'dev-clean', 'test-clean', etc.\n",
        "            sample_rate: Taux d'échantillonnage\n",
        "            n_mfcc: Si None, utilise MelSpectrogram, sinon utilise MFCC avec n_mfcc coefficients\n",
        "        \"\"\"\n",
        "        self.librispeech_dir = Path(librispeech_dir)\n",
        "        self.subset_dir = self.librispeech_dir / subset\n",
        "        self.encoder = encoder\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.use_mfcc = n_mfcc is not None\n",
        "        \n",
        "        self.data = []\n",
        "        \n",
        "        if not self.subset_dir.exists():\n",
        "            print(f\" Attention: {self.subset_dir} n'existe pas!\")\n",
        "            print(f\"   Téléchargez LibriSpeech depuis https://www.openslr.org/12\")\n",
        "            print(f\"   Et extrayez-le dans {librispeech_dir}\")\n",
        "            return\n",
        "\n",
        "        for speaker_dir in sorted(self.subset_dir.iterdir()):\n",
        "            if not speaker_dir.is_dir():\n",
        "                continue\n",
        "            speaker_id = speaker_dir.name\n",
        "            for book_dir in sorted(speaker_dir.iterdir()):\n",
        "                if not book_dir.is_dir():\n",
        "                    continue\n",
        "                book_id = book_dir.name\n",
        "                \n",
        "                trans_file = book_dir / f\"{speaker_id}-{book_id}.trans.txt\"\n",
        "                if not trans_file.exists():\n",
        "                    continue\n",
        "\n",
        "                with open(trans_file, 'r', encoding='utf-8') as f:\n",
        "                    for line in f:\n",
        "                        line = line.strip()\n",
        "                        if not line:\n",
        "                            continue\n",
        "                        parts = line.split(' ', 1)\n",
        "                        if len(parts) == 2:\n",
        "                            transcript_id = parts[0]\n",
        "                            transcription = parts[1].lower()\n",
        "                            \n",
        "                            audio_file = book_dir / f\"{transcript_id}.flac\"\n",
        "                            if audio_file.exists():\n",
        "                                self.data.append((str(audio_file), transcription))\n",
        "        \n",
        "        print(f\" Chargé {len(self.data)} échantillons depuis {subset}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, transcription = self.data[idx]\n",
        "        \n",
        "        if self.use_mfcc:\n",
        "            features = extract_mfcc(audio_path, sample_rate=self.sample_rate, n_mfcc=self.n_mfcc)\n",
        "        else:\n",
        "            features = extract_melspectrogram(audio_path, sample_rate=self.sample_rate)\n",
        "        \n",
        "        encoded_text = self.encoder.encode(transcription)\n",
        "        \n",
        "        return features, torch.tensor(encoded_text, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ASRDataset(Dataset):\n",
        "    def __init__(self, transcriptions_file, audio_dir, encoder, sample_rate=SAMPLE_RATE):\n",
        "        self.audio_dir = Path(audio_dir)\n",
        "        self.encoder = encoder\n",
        "        self.sample_rate = sample_rate\n",
        "        \n",
        "        self.data = []\n",
        "        with open(transcriptions_file, 'r', encoding='utf-8') as f:\n",
        "            next(f)\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) >= 7:\n",
        "                    audio_file = parts[2]\n",
        "                    transcription = parts[6]\n",
        "                    if transcription:\n",
        "                        self.data.append((audio_file, transcription))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio_file, transcription = self.data[idx]\n",
        "        audio_path = self.audio_dir / audio_file\n",
        "        \n",
        "        melspectrogram = extract_melspectrogram(str(audio_path), sample_rate=self.sample_rate)\n",
        "        encoded_text = self.encoder.encode(transcription)\n",
        "        \n",
        "        return melspectrogram, torch.tensor(encoded_text, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ASRDatasetMFCC(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset pour la Partie 1 (MLP + MFCC).\n",
        "    \"\"\"\n",
        "    def __init__(self, transcriptions_file, audio_dir, encoder, sample_rate=SAMPLE_RATE, n_mfcc=13):\n",
        "        self.audio_dir = Path(audio_dir)\n",
        "        self.encoder = encoder\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mfcc = n_mfcc\n",
        "        \n",
        "        self.data = []\n",
        "        with open(transcriptions_file, 'r', encoding='utf-8') as f:\n",
        "            next(f)\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) >= 7:\n",
        "                    audio_file = parts[2]\n",
        "                    transcription = parts[6]\n",
        "                    if transcription:\n",
        "                        self.data.append((audio_file, transcription))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio_file, transcription = self.data[idx]\n",
        "        audio_path = self.audio_dir / audio_file\n",
        "        \n",
        "        mfcc = extract_mfcc(str(audio_path), sample_rate=self.sample_rate, n_mfcc=self.n_mfcc)\n",
        "        encoded_text = self.encoder.encode(transcription)\n",
        "        \n",
        "        return mfcc, torch.tensor(encoded_text, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    melspectrograms, texts = zip(*batch)\n",
        "    \n",
        "    max_mel_len = max(mel.shape[0] for mel in melspectrograms)\n",
        "    max_text_len = max(text.shape[0] for text in texts)\n",
        "    \n",
        "    batch_mels = []\n",
        "    batch_texts = []\n",
        "    mel_lengths = []\n",
        "    text_lengths = []\n",
        "    \n",
        "    for mel, text in zip(melspectrograms, texts):\n",
        "        mel_len = mel.shape[0]\n",
        "        text_len = text.shape[0]\n",
        "        \n",
        "        padded_mel = torch.zeros(max_mel_len, mel.shape[1])\n",
        "        padded_mel[:mel_len] = mel\n",
        "        batch_mels.append(padded_mel)\n",
        "        \n",
        "        padded_text = torch.zeros(max_text_len, dtype=torch.long)\n",
        "        padded_text[:text_len] = text\n",
        "        batch_texts.append(padded_text)\n",
        "        \n",
        "        mel_lengths.append(mel_len)\n",
        "        text_lengths.append(text_len)\n",
        "    \n",
        "    return torch.stack(batch_mels), torch.stack(batch_texts), torch.tensor(mel_lengths), torch.tensor(text_lengths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. CTC Loss and Decoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "USE_LIBRISPEECH = True\n",
        "\n",
        "if USE_LIBRISPEECH:\n",
        "    librispeech_dir = 'data/LibriSpeech'\n",
        "    subset = 'train-clean-100'\n",
        "    \n",
        "else:\n",
        "    print(\" Configuration dataset français (TSV)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ctc_greedy_decode(logits, encoder):\n",
        "    batch_size, seq_len, vocab_size = logits.size()\n",
        "    predictions = []\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        pred_indices = logits[i].argmax(dim=1).cpu().numpy()\n",
        "        \n",
        "        collapsed = []\n",
        "        prev = None\n",
        "        for idx in pred_indices:\n",
        "            if idx != prev:\n",
        "                collapsed.append(idx)\n",
        "            prev = idx\n",
        "        \n",
        "        filtered = [idx for idx in collapsed if idx != 0]\n",
        "        decoded = encoder.decode(filtered)\n",
        "        predictions.append(decoded)\n",
        "    \n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Partie 1 - Training MLP + MFCC + CTC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Chargement LibriSpeech pour Partie 1 (MLP + MFCC)...\n",
            " Chargé 28539 échantillons depuis train-clean-100\n",
            "Modèle MLP créé: 76829 paramètres\n",
            "Vocabulaire: 29 caractères\n",
            "Dataset: 28539 échantillons\n"
          ]
        }
      ],
      "source": [
        "\n",
        "N_MFCC = 13\n",
        "\n",
        "librispeech_dir = 'data/LibriSpeech'\n",
        "subset = 'train-clean-100'\n",
        "\n",
        "print(\" Chargement LibriSpeech pour Partie 1 (MLP + MFCC)...\")\n",
        "dataset_mfcc = LibriSpeechDataset(librispeech_dir, None, subset=subset, n_mfcc=N_MFCC)\n",
        "\n",
        "if len(dataset_mfcc.data) > 0:\n",
        "    texts = [transcription for _, transcription in dataset_mfcc.data]\n",
        "    encoder_mfcc = CharEncoder()\n",
        "    encoder_mfcc.build_vocab(texts)\n",
        "    vocab_size_mfcc = encoder_mfcc.get_vocab_size()\n",
        "    dataset_mfcc.encoder = encoder_mfcc\n",
        "    dataloader_mfcc = DataLoader(dataset_mfcc, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    model_mlp = SimpleMLPASR(\n",
        "        input_size=N_MFCC,\n",
        "        vocab_size=vocab_size_mfcc,\n",
        "    ).to(device)\n",
        "    \n",
        "    criterion_mfcc = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "    optimizer_mlp = optim.Adam(model_mlp.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    print(f\"Modèle MLP créé: {sum(p.numel() for p in model_mlp.parameters())} paramètres\")\n",
        "    print(f\"Vocabulaire: {vocab_size_mfcc} caractères\")\n",
        "    print(f\"Dataset: {len(dataset_mfcc)} échantillons\")\n",
        "else:\n",
        "    print(\" Aucun échantillon trouvé dans LibriSpeech!\")\n",
        "    print(f\"   Vérifiez que {librispeech_dir}/{subset} existe et contient des fichiers .flac\")\n",
        "    dataloader_mfcc = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/robindebastos/dev/Epita/stt_tp1/.venv/lib/python3.12/site-packages/torchaudio/functional/functional.py:582: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[MLP+MFCC] Epoch [1/10], Batch [10/892], Loss: 4.2328\n",
            "[MLP+MFCC] Epoch [1/10], Batch [20/892], Loss: 3.9936\n",
            "[MLP+MFCC] Epoch [1/10], Batch [30/892], Loss: 3.1418\n",
            "[MLP+MFCC] Epoch [1/10], Batch [40/892], Loss: 3.0028\n",
            "[MLP+MFCC] Epoch [1/10], Batch [50/892], Loss: 2.9178\n",
            "[MLP+MFCC] Epoch [1/10], Batch [60/892], Loss: 2.9057\n",
            "[MLP+MFCC] Epoch [1/10], Batch [70/892], Loss: 2.8761\n",
            "[MLP+MFCC] Epoch [1/10], Batch [80/892], Loss: 2.8916\n",
            "[MLP+MFCC] Epoch [1/10], Batch [90/892], Loss: 2.8790\n",
            "[MLP+MFCC] Epoch [1/10], Batch [100/892], Loss: 2.9035\n",
            "[MLP+MFCC] Epoch [1/10], Batch [110/892], Loss: 2.9223\n",
            "[MLP+MFCC] Epoch [1/10], Batch [120/892], Loss: 2.8773\n",
            "[MLP+MFCC] Epoch [1/10], Batch [130/892], Loss: 2.8767\n",
            "[MLP+MFCC] Epoch [1/10], Batch [140/892], Loss: 2.8794\n",
            "[MLP+MFCC] Epoch [1/10], Batch [150/892], Loss: 2.8626\n",
            "[MLP+MFCC] Epoch [1/10], Batch [160/892], Loss: 2.8890\n",
            "[MLP+MFCC] Epoch [1/10], Batch [170/892], Loss: 2.8884\n",
            "[MLP+MFCC] Epoch [1/10], Batch [180/892], Loss: 2.8803\n",
            "[MLP+MFCC] Epoch [1/10], Batch [190/892], Loss: 2.8578\n",
            "[MLP+MFCC] Epoch [1/10], Batch [200/892], Loss: 2.8584\n",
            "[MLP+MFCC] Epoch [1/10], Batch [210/892], Loss: 2.8644\n",
            "[MLP+MFCC] Epoch [1/10], Batch [220/892], Loss: 2.8836\n",
            "[MLP+MFCC] Epoch [1/10], Batch [230/892], Loss: 2.8822\n",
            "[MLP+MFCC] Epoch [1/10], Batch [240/892], Loss: 2.8705\n",
            "[MLP+MFCC] Epoch [1/10], Batch [250/892], Loss: 2.8831\n",
            "[MLP+MFCC] Epoch [1/10], Batch [260/892], Loss: 2.8788\n",
            "[MLP+MFCC] Epoch [1/10], Batch [270/892], Loss: 2.8656\n",
            "[MLP+MFCC] Epoch [1/10], Batch [280/892], Loss: 2.8471\n",
            "[MLP+MFCC] Epoch [1/10], Batch [290/892], Loss: 2.8706\n",
            "[MLP+MFCC] Epoch [1/10], Batch [300/892], Loss: 2.8883\n",
            "[MLP+MFCC] Epoch [1/10], Batch [310/892], Loss: 2.8605\n",
            "[MLP+MFCC] Epoch [1/10], Batch [320/892], Loss: 2.8495\n",
            "[MLP+MFCC] Epoch [1/10], Batch [330/892], Loss: 2.8496\n",
            "[MLP+MFCC] Epoch [1/10], Batch [340/892], Loss: 2.8601\n",
            "[MLP+MFCC] Epoch [1/10], Batch [350/892], Loss: 2.8671\n",
            "[MLP+MFCC] Epoch [1/10], Batch [360/892], Loss: 2.8533\n",
            "[MLP+MFCC] Epoch [1/10], Batch [370/892], Loss: 2.8335\n",
            "[MLP+MFCC] Epoch [1/10], Batch [380/892], Loss: 2.8536\n",
            "[MLP+MFCC] Epoch [1/10], Batch [390/892], Loss: 2.8311\n",
            "[MLP+MFCC] Epoch [1/10], Batch [400/892], Loss: 2.8245\n",
            "[MLP+MFCC] Epoch [1/10], Batch [410/892], Loss: 2.8027\n",
            "[MLP+MFCC] Epoch [1/10], Batch [420/892], Loss: 2.8145\n",
            "[MLP+MFCC] Epoch [1/10], Batch [430/892], Loss: 2.8036\n",
            "[MLP+MFCC] Epoch [1/10], Batch [440/892], Loss: 2.8040\n",
            "[MLP+MFCC] Epoch [1/10], Batch [450/892], Loss: 2.7607\n",
            "[MLP+MFCC] Epoch [1/10], Batch [460/892], Loss: 2.7176\n",
            "[MLP+MFCC] Epoch [1/10], Batch [470/892], Loss: 2.7290\n",
            "[MLP+MFCC] Epoch [1/10], Batch [480/892], Loss: 2.7438\n",
            "[MLP+MFCC] Epoch [1/10], Batch [490/892], Loss: 2.7047\n",
            "[MLP+MFCC] Epoch [1/10], Batch [500/892], Loss: 2.6936\n",
            "[MLP+MFCC] Epoch [1/10], Batch [510/892], Loss: 2.6667\n",
            "[MLP+MFCC] Epoch [1/10], Batch [520/892], Loss: 2.6955\n",
            "[MLP+MFCC] Epoch [1/10], Batch [530/892], Loss: 2.7365\n",
            "[MLP+MFCC] Epoch [1/10], Batch [540/892], Loss: 2.6976\n",
            "[MLP+MFCC] Epoch [1/10], Batch [550/892], Loss: 2.6907\n",
            "[MLP+MFCC] Epoch [1/10], Batch [560/892], Loss: 2.6540\n",
            "[MLP+MFCC] Epoch [1/10], Batch [570/892], Loss: 2.6609\n",
            "[MLP+MFCC] Epoch [1/10], Batch [580/892], Loss: 2.6584\n",
            "[MLP+MFCC] Epoch [1/10], Batch [590/892], Loss: 2.6556\n",
            "[MLP+MFCC] Epoch [1/10], Batch [600/892], Loss: 2.6607\n",
            "[MLP+MFCC] Epoch [1/10], Batch [610/892], Loss: 2.6803\n",
            "[MLP+MFCC] Epoch [1/10], Batch [620/892], Loss: 2.6766\n",
            "[MLP+MFCC] Epoch [1/10], Batch [630/892], Loss: 2.6518\n",
            "[MLP+MFCC] Epoch [1/10], Batch [640/892], Loss: 2.6535\n",
            "[MLP+MFCC] Epoch [1/10], Batch [650/892], Loss: 2.6475\n",
            "[MLP+MFCC] Epoch [1/10], Batch [660/892], Loss: 2.6459\n",
            "[MLP+MFCC] Epoch [1/10], Batch [670/892], Loss: 2.6606\n",
            "[MLP+MFCC] Epoch [1/10], Batch [680/892], Loss: 2.6147\n",
            "[MLP+MFCC] Epoch [1/10], Batch [690/892], Loss: 2.6294\n",
            "[MLP+MFCC] Epoch [1/10], Batch [700/892], Loss: 2.6146\n",
            "[MLP+MFCC] Epoch [1/10], Batch [710/892], Loss: 2.6357\n",
            "[MLP+MFCC] Epoch [1/10], Batch [720/892], Loss: 2.6187\n",
            "[MLP+MFCC] Epoch [1/10], Batch [730/892], Loss: 2.6119\n",
            "[MLP+MFCC] Epoch [1/10], Batch [740/892], Loss: 2.6240\n",
            "[MLP+MFCC] Epoch [1/10], Batch [750/892], Loss: 2.6040\n",
            "[MLP+MFCC] Epoch [1/10], Batch [760/892], Loss: 2.5945\n",
            "[MLP+MFCC] Epoch [1/10], Batch [770/892], Loss: 2.6095\n",
            "[MLP+MFCC] Epoch [1/10], Batch [780/892], Loss: 2.6134\n",
            "[MLP+MFCC] Epoch [1/10], Batch [790/892], Loss: 2.5582\n",
            "[MLP+MFCC] Epoch [1/10], Batch [800/892], Loss: 2.6124\n",
            "[MLP+MFCC] Epoch [1/10], Batch [810/892], Loss: 2.6047\n",
            "[MLP+MFCC] Epoch [1/10], Batch [820/892], Loss: 2.6051\n",
            "[MLP+MFCC] Epoch [1/10], Batch [830/892], Loss: 2.5963\n",
            "[MLP+MFCC] Epoch [1/10], Batch [840/892], Loss: 2.6022\n",
            "[MLP+MFCC] Epoch [1/10], Batch [850/892], Loss: 2.6025\n",
            "[MLP+MFCC] Epoch [1/10], Batch [860/892], Loss: 2.5919\n",
            "[MLP+MFCC] Epoch [1/10], Batch [870/892], Loss: 2.6083\n",
            "[MLP+MFCC] Epoch [1/10], Batch [880/892], Loss: 2.5738\n",
            "[MLP+MFCC] Epoch [1/10], Batch [890/892], Loss: 2.5485\n",
            "[MLP+MFCC] Epoch [1/10] completed, Average Loss: 2.8719\n",
            "[MLP+MFCC] Epoch [2/10], Batch [10/892], Loss: 2.6032\n",
            "[MLP+MFCC] Epoch [2/10], Batch [20/892], Loss: 2.5708\n",
            "[MLP+MFCC] Epoch [2/10], Batch [30/892], Loss: 2.5591\n",
            "[MLP+MFCC] Epoch [2/10], Batch [40/892], Loss: 2.6029\n",
            "[MLP+MFCC] Epoch [2/10], Batch [50/892], Loss: 2.5831\n",
            "[MLP+MFCC] Epoch [2/10], Batch [60/892], Loss: 2.5543\n",
            "[MLP+MFCC] Epoch [2/10], Batch [70/892], Loss: 2.5999\n",
            "[MLP+MFCC] Epoch [2/10], Batch [80/892], Loss: 2.5930\n",
            "[MLP+MFCC] Epoch [2/10], Batch [90/892], Loss: 2.5661\n",
            "[MLP+MFCC] Epoch [2/10], Batch [100/892], Loss: 2.5909\n",
            "[MLP+MFCC] Epoch [2/10], Batch [110/892], Loss: 2.6132\n",
            "[MLP+MFCC] Epoch [2/10], Batch [120/892], Loss: 2.5780\n",
            "[MLP+MFCC] Epoch [2/10], Batch [130/892], Loss: 2.5687\n",
            "[MLP+MFCC] Epoch [2/10], Batch [140/892], Loss: 2.5606\n",
            "[MLP+MFCC] Epoch [2/10], Batch [150/892], Loss: 2.5736\n",
            "[MLP+MFCC] Epoch [2/10], Batch [160/892], Loss: 2.5777\n",
            "[MLP+MFCC] Epoch [2/10], Batch [170/892], Loss: 2.5721\n",
            "[MLP+MFCC] Epoch [2/10], Batch [180/892], Loss: 2.5804\n",
            "[MLP+MFCC] Epoch [2/10], Batch [190/892], Loss: 2.5763\n",
            "[MLP+MFCC] Epoch [2/10], Batch [200/892], Loss: 2.5260\n",
            "[MLP+MFCC] Epoch [2/10], Batch [210/892], Loss: 2.5720\n",
            "[MLP+MFCC] Epoch [2/10], Batch [220/892], Loss: 2.5636\n",
            "[MLP+MFCC] Epoch [2/10], Batch [230/892], Loss: 2.5742\n",
            "[MLP+MFCC] Epoch [2/10], Batch [240/892], Loss: 2.5379\n",
            "[MLP+MFCC] Epoch [2/10], Batch [250/892], Loss: 2.5465\n",
            "[MLP+MFCC] Epoch [2/10], Batch [260/892], Loss: 2.5790\n",
            "[MLP+MFCC] Epoch [2/10], Batch [270/892], Loss: 2.5641\n",
            "[MLP+MFCC] Epoch [2/10], Batch [280/892], Loss: 2.5645\n",
            "[MLP+MFCC] Epoch [2/10], Batch [290/892], Loss: 2.5370\n",
            "[MLP+MFCC] Epoch [2/10], Batch [300/892], Loss: 2.5434\n",
            "[MLP+MFCC] Epoch [2/10], Batch [310/892], Loss: 2.5700\n",
            "[MLP+MFCC] Epoch [2/10], Batch [320/892], Loss: 2.5122\n",
            "[MLP+MFCC] Epoch [2/10], Batch [330/892], Loss: 2.5502\n",
            "[MLP+MFCC] Epoch [2/10], Batch [340/892], Loss: 2.5503\n",
            "[MLP+MFCC] Epoch [2/10], Batch [350/892], Loss: 2.5494\n",
            "[MLP+MFCC] Epoch [2/10], Batch [360/892], Loss: 2.5378\n",
            "[MLP+MFCC] Epoch [2/10], Batch [370/892], Loss: 2.5408\n",
            "[MLP+MFCC] Epoch [2/10], Batch [380/892], Loss: 2.5140\n",
            "[MLP+MFCC] Epoch [2/10], Batch [390/892], Loss: 2.5556\n",
            "[MLP+MFCC] Epoch [2/10], Batch [400/892], Loss: 2.5301\n",
            "[MLP+MFCC] Epoch [2/10], Batch [410/892], Loss: 2.5331\n",
            "[MLP+MFCC] Epoch [2/10], Batch [420/892], Loss: 2.4967\n",
            "[MLP+MFCC] Epoch [2/10], Batch [430/892], Loss: 2.5452\n",
            "[MLP+MFCC] Epoch [2/10], Batch [440/892], Loss: 2.5315\n",
            "[MLP+MFCC] Epoch [2/10], Batch [450/892], Loss: 2.5225\n",
            "[MLP+MFCC] Epoch [2/10], Batch [460/892], Loss: 2.5508\n",
            "[MLP+MFCC] Epoch [2/10], Batch [470/892], Loss: 2.5219\n",
            "[MLP+MFCC] Epoch [2/10], Batch [480/892], Loss: 2.5391\n",
            "[MLP+MFCC] Epoch [2/10], Batch [490/892], Loss: 2.5383\n",
            "[MLP+MFCC] Epoch [2/10], Batch [500/892], Loss: 2.5024\n",
            "[MLP+MFCC] Epoch [2/10], Batch [510/892], Loss: 2.5027\n",
            "[MLP+MFCC] Epoch [2/10], Batch [520/892], Loss: 2.4936\n",
            "[MLP+MFCC] Epoch [2/10], Batch [530/892], Loss: 2.5323\n",
            "[MLP+MFCC] Epoch [2/10], Batch [540/892], Loss: 2.5332\n",
            "[MLP+MFCC] Epoch [2/10], Batch [550/892], Loss: 2.5315\n",
            "[MLP+MFCC] Epoch [2/10], Batch [560/892], Loss: 2.5060\n",
            "[MLP+MFCC] Epoch [2/10], Batch [570/892], Loss: 2.5211\n",
            "[MLP+MFCC] Epoch [2/10], Batch [580/892], Loss: 2.5403\n",
            "[MLP+MFCC] Epoch [2/10], Batch [590/892], Loss: 2.5181\n",
            "[MLP+MFCC] Epoch [2/10], Batch [600/892], Loss: 2.5005\n",
            "[MLP+MFCC] Epoch [2/10], Batch [610/892], Loss: 2.5084\n",
            "[MLP+MFCC] Epoch [2/10], Batch [620/892], Loss: 2.5316\n",
            "[MLP+MFCC] Epoch [2/10], Batch [630/892], Loss: 2.5351\n",
            "[MLP+MFCC] Epoch [2/10], Batch [640/892], Loss: 2.4928\n",
            "[MLP+MFCC] Epoch [2/10], Batch [650/892], Loss: 2.5304\n",
            "[MLP+MFCC] Epoch [2/10], Batch [660/892], Loss: 2.4984\n",
            "[MLP+MFCC] Epoch [2/10], Batch [670/892], Loss: 2.5111\n",
            "[MLP+MFCC] Epoch [2/10], Batch [680/892], Loss: 2.5047\n",
            "[MLP+MFCC] Epoch [2/10], Batch [690/892], Loss: 2.5155\n",
            "[MLP+MFCC] Epoch [2/10], Batch [700/892], Loss: 2.5404\n",
            "[MLP+MFCC] Epoch [2/10], Batch [710/892], Loss: 2.5213\n",
            "[MLP+MFCC] Epoch [2/10], Batch [720/892], Loss: 2.5285\n",
            "[MLP+MFCC] Epoch [2/10], Batch [730/892], Loss: 2.4991\n",
            "[MLP+MFCC] Epoch [2/10], Batch [740/892], Loss: 2.5109\n",
            "[MLP+MFCC] Epoch [2/10], Batch [750/892], Loss: 2.5081\n",
            "[MLP+MFCC] Epoch [2/10], Batch [760/892], Loss: 2.5130\n",
            "[MLP+MFCC] Epoch [2/10], Batch [770/892], Loss: 2.4862\n",
            "[MLP+MFCC] Epoch [2/10], Batch [780/892], Loss: 2.4637\n",
            "[MLP+MFCC] Epoch [2/10], Batch [790/892], Loss: 2.5022\n",
            "[MLP+MFCC] Epoch [2/10], Batch [800/892], Loss: 2.5120\n",
            "[MLP+MFCC] Epoch [2/10], Batch [810/892], Loss: 2.5338\n",
            "[MLP+MFCC] Epoch [2/10], Batch [820/892], Loss: 2.5288\n",
            "[MLP+MFCC] Epoch [2/10], Batch [830/892], Loss: 2.5075\n",
            "[MLP+MFCC] Epoch [2/10], Batch [840/892], Loss: 2.5068\n",
            "[MLP+MFCC] Epoch [2/10], Batch [850/892], Loss: 2.5037\n",
            "[MLP+MFCC] Epoch [2/10], Batch [860/892], Loss: 2.4955\n",
            "[MLP+MFCC] Epoch [2/10], Batch [870/892], Loss: 2.4857\n",
            "[MLP+MFCC] Epoch [2/10], Batch [880/892], Loss: 2.4998\n",
            "[MLP+MFCC] Epoch [2/10], Batch [890/892], Loss: 2.5001\n",
            "[MLP+MFCC] Epoch [2/10] completed, Average Loss: 2.5362\n",
            "[MLP+MFCC] Epoch [3/10], Batch [10/892], Loss: 2.4979\n",
            "[MLP+MFCC] Epoch [3/10], Batch [20/892], Loss: 2.4985\n",
            "[MLP+MFCC] Epoch [3/10], Batch [30/892], Loss: 2.5020\n",
            "[MLP+MFCC] Epoch [3/10], Batch [40/892], Loss: 2.5216\n",
            "[MLP+MFCC] Epoch [3/10], Batch [50/892], Loss: 2.5037\n",
            "[MLP+MFCC] Epoch [3/10], Batch [60/892], Loss: 2.5118\n",
            "[MLP+MFCC] Epoch [3/10], Batch [70/892], Loss: 2.4729\n",
            "[MLP+MFCC] Epoch [3/10], Batch [80/892], Loss: 2.4872\n",
            "[MLP+MFCC] Epoch [3/10], Batch [90/892], Loss: 2.4743\n",
            "[MLP+MFCC] Epoch [3/10], Batch [100/892], Loss: 2.4889\n",
            "[MLP+MFCC] Epoch [3/10], Batch [110/892], Loss: 2.4836\n",
            "[MLP+MFCC] Epoch [3/10], Batch [120/892], Loss: 2.4812\n",
            "[MLP+MFCC] Epoch [3/10], Batch [130/892], Loss: 2.5157\n",
            "[MLP+MFCC] Epoch [3/10], Batch [140/892], Loss: 2.5128\n",
            "[MLP+MFCC] Epoch [3/10], Batch [150/892], Loss: 2.4832\n",
            "[MLP+MFCC] Epoch [3/10], Batch [160/892], Loss: 2.4895\n",
            "[MLP+MFCC] Epoch [3/10], Batch [170/892], Loss: 2.4714\n",
            "[MLP+MFCC] Epoch [3/10], Batch [180/892], Loss: 2.4799\n",
            "[MLP+MFCC] Epoch [3/10], Batch [190/892], Loss: 2.4709\n",
            "[MLP+MFCC] Epoch [3/10], Batch [200/892], Loss: 2.4799\n",
            "[MLP+MFCC] Epoch [3/10], Batch [210/892], Loss: 2.4942\n",
            "[MLP+MFCC] Epoch [3/10], Batch [220/892], Loss: 2.5039\n",
            "[MLP+MFCC] Epoch [3/10], Batch [230/892], Loss: 2.4657\n",
            "[MLP+MFCC] Epoch [3/10], Batch [240/892], Loss: 2.4662\n",
            "[MLP+MFCC] Epoch [3/10], Batch [250/892], Loss: 2.4844\n",
            "[MLP+MFCC] Epoch [3/10], Batch [260/892], Loss: 2.5021\n",
            "[MLP+MFCC] Epoch [3/10], Batch [270/892], Loss: 2.4931\n",
            "[MLP+MFCC] Epoch [3/10], Batch [280/892], Loss: 2.4722\n",
            "[MLP+MFCC] Epoch [3/10], Batch [290/892], Loss: 2.5236\n",
            "[MLP+MFCC] Epoch [3/10], Batch [300/892], Loss: 2.4825\n",
            "[MLP+MFCC] Epoch [3/10], Batch [310/892], Loss: 2.4763\n",
            "[MLP+MFCC] Epoch [3/10], Batch [320/892], Loss: 2.4877\n",
            "[MLP+MFCC] Epoch [3/10], Batch [330/892], Loss: 2.4633\n",
            "[MLP+MFCC] Epoch [3/10], Batch [340/892], Loss: 2.4894\n",
            "[MLP+MFCC] Epoch [3/10], Batch [350/892], Loss: 2.5000\n",
            "[MLP+MFCC] Epoch [3/10], Batch [360/892], Loss: 2.4957\n",
            "[MLP+MFCC] Epoch [3/10], Batch [370/892], Loss: 2.4769\n",
            "[MLP+MFCC] Epoch [3/10], Batch [380/892], Loss: 2.4739\n",
            "[MLP+MFCC] Epoch [3/10], Batch [390/892], Loss: 2.4818\n",
            "[MLP+MFCC] Epoch [3/10], Batch [400/892], Loss: 2.4630\n",
            "[MLP+MFCC] Epoch [3/10], Batch [410/892], Loss: 2.4707\n",
            "[MLP+MFCC] Epoch [3/10], Batch [420/892], Loss: 2.4778\n",
            "[MLP+MFCC] Epoch [3/10], Batch [430/892], Loss: 2.5044\n",
            "[MLP+MFCC] Epoch [3/10], Batch [440/892], Loss: 2.4647\n",
            "[MLP+MFCC] Epoch [3/10], Batch [450/892], Loss: 2.4918\n",
            "[MLP+MFCC] Epoch [3/10], Batch [460/892], Loss: 2.4913\n",
            "[MLP+MFCC] Epoch [3/10], Batch [470/892], Loss: 2.5080\n",
            "[MLP+MFCC] Epoch [3/10], Batch [480/892], Loss: 2.4774\n",
            "[MLP+MFCC] Epoch [3/10], Batch [490/892], Loss: 2.4966\n",
            "[MLP+MFCC] Epoch [3/10], Batch [500/892], Loss: 2.4694\n",
            "[MLP+MFCC] Epoch [3/10], Batch [510/892], Loss: 2.4550\n",
            "[MLP+MFCC] Epoch [3/10], Batch [520/892], Loss: 2.4558\n",
            "[MLP+MFCC] Epoch [3/10], Batch [530/892], Loss: 2.4804\n",
            "[MLP+MFCC] Epoch [3/10], Batch [540/892], Loss: 2.4657\n",
            "[MLP+MFCC] Epoch [3/10], Batch [550/892], Loss: 2.5012\n",
            "[MLP+MFCC] Epoch [3/10], Batch [560/892], Loss: 2.4925\n",
            "[MLP+MFCC] Epoch [3/10], Batch [570/892], Loss: 2.4744\n",
            "[MLP+MFCC] Epoch [3/10], Batch [580/892], Loss: 2.4802\n",
            "[MLP+MFCC] Epoch [3/10], Batch [590/892], Loss: 2.4670\n",
            "[MLP+MFCC] Epoch [3/10], Batch [600/892], Loss: 2.5083\n",
            "[MLP+MFCC] Epoch [3/10], Batch [610/892], Loss: 2.4768\n",
            "[MLP+MFCC] Epoch [3/10], Batch [620/892], Loss: 2.4879\n",
            "[MLP+MFCC] Epoch [3/10], Batch [630/892], Loss: 2.4670\n",
            "[MLP+MFCC] Epoch [3/10], Batch [640/892], Loss: 2.5015\n",
            "[MLP+MFCC] Epoch [3/10], Batch [650/892], Loss: 2.5075\n",
            "[MLP+MFCC] Epoch [3/10], Batch [660/892], Loss: 2.4796\n",
            "[MLP+MFCC] Epoch [3/10], Batch [670/892], Loss: 2.4836\n",
            "[MLP+MFCC] Epoch [3/10], Batch [680/892], Loss: 2.5017\n",
            "[MLP+MFCC] Epoch [3/10], Batch [690/892], Loss: 2.4777\n",
            "[MLP+MFCC] Epoch [3/10], Batch [700/892], Loss: 2.4704\n",
            "[MLP+MFCC] Epoch [3/10], Batch [710/892], Loss: 2.4996\n",
            "[MLP+MFCC] Epoch [3/10], Batch [720/892], Loss: 2.4693\n",
            "[MLP+MFCC] Epoch [3/10], Batch [730/892], Loss: 2.4577\n",
            "[MLP+MFCC] Epoch [3/10], Batch [740/892], Loss: 2.4608\n",
            "[MLP+MFCC] Epoch [3/10], Batch [750/892], Loss: 2.4496\n",
            "[MLP+MFCC] Epoch [3/10], Batch [760/892], Loss: 2.4734\n",
            "[MLP+MFCC] Epoch [3/10], Batch [770/892], Loss: 2.4788\n",
            "[MLP+MFCC] Epoch [3/10], Batch [780/892], Loss: 2.5013\n",
            "[MLP+MFCC] Epoch [3/10], Batch [790/892], Loss: 2.4807\n",
            "[MLP+MFCC] Epoch [3/10], Batch [800/892], Loss: 2.4831\n",
            "[MLP+MFCC] Epoch [3/10], Batch [810/892], Loss: 2.4815\n",
            "[MLP+MFCC] Epoch [3/10], Batch [820/892], Loss: 2.4712\n",
            "[MLP+MFCC] Epoch [3/10], Batch [830/892], Loss: 2.4660\n",
            "[MLP+MFCC] Epoch [3/10], Batch [840/892], Loss: 2.4658\n",
            "[MLP+MFCC] Epoch [3/10], Batch [850/892], Loss: 2.4821\n",
            "[MLP+MFCC] Epoch [3/10], Batch [860/892], Loss: 2.4830\n",
            "[MLP+MFCC] Epoch [3/10], Batch [870/892], Loss: 2.4854\n",
            "[MLP+MFCC] Epoch [3/10], Batch [880/892], Loss: 2.4790\n",
            "[MLP+MFCC] Epoch [3/10], Batch [890/892], Loss: 2.4740\n",
            "[MLP+MFCC] Epoch [3/10] completed, Average Loss: 2.4862\n",
            "[MLP+MFCC] Epoch [4/10], Batch [10/892], Loss: 2.5053\n",
            "[MLP+MFCC] Epoch [4/10], Batch [20/892], Loss: 2.4471\n",
            "[MLP+MFCC] Epoch [4/10], Batch [30/892], Loss: 2.4434\n",
            "[MLP+MFCC] Epoch [4/10], Batch [40/892], Loss: 2.4905\n",
            "[MLP+MFCC] Epoch [4/10], Batch [50/892], Loss: 2.4599\n",
            "[MLP+MFCC] Epoch [4/10], Batch [60/892], Loss: 2.4846\n",
            "[MLP+MFCC] Epoch [4/10], Batch [70/892], Loss: 2.4711\n",
            "[MLP+MFCC] Epoch [4/10], Batch [80/892], Loss: 2.4838\n",
            "[MLP+MFCC] Epoch [4/10], Batch [90/892], Loss: 2.4524\n",
            "[MLP+MFCC] Epoch [4/10], Batch [100/892], Loss: 2.5123\n",
            "[MLP+MFCC] Epoch [4/10], Batch [110/892], Loss: 2.5087\n",
            "[MLP+MFCC] Epoch [4/10], Batch [120/892], Loss: 2.4731\n",
            "[MLP+MFCC] Epoch [4/10], Batch [130/892], Loss: 2.4544\n",
            "[MLP+MFCC] Epoch [4/10], Batch [140/892], Loss: 2.4564\n",
            "[MLP+MFCC] Epoch [4/10], Batch [150/892], Loss: 2.4470\n",
            "[MLP+MFCC] Epoch [4/10], Batch [160/892], Loss: 2.4540\n",
            "[MLP+MFCC] Epoch [4/10], Batch [170/892], Loss: 2.4857\n",
            "[MLP+MFCC] Epoch [4/10], Batch [180/892], Loss: 2.4454\n",
            "[MLP+MFCC] Epoch [4/10], Batch [190/892], Loss: 2.4505\n",
            "[MLP+MFCC] Epoch [4/10], Batch [200/892], Loss: 2.4651\n",
            "[MLP+MFCC] Epoch [4/10], Batch [210/892], Loss: 2.4792\n",
            "[MLP+MFCC] Epoch [4/10], Batch [220/892], Loss: 2.5064\n",
            "[MLP+MFCC] Epoch [4/10], Batch [230/892], Loss: 2.4776\n",
            "[MLP+MFCC] Epoch [4/10], Batch [240/892], Loss: 2.4751\n",
            "[MLP+MFCC] Epoch [4/10], Batch [250/892], Loss: 2.4831\n",
            "[MLP+MFCC] Epoch [4/10], Batch [260/892], Loss: 2.4674\n",
            "[MLP+MFCC] Epoch [4/10], Batch [270/892], Loss: 2.4779\n",
            "[MLP+MFCC] Epoch [4/10], Batch [280/892], Loss: 2.4536\n",
            "[MLP+MFCC] Epoch [4/10], Batch [290/892], Loss: 2.4764\n",
            "[MLP+MFCC] Epoch [4/10], Batch [300/892], Loss: 2.4436\n",
            "[MLP+MFCC] Epoch [4/10], Batch [310/892], Loss: 2.4651\n",
            "[MLP+MFCC] Epoch [4/10], Batch [320/892], Loss: 2.4795\n",
            "[MLP+MFCC] Epoch [4/10], Batch [330/892], Loss: 2.4270\n",
            "[MLP+MFCC] Epoch [4/10], Batch [340/892], Loss: 2.4866\n",
            "[MLP+MFCC] Epoch [4/10], Batch [350/892], Loss: 2.4892\n",
            "[MLP+MFCC] Epoch [4/10], Batch [360/892], Loss: 2.4852\n",
            "[MLP+MFCC] Epoch [4/10], Batch [370/892], Loss: 2.4791\n",
            "[MLP+MFCC] Epoch [4/10], Batch [380/892], Loss: 2.4605\n",
            "[MLP+MFCC] Epoch [4/10], Batch [390/892], Loss: 2.4112\n",
            "[MLP+MFCC] Epoch [4/10], Batch [400/892], Loss: 2.4630\n",
            "[MLP+MFCC] Epoch [4/10], Batch [410/892], Loss: 2.4744\n",
            "[MLP+MFCC] Epoch [4/10], Batch [420/892], Loss: 2.4358\n",
            "[MLP+MFCC] Epoch [4/10], Batch [430/892], Loss: 2.4675\n",
            "[MLP+MFCC] Epoch [4/10], Batch [440/892], Loss: 2.4503\n",
            "[MLP+MFCC] Epoch [4/10], Batch [450/892], Loss: 2.4525\n",
            "[MLP+MFCC] Epoch [4/10], Batch [460/892], Loss: 2.4424\n",
            "[MLP+MFCC] Epoch [4/10], Batch [470/892], Loss: 2.4630\n",
            "[MLP+MFCC] Epoch [4/10], Batch [480/892], Loss: 2.4980\n",
            "[MLP+MFCC] Epoch [4/10], Batch [490/892], Loss: 2.4670\n",
            "[MLP+MFCC] Epoch [4/10], Batch [500/892], Loss: 2.4637\n",
            "[MLP+MFCC] Epoch [4/10], Batch [510/892], Loss: 2.4806\n",
            "[MLP+MFCC] Epoch [4/10], Batch [520/892], Loss: 2.4608\n",
            "[MLP+MFCC] Epoch [4/10], Batch [530/892], Loss: 2.4528\n",
            "[MLP+MFCC] Epoch [4/10], Batch [540/892], Loss: 2.4703\n",
            "[MLP+MFCC] Epoch [4/10], Batch [550/892], Loss: 2.4965\n",
            "[MLP+MFCC] Epoch [4/10], Batch [560/892], Loss: 2.4892\n",
            "[MLP+MFCC] Epoch [4/10], Batch [570/892], Loss: 2.4788\n",
            "[MLP+MFCC] Epoch [4/10], Batch [580/892], Loss: 2.4820\n",
            "[MLP+MFCC] Epoch [4/10], Batch [590/892], Loss: 2.4415\n",
            "[MLP+MFCC] Epoch [4/10], Batch [600/892], Loss: 2.4210\n",
            "[MLP+MFCC] Epoch [4/10], Batch [610/892], Loss: 2.4958\n",
            "[MLP+MFCC] Epoch [4/10], Batch [620/892], Loss: 2.4497\n",
            "[MLP+MFCC] Epoch [4/10], Batch [630/892], Loss: 2.4416\n",
            "[MLP+MFCC] Epoch [4/10], Batch [640/892], Loss: 2.4373\n",
            "[MLP+MFCC] Epoch [4/10], Batch [650/892], Loss: 2.4852\n",
            "[MLP+MFCC] Epoch [4/10], Batch [660/892], Loss: 2.4569\n",
            "[MLP+MFCC] Epoch [4/10], Batch [670/892], Loss: 2.4472\n",
            "[MLP+MFCC] Epoch [4/10], Batch [680/892], Loss: 2.4684\n",
            "[MLP+MFCC] Epoch [4/10], Batch [690/892], Loss: 2.4551\n",
            "[MLP+MFCC] Epoch [4/10], Batch [700/892], Loss: 2.4409\n",
            "[MLP+MFCC] Epoch [4/10], Batch [710/892], Loss: 2.4583\n",
            "[MLP+MFCC] Epoch [4/10], Batch [720/892], Loss: 2.4311\n",
            "[MLP+MFCC] Epoch [4/10], Batch [730/892], Loss: 2.4555\n",
            "[MLP+MFCC] Epoch [4/10], Batch [740/892], Loss: 2.4542\n",
            "[MLP+MFCC] Epoch [4/10], Batch [750/892], Loss: 2.4501\n",
            "[MLP+MFCC] Epoch [4/10], Batch [760/892], Loss: 2.4594\n",
            "[MLP+MFCC] Epoch [4/10], Batch [770/892], Loss: 2.4716\n",
            "[MLP+MFCC] Epoch [4/10], Batch [780/892], Loss: 2.4595\n",
            "[MLP+MFCC] Epoch [4/10], Batch [790/892], Loss: 2.4472\n",
            "[MLP+MFCC] Epoch [4/10], Batch [800/892], Loss: 2.4541\n",
            "[MLP+MFCC] Epoch [4/10], Batch [810/892], Loss: 2.4649\n",
            "[MLP+MFCC] Epoch [4/10], Batch [820/892], Loss: 2.4432\n",
            "[MLP+MFCC] Epoch [4/10], Batch [830/892], Loss: 2.4479\n",
            "[MLP+MFCC] Epoch [4/10], Batch [840/892], Loss: 2.4870\n",
            "[MLP+MFCC] Epoch [4/10], Batch [850/892], Loss: 2.4685\n",
            "[MLP+MFCC] Epoch [4/10], Batch [860/892], Loss: 2.4269\n",
            "[MLP+MFCC] Epoch [4/10], Batch [870/892], Loss: 2.4658\n",
            "[MLP+MFCC] Epoch [4/10], Batch [880/892], Loss: 2.4565\n",
            "[MLP+MFCC] Epoch [4/10], Batch [890/892], Loss: 2.4629\n",
            "[MLP+MFCC] Epoch [4/10] completed, Average Loss: 2.4606\n",
            "[MLP+MFCC] Epoch [5/10], Batch [10/892], Loss: 2.4224\n",
            "[MLP+MFCC] Epoch [5/10], Batch [20/892], Loss: 2.4244\n",
            "[MLP+MFCC] Epoch [5/10], Batch [30/892], Loss: 2.4601\n",
            "[MLP+MFCC] Epoch [5/10], Batch [40/892], Loss: 2.4610\n",
            "[MLP+MFCC] Epoch [5/10], Batch [50/892], Loss: 2.4394\n",
            "[MLP+MFCC] Epoch [5/10], Batch [60/892], Loss: 2.4599\n",
            "[MLP+MFCC] Epoch [5/10], Batch [70/892], Loss: 2.4584\n",
            "[MLP+MFCC] Epoch [5/10], Batch [80/892], Loss: 2.4272\n",
            "[MLP+MFCC] Epoch [5/10], Batch [90/892], Loss: 2.4488\n",
            "[MLP+MFCC] Epoch [5/10], Batch [100/892], Loss: 2.4434\n",
            "[MLP+MFCC] Epoch [5/10], Batch [110/892], Loss: 2.4111\n",
            "[MLP+MFCC] Epoch [5/10], Batch [120/892], Loss: 2.4574\n",
            "[MLP+MFCC] Epoch [5/10], Batch [130/892], Loss: 2.4448\n",
            "[MLP+MFCC] Epoch [5/10], Batch [140/892], Loss: 2.4385\n",
            "[MLP+MFCC] Epoch [5/10], Batch [150/892], Loss: 2.4549\n",
            "[MLP+MFCC] Epoch [5/10], Batch [160/892], Loss: 2.4440\n",
            "[MLP+MFCC] Epoch [5/10], Batch [170/892], Loss: 2.4215\n",
            "[MLP+MFCC] Epoch [5/10], Batch [180/892], Loss: 2.4841\n",
            "[MLP+MFCC] Epoch [5/10], Batch [190/892], Loss: 2.4173\n",
            "[MLP+MFCC] Epoch [5/10], Batch [200/892], Loss: 2.4452\n",
            "[MLP+MFCC] Epoch [5/10], Batch [210/892], Loss: 2.4528\n",
            "[MLP+MFCC] Epoch [5/10], Batch [220/892], Loss: 2.4501\n",
            "[MLP+MFCC] Epoch [5/10], Batch [230/892], Loss: 2.4412\n",
            "[MLP+MFCC] Epoch [5/10], Batch [240/892], Loss: 2.4809\n",
            "[MLP+MFCC] Epoch [5/10], Batch [250/892], Loss: 2.4351\n",
            "[MLP+MFCC] Epoch [5/10], Batch [260/892], Loss: 2.4547\n",
            "[MLP+MFCC] Epoch [5/10], Batch [270/892], Loss: 2.4476\n",
            "[MLP+MFCC] Epoch [5/10], Batch [280/892], Loss: 2.4448\n",
            "[MLP+MFCC] Epoch [5/10], Batch [290/892], Loss: 2.4526\n",
            "[MLP+MFCC] Epoch [5/10], Batch [300/892], Loss: 2.4355\n",
            "[MLP+MFCC] Epoch [5/10], Batch [310/892], Loss: 2.4265\n",
            "[MLP+MFCC] Epoch [5/10], Batch [320/892], Loss: 2.4286\n",
            "[MLP+MFCC] Epoch [5/10], Batch [330/892], Loss: 2.4594\n",
            "[MLP+MFCC] Epoch [5/10], Batch [340/892], Loss: 2.4538\n",
            "[MLP+MFCC] Epoch [5/10], Batch [350/892], Loss: 2.4263\n",
            "[MLP+MFCC] Epoch [5/10], Batch [360/892], Loss: 2.4573\n",
            "[MLP+MFCC] Epoch [5/10], Batch [370/892], Loss: 2.4934\n",
            "[MLP+MFCC] Epoch [5/10], Batch [380/892], Loss: 2.4531\n",
            "[MLP+MFCC] Epoch [5/10], Batch [390/892], Loss: 2.4711\n",
            "[MLP+MFCC] Epoch [5/10], Batch [400/892], Loss: 2.4532\n",
            "[MLP+MFCC] Epoch [5/10], Batch [410/892], Loss: 2.4435\n",
            "[MLP+MFCC] Epoch [5/10], Batch [420/892], Loss: 2.4764\n",
            "[MLP+MFCC] Epoch [5/10], Batch [430/892], Loss: 2.4619\n",
            "[MLP+MFCC] Epoch [5/10], Batch [440/892], Loss: 2.4103\n",
            "[MLP+MFCC] Epoch [5/10], Batch [450/892], Loss: 2.4461\n",
            "[MLP+MFCC] Epoch [5/10], Batch [460/892], Loss: 2.3947\n",
            "[MLP+MFCC] Epoch [5/10], Batch [470/892], Loss: 2.4634\n",
            "[MLP+MFCC] Epoch [5/10], Batch [480/892], Loss: 2.4613\n",
            "[MLP+MFCC] Epoch [5/10], Batch [490/892], Loss: 2.4366\n",
            "[MLP+MFCC] Epoch [5/10], Batch [500/892], Loss: 2.4385\n",
            "[MLP+MFCC] Epoch [5/10], Batch [510/892], Loss: 2.4670\n",
            "[MLP+MFCC] Epoch [5/10], Batch [520/892], Loss: 2.4310\n",
            "[MLP+MFCC] Epoch [5/10], Batch [530/892], Loss: 2.4141\n",
            "[MLP+MFCC] Epoch [5/10], Batch [540/892], Loss: 2.4101\n",
            "[MLP+MFCC] Epoch [5/10], Batch [550/892], Loss: 2.4446\n",
            "[MLP+MFCC] Epoch [5/10], Batch [560/892], Loss: 2.4274\n",
            "[MLP+MFCC] Epoch [5/10], Batch [570/892], Loss: 2.4377\n",
            "[MLP+MFCC] Epoch [5/10], Batch [580/892], Loss: 2.4807\n",
            "[MLP+MFCC] Epoch [5/10], Batch [590/892], Loss: 2.4526\n",
            "[MLP+MFCC] Epoch [5/10], Batch [600/892], Loss: 2.4449\n",
            "[MLP+MFCC] Epoch [5/10], Batch [610/892], Loss: 2.4460\n",
            "[MLP+MFCC] Epoch [5/10], Batch [620/892], Loss: 2.4585\n",
            "[MLP+MFCC] Epoch [5/10], Batch [630/892], Loss: 2.4667\n",
            "[MLP+MFCC] Epoch [5/10], Batch [640/892], Loss: 2.4216\n",
            "[MLP+MFCC] Epoch [5/10], Batch [650/892], Loss: 2.4597\n",
            "[MLP+MFCC] Epoch [5/10], Batch [660/892], Loss: 2.4092\n",
            "[MLP+MFCC] Epoch [5/10], Batch [670/892], Loss: 2.4729\n",
            "[MLP+MFCC] Epoch [5/10], Batch [680/892], Loss: 2.4658\n",
            "[MLP+MFCC] Epoch [5/10], Batch [690/892], Loss: 2.4571\n",
            "[MLP+MFCC] Epoch [5/10], Batch [700/892], Loss: 2.4450\n",
            "[MLP+MFCC] Epoch [5/10], Batch [710/892], Loss: 2.4528\n",
            "[MLP+MFCC] Epoch [5/10], Batch [720/892], Loss: 2.4362\n",
            "[MLP+MFCC] Epoch [5/10], Batch [730/892], Loss: 2.4183\n",
            "[MLP+MFCC] Epoch [5/10], Batch [740/892], Loss: 2.4580\n",
            "[MLP+MFCC] Epoch [5/10], Batch [750/892], Loss: 2.4018\n",
            "[MLP+MFCC] Epoch [5/10], Batch [760/892], Loss: 2.4543\n",
            "[MLP+MFCC] Epoch [5/10], Batch [770/892], Loss: 2.4278\n",
            "[MLP+MFCC] Epoch [5/10], Batch [780/892], Loss: 2.4693\n",
            "[MLP+MFCC] Epoch [5/10], Batch [790/892], Loss: 2.4399\n",
            "[MLP+MFCC] Epoch [5/10], Batch [800/892], Loss: 2.4434\n",
            "[MLP+MFCC] Epoch [5/10], Batch [810/892], Loss: 2.4759\n",
            "[MLP+MFCC] Epoch [5/10], Batch [820/892], Loss: 2.4480\n",
            "[MLP+MFCC] Epoch [5/10], Batch [830/892], Loss: 2.4491\n",
            "[MLP+MFCC] Epoch [5/10], Batch [840/892], Loss: 2.4258\n",
            "[MLP+MFCC] Epoch [5/10], Batch [850/892], Loss: 2.4169\n",
            "[MLP+MFCC] Epoch [5/10], Batch [860/892], Loss: 2.4130\n",
            "[MLP+MFCC] Epoch [5/10], Batch [870/892], Loss: 2.4413\n",
            "[MLP+MFCC] Epoch [5/10], Batch [880/892], Loss: 2.4192\n",
            "[MLP+MFCC] Epoch [5/10], Batch [890/892], Loss: 2.4510\n",
            "[MLP+MFCC] Epoch [5/10] completed, Average Loss: 2.4435\n",
            "[MLP+MFCC] Epoch [6/10], Batch [10/892], Loss: 2.4304\n",
            "[MLP+MFCC] Epoch [6/10], Batch [20/892], Loss: 2.4513\n",
            "[MLP+MFCC] Epoch [6/10], Batch [30/892], Loss: 2.4481\n",
            "[MLP+MFCC] Epoch [6/10], Batch [40/892], Loss: 2.4402\n",
            "[MLP+MFCC] Epoch [6/10], Batch [50/892], Loss: 2.4188\n",
            "[MLP+MFCC] Epoch [6/10], Batch [60/892], Loss: 2.4083\n",
            "[MLP+MFCC] Epoch [6/10], Batch [70/892], Loss: 2.4210\n",
            "[MLP+MFCC] Epoch [6/10], Batch [80/892], Loss: 2.4356\n",
            "[MLP+MFCC] Epoch [6/10], Batch [90/892], Loss: 2.4441\n",
            "[MLP+MFCC] Epoch [6/10], Batch [100/892], Loss: 2.4327\n",
            "[MLP+MFCC] Epoch [6/10], Batch [110/892], Loss: 2.4417\n",
            "[MLP+MFCC] Epoch [6/10], Batch [120/892], Loss: 2.4197\n",
            "[MLP+MFCC] Epoch [6/10], Batch [130/892], Loss: 2.3989\n",
            "[MLP+MFCC] Epoch [6/10], Batch [140/892], Loss: 2.4395\n",
            "[MLP+MFCC] Epoch [6/10], Batch [150/892], Loss: 2.4305\n",
            "[MLP+MFCC] Epoch [6/10], Batch [160/892], Loss: 2.4397\n",
            "[MLP+MFCC] Epoch [6/10], Batch [170/892], Loss: 2.4284\n",
            "[MLP+MFCC] Epoch [6/10], Batch [180/892], Loss: 2.4181\n",
            "[MLP+MFCC] Epoch [6/10], Batch [190/892], Loss: 2.4551\n",
            "[MLP+MFCC] Epoch [6/10], Batch [200/892], Loss: 2.4502\n",
            "[MLP+MFCC] Epoch [6/10], Batch [210/892], Loss: 2.4384\n",
            "[MLP+MFCC] Epoch [6/10], Batch [220/892], Loss: 2.4426\n",
            "[MLP+MFCC] Epoch [6/10], Batch [230/892], Loss: 2.4267\n",
            "[MLP+MFCC] Epoch [6/10], Batch [240/892], Loss: 2.4197\n",
            "[MLP+MFCC] Epoch [6/10], Batch [250/892], Loss: 2.4376\n",
            "[MLP+MFCC] Epoch [6/10], Batch [260/892], Loss: 2.4523\n",
            "[MLP+MFCC] Epoch [6/10], Batch [270/892], Loss: 2.4307\n",
            "[MLP+MFCC] Epoch [6/10], Batch [280/892], Loss: 2.4330\n",
            "[MLP+MFCC] Epoch [6/10], Batch [290/892], Loss: 2.4226\n",
            "[MLP+MFCC] Epoch [6/10], Batch [300/892], Loss: 2.4638\n",
            "[MLP+MFCC] Epoch [6/10], Batch [310/892], Loss: 2.4181\n",
            "[MLP+MFCC] Epoch [6/10], Batch [320/892], Loss: 2.4273\n",
            "[MLP+MFCC] Epoch [6/10], Batch [330/892], Loss: 2.4251\n",
            "[MLP+MFCC] Epoch [6/10], Batch [340/892], Loss: 2.4155\n",
            "[MLP+MFCC] Epoch [6/10], Batch [350/892], Loss: 2.4112\n",
            "[MLP+MFCC] Epoch [6/10], Batch [360/892], Loss: 2.4415\n",
            "[MLP+MFCC] Epoch [6/10], Batch [370/892], Loss: 2.4197\n",
            "[MLP+MFCC] Epoch [6/10], Batch [380/892], Loss: 2.4519\n",
            "[MLP+MFCC] Epoch [6/10], Batch [390/892], Loss: 2.4539\n",
            "[MLP+MFCC] Epoch [6/10], Batch [400/892], Loss: 2.4155\n",
            "[MLP+MFCC] Epoch [6/10], Batch [410/892], Loss: 2.4409\n",
            "[MLP+MFCC] Epoch [6/10], Batch [420/892], Loss: 2.4200\n",
            "[MLP+MFCC] Epoch [6/10], Batch [430/892], Loss: 2.4196\n",
            "[MLP+MFCC] Epoch [6/10], Batch [440/892], Loss: 2.4241\n",
            "[MLP+MFCC] Epoch [6/10], Batch [450/892], Loss: 2.4366\n",
            "[MLP+MFCC] Epoch [6/10], Batch [460/892], Loss: 2.4278\n",
            "[MLP+MFCC] Epoch [6/10], Batch [470/892], Loss: 2.4508\n",
            "[MLP+MFCC] Epoch [6/10], Batch [480/892], Loss: 2.4167\n",
            "[MLP+MFCC] Epoch [6/10], Batch [490/892], Loss: 2.4577\n",
            "[MLP+MFCC] Epoch [6/10], Batch [500/892], Loss: 2.3937\n",
            "[MLP+MFCC] Epoch [6/10], Batch [510/892], Loss: 2.3896\n",
            "[MLP+MFCC] Epoch [6/10], Batch [520/892], Loss: 2.4276\n",
            "[MLP+MFCC] Epoch [6/10], Batch [530/892], Loss: 2.4102\n",
            "[MLP+MFCC] Epoch [6/10], Batch [540/892], Loss: 2.4325\n",
            "[MLP+MFCC] Epoch [6/10], Batch [550/892], Loss: 2.4316\n",
            "[MLP+MFCC] Epoch [6/10], Batch [560/892], Loss: 2.3835\n",
            "[MLP+MFCC] Epoch [6/10], Batch [570/892], Loss: 2.4254\n",
            "[MLP+MFCC] Epoch [6/10], Batch [580/892], Loss: 2.4236\n",
            "[MLP+MFCC] Epoch [6/10], Batch [590/892], Loss: 2.4088\n",
            "[MLP+MFCC] Epoch [6/10], Batch [600/892], Loss: 2.4637\n",
            "[MLP+MFCC] Epoch [6/10], Batch [610/892], Loss: 2.4260\n",
            "[MLP+MFCC] Epoch [6/10], Batch [620/892], Loss: 2.4332\n",
            "[MLP+MFCC] Epoch [6/10], Batch [630/892], Loss: 2.4303\n",
            "[MLP+MFCC] Epoch [6/10], Batch [640/892], Loss: 2.4220\n",
            "[MLP+MFCC] Epoch [6/10], Batch [650/892], Loss: 2.3883\n",
            "[MLP+MFCC] Epoch [6/10], Batch [660/892], Loss: 2.4651\n",
            "[MLP+MFCC] Epoch [6/10], Batch [670/892], Loss: 2.4484\n",
            "[MLP+MFCC] Epoch [6/10], Batch [680/892], Loss: 2.4104\n",
            "[MLP+MFCC] Epoch [6/10], Batch [690/892], Loss: 2.4326\n",
            "[MLP+MFCC] Epoch [6/10], Batch [700/892], Loss: 2.4178\n",
            "[MLP+MFCC] Epoch [6/10], Batch [710/892], Loss: 2.4465\n",
            "[MLP+MFCC] Epoch [6/10], Batch [720/892], Loss: 2.4485\n",
            "[MLP+MFCC] Epoch [6/10], Batch [730/892], Loss: 2.4336\n",
            "[MLP+MFCC] Epoch [6/10], Batch [740/892], Loss: 2.4461\n",
            "[MLP+MFCC] Epoch [6/10], Batch [750/892], Loss: 2.3939\n",
            "[MLP+MFCC] Epoch [6/10], Batch [760/892], Loss: 2.4155\n",
            "[MLP+MFCC] Epoch [6/10], Batch [770/892], Loss: 2.4555\n",
            "[MLP+MFCC] Epoch [6/10], Batch [780/892], Loss: 2.4217\n",
            "[MLP+MFCC] Epoch [6/10], Batch [790/892], Loss: 2.4632\n",
            "[MLP+MFCC] Epoch [6/10], Batch [800/892], Loss: 2.3914\n",
            "[MLP+MFCC] Epoch [6/10], Batch [810/892], Loss: 2.4077\n",
            "[MLP+MFCC] Epoch [6/10], Batch [820/892], Loss: 2.4186\n",
            "[MLP+MFCC] Epoch [6/10], Batch [830/892], Loss: 2.4115\n",
            "[MLP+MFCC] Epoch [6/10], Batch [840/892], Loss: 2.4137\n",
            "[MLP+MFCC] Epoch [6/10], Batch [850/892], Loss: 2.4307\n",
            "[MLP+MFCC] Epoch [6/10], Batch [860/892], Loss: 2.4225\n",
            "[MLP+MFCC] Epoch [6/10], Batch [870/892], Loss: 2.4478\n",
            "[MLP+MFCC] Epoch [6/10], Batch [880/892], Loss: 2.4303\n",
            "[MLP+MFCC] Epoch [6/10], Batch [890/892], Loss: 2.4117\n",
            "[MLP+MFCC] Epoch [6/10] completed, Average Loss: 2.4315\n",
            "[MLP+MFCC] Epoch [7/10], Batch [10/892], Loss: 2.4439\n",
            "[MLP+MFCC] Epoch [7/10], Batch [20/892], Loss: 2.4541\n",
            "[MLP+MFCC] Epoch [7/10], Batch [30/892], Loss: 2.4104\n",
            "[MLP+MFCC] Epoch [7/10], Batch [40/892], Loss: 2.3874\n",
            "[MLP+MFCC] Epoch [7/10], Batch [50/892], Loss: 2.4569\n",
            "[MLP+MFCC] Epoch [7/10], Batch [60/892], Loss: 2.4234\n",
            "[MLP+MFCC] Epoch [7/10], Batch [70/892], Loss: 2.4290\n",
            "[MLP+MFCC] Epoch [7/10], Batch [80/892], Loss: 2.4294\n",
            "[MLP+MFCC] Epoch [7/10], Batch [90/892], Loss: 2.4012\n",
            "[MLP+MFCC] Epoch [7/10], Batch [100/892], Loss: 2.4169\n",
            "[MLP+MFCC] Epoch [7/10], Batch [110/892], Loss: 2.4087\n",
            "[MLP+MFCC] Epoch [7/10], Batch [120/892], Loss: 2.4439\n",
            "[MLP+MFCC] Epoch [7/10], Batch [130/892], Loss: 2.4429\n",
            "[MLP+MFCC] Epoch [7/10], Batch [140/892], Loss: 2.4067\n",
            "[MLP+MFCC] Epoch [7/10], Batch [150/892], Loss: 2.4023\n",
            "[MLP+MFCC] Epoch [7/10], Batch [160/892], Loss: 2.4113\n",
            "[MLP+MFCC] Epoch [7/10], Batch [170/892], Loss: 2.4762\n",
            "[MLP+MFCC] Epoch [7/10], Batch [180/892], Loss: 2.4476\n",
            "[MLP+MFCC] Epoch [7/10], Batch [190/892], Loss: 2.4499\n",
            "[MLP+MFCC] Epoch [7/10], Batch [200/892], Loss: 2.4659\n",
            "[MLP+MFCC] Epoch [7/10], Batch [210/892], Loss: 2.4165\n",
            "[MLP+MFCC] Epoch [7/10], Batch [220/892], Loss: 2.4400\n",
            "[MLP+MFCC] Epoch [7/10], Batch [230/892], Loss: 2.4280\n",
            "[MLP+MFCC] Epoch [7/10], Batch [240/892], Loss: 2.4376\n",
            "[MLP+MFCC] Epoch [7/10], Batch [250/892], Loss: 2.4422\n",
            "[MLP+MFCC] Epoch [7/10], Batch [260/892], Loss: 2.4142\n",
            "[MLP+MFCC] Epoch [7/10], Batch [270/892], Loss: 2.4696\n",
            "[MLP+MFCC] Epoch [7/10], Batch [280/892], Loss: 2.4388\n",
            "[MLP+MFCC] Epoch [7/10], Batch [290/892], Loss: 2.4406\n",
            "[MLP+MFCC] Epoch [7/10], Batch [300/892], Loss: 2.4241\n",
            "[MLP+MFCC] Epoch [7/10], Batch [310/892], Loss: 2.4146\n",
            "[MLP+MFCC] Epoch [7/10], Batch [320/892], Loss: 2.4356\n",
            "[MLP+MFCC] Epoch [7/10], Batch [330/892], Loss: 2.4421\n",
            "[MLP+MFCC] Epoch [7/10], Batch [340/892], Loss: 2.4395\n",
            "[MLP+MFCC] Epoch [7/10], Batch [350/892], Loss: 2.3983\n",
            "[MLP+MFCC] Epoch [7/10], Batch [360/892], Loss: 2.4137\n",
            "[MLP+MFCC] Epoch [7/10], Batch [370/892], Loss: 2.4253\n",
            "[MLP+MFCC] Epoch [7/10], Batch [380/892], Loss: 2.4444\n",
            "[MLP+MFCC] Epoch [7/10], Batch [390/892], Loss: 2.4479\n",
            "[MLP+MFCC] Epoch [7/10], Batch [400/892], Loss: 2.4251\n",
            "[MLP+MFCC] Epoch [7/10], Batch [410/892], Loss: 2.4276\n",
            "[MLP+MFCC] Epoch [7/10], Batch [420/892], Loss: 2.4201\n",
            "[MLP+MFCC] Epoch [7/10], Batch [430/892], Loss: 2.3974\n",
            "[MLP+MFCC] Epoch [7/10], Batch [440/892], Loss: 2.4616\n",
            "[MLP+MFCC] Epoch [7/10], Batch [450/892], Loss: 2.3842\n",
            "[MLP+MFCC] Epoch [7/10], Batch [460/892], Loss: 2.3960\n",
            "[MLP+MFCC] Epoch [7/10], Batch [470/892], Loss: 2.4258\n",
            "[MLP+MFCC] Epoch [7/10], Batch [480/892], Loss: 2.4241\n",
            "[MLP+MFCC] Epoch [7/10], Batch [490/892], Loss: 2.4340\n",
            "[MLP+MFCC] Epoch [7/10], Batch [500/892], Loss: 2.4347\n",
            "[MLP+MFCC] Epoch [7/10], Batch [510/892], Loss: 2.4123\n",
            "[MLP+MFCC] Epoch [7/10], Batch [520/892], Loss: 2.3902\n",
            "[MLP+MFCC] Epoch [7/10], Batch [530/892], Loss: 2.4021\n",
            "[MLP+MFCC] Epoch [7/10], Batch [540/892], Loss: 2.3983\n",
            "[MLP+MFCC] Epoch [7/10], Batch [550/892], Loss: 2.4017\n",
            "[MLP+MFCC] Epoch [7/10], Batch [560/892], Loss: 2.4479\n",
            "[MLP+MFCC] Epoch [7/10], Batch [570/892], Loss: 2.4558\n",
            "[MLP+MFCC] Epoch [7/10], Batch [580/892], Loss: 2.4111\n",
            "[MLP+MFCC] Epoch [7/10], Batch [590/892], Loss: 2.4299\n",
            "[MLP+MFCC] Epoch [7/10], Batch [600/892], Loss: 2.4598\n",
            "[MLP+MFCC] Epoch [7/10], Batch [610/892], Loss: 2.4315\n",
            "[MLP+MFCC] Epoch [7/10], Batch [620/892], Loss: 2.3894\n",
            "[MLP+MFCC] Epoch [7/10], Batch [630/892], Loss: 2.4182\n",
            "[MLP+MFCC] Epoch [7/10], Batch [640/892], Loss: 2.4068\n",
            "[MLP+MFCC] Epoch [7/10], Batch [650/892], Loss: 2.4080\n",
            "[MLP+MFCC] Epoch [7/10], Batch [660/892], Loss: 2.3851\n",
            "[MLP+MFCC] Epoch [7/10], Batch [670/892], Loss: 2.4135\n",
            "[MLP+MFCC] Epoch [7/10], Batch [680/892], Loss: 2.4164\n",
            "[MLP+MFCC] Epoch [7/10], Batch [690/892], Loss: 2.4067\n",
            "[MLP+MFCC] Epoch [7/10], Batch [700/892], Loss: 2.4409\n",
            "[MLP+MFCC] Epoch [7/10], Batch [710/892], Loss: 2.4378\n",
            "[MLP+MFCC] Epoch [7/10], Batch [720/892], Loss: 2.4246\n",
            "[MLP+MFCC] Epoch [7/10], Batch [730/892], Loss: 2.4150\n",
            "[MLP+MFCC] Epoch [7/10], Batch [740/892], Loss: 2.4092\n",
            "[MLP+MFCC] Epoch [7/10], Batch [750/892], Loss: 2.4214\n",
            "[MLP+MFCC] Epoch [7/10], Batch [760/892], Loss: 2.4144\n",
            "[MLP+MFCC] Epoch [7/10], Batch [770/892], Loss: 2.3794\n",
            "[MLP+MFCC] Epoch [7/10], Batch [780/892], Loss: 2.4028\n",
            "[MLP+MFCC] Epoch [7/10], Batch [790/892], Loss: 2.4018\n",
            "[MLP+MFCC] Epoch [7/10], Batch [800/892], Loss: 2.4573\n",
            "[MLP+MFCC] Epoch [7/10], Batch [810/892], Loss: 2.4286\n",
            "[MLP+MFCC] Epoch [7/10], Batch [820/892], Loss: 2.4208\n",
            "[MLP+MFCC] Epoch [7/10], Batch [830/892], Loss: 2.4158\n",
            "[MLP+MFCC] Epoch [7/10], Batch [840/892], Loss: 2.4331\n",
            "[MLP+MFCC] Epoch [7/10], Batch [850/892], Loss: 2.3705\n",
            "[MLP+MFCC] Epoch [7/10], Batch [860/892], Loss: 2.3904\n",
            "[MLP+MFCC] Epoch [7/10], Batch [870/892], Loss: 2.4461\n",
            "[MLP+MFCC] Epoch [7/10], Batch [880/892], Loss: 2.3832\n",
            "[MLP+MFCC] Epoch [7/10], Batch [890/892], Loss: 2.4203\n",
            "[MLP+MFCC] Epoch [7/10] completed, Average Loss: 2.4216\n",
            "[MLP+MFCC] Epoch [8/10], Batch [10/892], Loss: 2.4308\n",
            "[MLP+MFCC] Epoch [8/10], Batch [20/892], Loss: 2.4093\n",
            "[MLP+MFCC] Epoch [8/10], Batch [30/892], Loss: 2.4599\n",
            "[MLP+MFCC] Epoch [8/10], Batch [40/892], Loss: 2.4197\n",
            "[MLP+MFCC] Epoch [8/10], Batch [50/892], Loss: 2.3836\n",
            "[MLP+MFCC] Epoch [8/10], Batch [60/892], Loss: 2.4385\n",
            "[MLP+MFCC] Epoch [8/10], Batch [70/892], Loss: 2.4147\n",
            "[MLP+MFCC] Epoch [8/10], Batch [80/892], Loss: 2.4895\n",
            "[MLP+MFCC] Epoch [8/10], Batch [90/892], Loss: 2.4187\n",
            "[MLP+MFCC] Epoch [8/10], Batch [100/892], Loss: 2.4065\n",
            "[MLP+MFCC] Epoch [8/10], Batch [110/892], Loss: 2.4086\n",
            "[MLP+MFCC] Epoch [8/10], Batch [120/892], Loss: 2.4335\n",
            "[MLP+MFCC] Epoch [8/10], Batch [130/892], Loss: 2.4297\n",
            "[MLP+MFCC] Epoch [8/10], Batch [140/892], Loss: 2.4551\n",
            "[MLP+MFCC] Epoch [8/10], Batch [150/892], Loss: 2.3967\n",
            "[MLP+MFCC] Epoch [8/10], Batch [160/892], Loss: 2.4192\n",
            "[MLP+MFCC] Epoch [8/10], Batch [170/892], Loss: 2.4089\n",
            "[MLP+MFCC] Epoch [8/10], Batch [180/892], Loss: 2.4391\n",
            "[MLP+MFCC] Epoch [8/10], Batch [190/892], Loss: 2.4008\n",
            "[MLP+MFCC] Epoch [8/10], Batch [200/892], Loss: 2.3798\n",
            "[MLP+MFCC] Epoch [8/10], Batch [210/892], Loss: 2.4502\n",
            "[MLP+MFCC] Epoch [8/10], Batch [220/892], Loss: 2.3830\n",
            "[MLP+MFCC] Epoch [8/10], Batch [230/892], Loss: 2.4134\n",
            "[MLP+MFCC] Epoch [8/10], Batch [240/892], Loss: 2.3853\n",
            "[MLP+MFCC] Epoch [8/10], Batch [250/892], Loss: 2.4040\n",
            "[MLP+MFCC] Epoch [8/10], Batch [260/892], Loss: 2.3917\n",
            "[MLP+MFCC] Epoch [8/10], Batch [270/892], Loss: 2.4255\n",
            "[MLP+MFCC] Epoch [8/10], Batch [280/892], Loss: 2.4205\n",
            "[MLP+MFCC] Epoch [8/10], Batch [290/892], Loss: 2.4188\n",
            "[MLP+MFCC] Epoch [8/10], Batch [300/892], Loss: 2.4659\n",
            "[MLP+MFCC] Epoch [8/10], Batch [310/892], Loss: 2.4154\n",
            "[MLP+MFCC] Epoch [8/10], Batch [320/892], Loss: 2.4238\n",
            "[MLP+MFCC] Epoch [8/10], Batch [330/892], Loss: 2.4296\n",
            "[MLP+MFCC] Epoch [8/10], Batch [340/892], Loss: 2.4203\n",
            "[MLP+MFCC] Epoch [8/10], Batch [350/892], Loss: 2.4604\n",
            "[MLP+MFCC] Epoch [8/10], Batch [360/892], Loss: 2.4022\n",
            "[MLP+MFCC] Epoch [8/10], Batch [370/892], Loss: 2.4213\n",
            "[MLP+MFCC] Epoch [8/10], Batch [380/892], Loss: 2.3934\n",
            "[MLP+MFCC] Epoch [8/10], Batch [390/892], Loss: 2.4098\n",
            "[MLP+MFCC] Epoch [8/10], Batch [400/892], Loss: 2.4029\n",
            "[MLP+MFCC] Epoch [8/10], Batch [410/892], Loss: 2.3930\n",
            "[MLP+MFCC] Epoch [8/10], Batch [420/892], Loss: 2.4242\n",
            "[MLP+MFCC] Epoch [8/10], Batch [430/892], Loss: 2.3857\n",
            "[MLP+MFCC] Epoch [8/10], Batch [440/892], Loss: 2.3957\n",
            "[MLP+MFCC] Epoch [8/10], Batch [450/892], Loss: 2.4085\n",
            "[MLP+MFCC] Epoch [8/10], Batch [460/892], Loss: 2.4159\n",
            "[MLP+MFCC] Epoch [8/10], Batch [470/892], Loss: 2.4257\n",
            "[MLP+MFCC] Epoch [8/10], Batch [480/892], Loss: 2.3931\n",
            "[MLP+MFCC] Epoch [8/10], Batch [490/892], Loss: 2.4122\n",
            "[MLP+MFCC] Epoch [8/10], Batch [500/892], Loss: 2.4233\n",
            "[MLP+MFCC] Epoch [8/10], Batch [510/892], Loss: 2.4204\n",
            "[MLP+MFCC] Epoch [8/10], Batch [520/892], Loss: 2.3906\n",
            "[MLP+MFCC] Epoch [8/10], Batch [530/892], Loss: 2.4569\n",
            "[MLP+MFCC] Epoch [8/10], Batch [540/892], Loss: 2.4089\n",
            "[MLP+MFCC] Epoch [8/10], Batch [550/892], Loss: 2.4349\n",
            "[MLP+MFCC] Epoch [8/10], Batch [560/892], Loss: 2.3826\n",
            "[MLP+MFCC] Epoch [8/10], Batch [570/892], Loss: 2.4087\n",
            "[MLP+MFCC] Epoch [8/10], Batch [580/892], Loss: 2.4071\n",
            "[MLP+MFCC] Epoch [8/10], Batch [590/892], Loss: 2.4300\n",
            "[MLP+MFCC] Epoch [8/10], Batch [600/892], Loss: 2.4206\n",
            "[MLP+MFCC] Epoch [8/10], Batch [610/892], Loss: 2.4068\n",
            "[MLP+MFCC] Epoch [8/10], Batch [620/892], Loss: 2.4062\n",
            "[MLP+MFCC] Epoch [8/10], Batch [630/892], Loss: 2.4314\n",
            "[MLP+MFCC] Epoch [8/10], Batch [640/892], Loss: 2.3924\n",
            "[MLP+MFCC] Epoch [8/10], Batch [650/892], Loss: 2.3768\n",
            "[MLP+MFCC] Epoch [8/10], Batch [660/892], Loss: 2.3942\n",
            "[MLP+MFCC] Epoch [8/10], Batch [670/892], Loss: 2.4182\n",
            "[MLP+MFCC] Epoch [8/10], Batch [680/892], Loss: 2.4382\n",
            "[MLP+MFCC] Epoch [8/10], Batch [690/892], Loss: 2.4214\n",
            "[MLP+MFCC] Epoch [8/10], Batch [700/892], Loss: 2.3996\n",
            "[MLP+MFCC] Epoch [8/10], Batch [710/892], Loss: 2.4115\n",
            "[MLP+MFCC] Epoch [8/10], Batch [720/892], Loss: 2.3998\n",
            "[MLP+MFCC] Epoch [8/10], Batch [730/892], Loss: 2.4115\n",
            "[MLP+MFCC] Epoch [8/10], Batch [740/892], Loss: 2.4141\n",
            "[MLP+MFCC] Epoch [8/10], Batch [750/892], Loss: 2.4000\n",
            "[MLP+MFCC] Epoch [8/10], Batch [760/892], Loss: 2.3709\n",
            "[MLP+MFCC] Epoch [8/10], Batch [770/892], Loss: 2.4042\n",
            "[MLP+MFCC] Epoch [8/10], Batch [780/892], Loss: 2.4299\n",
            "[MLP+MFCC] Epoch [8/10], Batch [790/892], Loss: 2.4043\n",
            "[MLP+MFCC] Epoch [8/10], Batch [800/892], Loss: 2.4095\n",
            "[MLP+MFCC] Epoch [8/10], Batch [810/892], Loss: 2.4041\n",
            "[MLP+MFCC] Epoch [8/10], Batch [820/892], Loss: 2.3975\n",
            "[MLP+MFCC] Epoch [8/10], Batch [830/892], Loss: 2.3922\n",
            "[MLP+MFCC] Epoch [8/10], Batch [840/892], Loss: 2.4124\n",
            "[MLP+MFCC] Epoch [8/10], Batch [850/892], Loss: 2.3983\n",
            "[MLP+MFCC] Epoch [8/10], Batch [860/892], Loss: 2.4099\n",
            "[MLP+MFCC] Epoch [8/10], Batch [870/892], Loss: 2.4229\n",
            "[MLP+MFCC] Epoch [8/10], Batch [880/892], Loss: 2.4731\n",
            "[MLP+MFCC] Epoch [8/10], Batch [890/892], Loss: 2.4240\n",
            "[MLP+MFCC] Epoch [8/10] completed, Average Loss: 2.4128\n",
            "[MLP+MFCC] Epoch [9/10], Batch [10/892], Loss: 2.3859\n",
            "[MLP+MFCC] Epoch [9/10], Batch [20/892], Loss: 2.4023\n",
            "[MLP+MFCC] Epoch [9/10], Batch [30/892], Loss: 2.3803\n",
            "[MLP+MFCC] Epoch [9/10], Batch [40/892], Loss: 2.4188\n",
            "[MLP+MFCC] Epoch [9/10], Batch [50/892], Loss: 2.4165\n",
            "[MLP+MFCC] Epoch [9/10], Batch [60/892], Loss: 2.4201\n",
            "[MLP+MFCC] Epoch [9/10], Batch [70/892], Loss: 2.3756\n",
            "[MLP+MFCC] Epoch [9/10], Batch [80/892], Loss: 2.4094\n",
            "[MLP+MFCC] Epoch [9/10], Batch [90/892], Loss: 2.3488\n",
            "[MLP+MFCC] Epoch [9/10], Batch [100/892], Loss: 2.3853\n",
            "[MLP+MFCC] Epoch [9/10], Batch [110/892], Loss: 2.4134\n",
            "[MLP+MFCC] Epoch [9/10], Batch [120/892], Loss: 2.3918\n",
            "[MLP+MFCC] Epoch [9/10], Batch [130/892], Loss: 2.3905\n",
            "[MLP+MFCC] Epoch [9/10], Batch [140/892], Loss: 2.4260\n",
            "[MLP+MFCC] Epoch [9/10], Batch [150/892], Loss: 2.3906\n",
            "[MLP+MFCC] Epoch [9/10], Batch [160/892], Loss: 2.4500\n",
            "[MLP+MFCC] Epoch [9/10], Batch [170/892], Loss: 2.4016\n",
            "[MLP+MFCC] Epoch [9/10], Batch [180/892], Loss: 2.3926\n",
            "[MLP+MFCC] Epoch [9/10], Batch [190/892], Loss: 2.3857\n",
            "[MLP+MFCC] Epoch [9/10], Batch [200/892], Loss: 2.3963\n",
            "[MLP+MFCC] Epoch [9/10], Batch [210/892], Loss: 2.4099\n",
            "[MLP+MFCC] Epoch [9/10], Batch [220/892], Loss: 2.4183\n",
            "[MLP+MFCC] Epoch [9/10], Batch [230/892], Loss: 2.4243\n",
            "[MLP+MFCC] Epoch [9/10], Batch [240/892], Loss: 2.4342\n",
            "[MLP+MFCC] Epoch [9/10], Batch [250/892], Loss: 2.4142\n",
            "[MLP+MFCC] Epoch [9/10], Batch [260/892], Loss: 2.4011\n",
            "[MLP+MFCC] Epoch [9/10], Batch [270/892], Loss: 2.3869\n",
            "[MLP+MFCC] Epoch [9/10], Batch [280/892], Loss: 2.4332\n",
            "[MLP+MFCC] Epoch [9/10], Batch [290/892], Loss: 2.4115\n",
            "[MLP+MFCC] Epoch [9/10], Batch [300/892], Loss: 2.4300\n",
            "[MLP+MFCC] Epoch [9/10], Batch [310/892], Loss: 2.4319\n",
            "[MLP+MFCC] Epoch [9/10], Batch [320/892], Loss: 2.4378\n",
            "[MLP+MFCC] Epoch [9/10], Batch [330/892], Loss: 2.4158\n",
            "[MLP+MFCC] Epoch [9/10], Batch [340/892], Loss: 2.4143\n",
            "[MLP+MFCC] Epoch [9/10], Batch [350/892], Loss: 2.4027\n",
            "[MLP+MFCC] Epoch [9/10], Batch [360/892], Loss: 2.4116\n",
            "[MLP+MFCC] Epoch [9/10], Batch [370/892], Loss: 2.4175\n",
            "[MLP+MFCC] Epoch [9/10], Batch [380/892], Loss: 2.4138\n",
            "[MLP+MFCC] Epoch [9/10], Batch [390/892], Loss: 2.3928\n",
            "[MLP+MFCC] Epoch [9/10], Batch [400/892], Loss: 2.3921\n",
            "[MLP+MFCC] Epoch [9/10], Batch [410/892], Loss: 2.4425\n",
            "[MLP+MFCC] Epoch [9/10], Batch [420/892], Loss: 2.3991\n",
            "[MLP+MFCC] Epoch [9/10], Batch [430/892], Loss: 2.3961\n",
            "[MLP+MFCC] Epoch [9/10], Batch [440/892], Loss: 2.3838\n",
            "[MLP+MFCC] Epoch [9/10], Batch [450/892], Loss: 2.4389\n",
            "[MLP+MFCC] Epoch [9/10], Batch [460/892], Loss: 2.3692\n",
            "[MLP+MFCC] Epoch [9/10], Batch [470/892], Loss: 2.3798\n",
            "[MLP+MFCC] Epoch [9/10], Batch [480/892], Loss: 2.3892\n",
            "[MLP+MFCC] Epoch [9/10], Batch [490/892], Loss: 2.4186\n",
            "[MLP+MFCC] Epoch [9/10], Batch [500/892], Loss: 2.4168\n",
            "[MLP+MFCC] Epoch [9/10], Batch [510/892], Loss: 2.3670\n",
            "[MLP+MFCC] Epoch [9/10], Batch [520/892], Loss: 2.4111\n",
            "[MLP+MFCC] Epoch [9/10], Batch [530/892], Loss: 2.3932\n",
            "[MLP+MFCC] Epoch [9/10], Batch [540/892], Loss: 2.3827\n",
            "[MLP+MFCC] Epoch [9/10], Batch [550/892], Loss: 2.3937\n",
            "[MLP+MFCC] Epoch [9/10], Batch [560/892], Loss: 2.4073\n",
            "[MLP+MFCC] Epoch [9/10], Batch [570/892], Loss: 2.3991\n",
            "[MLP+MFCC] Epoch [9/10], Batch [580/892], Loss: 2.4045\n",
            "[MLP+MFCC] Epoch [9/10], Batch [590/892], Loss: 2.3871\n",
            "[MLP+MFCC] Epoch [9/10], Batch [600/892], Loss: 2.4049\n",
            "[MLP+MFCC] Epoch [9/10], Batch [610/892], Loss: 2.4470\n",
            "[MLP+MFCC] Epoch [9/10], Batch [620/892], Loss: 2.4211\n",
            "[MLP+MFCC] Epoch [9/10], Batch [630/892], Loss: 2.4026\n",
            "[MLP+MFCC] Epoch [9/10], Batch [640/892], Loss: 2.3962\n",
            "[MLP+MFCC] Epoch [9/10], Batch [650/892], Loss: 2.3970\n",
            "[MLP+MFCC] Epoch [9/10], Batch [660/892], Loss: 2.4068\n",
            "[MLP+MFCC] Epoch [9/10], Batch [670/892], Loss: 2.3972\n",
            "[MLP+MFCC] Epoch [9/10], Batch [680/892], Loss: 2.4000\n",
            "[MLP+MFCC] Epoch [9/10], Batch [690/892], Loss: 2.3783\n",
            "[MLP+MFCC] Epoch [9/10], Batch [700/892], Loss: 2.4152\n",
            "[MLP+MFCC] Epoch [9/10], Batch [710/892], Loss: 2.3874\n",
            "[MLP+MFCC] Epoch [9/10], Batch [720/892], Loss: 2.3989\n",
            "[MLP+MFCC] Epoch [9/10], Batch [730/892], Loss: 2.4202\n",
            "[MLP+MFCC] Epoch [9/10], Batch [740/892], Loss: 2.4147\n",
            "[MLP+MFCC] Epoch [9/10], Batch [750/892], Loss: 2.4042\n",
            "[MLP+MFCC] Epoch [9/10], Batch [760/892], Loss: 2.4481\n",
            "[MLP+MFCC] Epoch [9/10], Batch [770/892], Loss: 2.4193\n",
            "[MLP+MFCC] Epoch [9/10], Batch [780/892], Loss: 2.4001\n",
            "[MLP+MFCC] Epoch [9/10], Batch [790/892], Loss: 2.4153\n",
            "[MLP+MFCC] Epoch [9/10], Batch [800/892], Loss: 2.3601\n",
            "[MLP+MFCC] Epoch [9/10], Batch [810/892], Loss: 2.4264\n",
            "[MLP+MFCC] Epoch [9/10], Batch [820/892], Loss: 2.3859\n",
            "[MLP+MFCC] Epoch [9/10], Batch [830/892], Loss: 2.4531\n",
            "[MLP+MFCC] Epoch [9/10], Batch [840/892], Loss: 2.3663\n",
            "[MLP+MFCC] Epoch [9/10], Batch [850/892], Loss: 2.4377\n",
            "[MLP+MFCC] Epoch [9/10], Batch [860/892], Loss: 2.3961\n",
            "[MLP+MFCC] Epoch [9/10], Batch [870/892], Loss: 2.4182\n",
            "[MLP+MFCC] Epoch [9/10], Batch [880/892], Loss: 2.3833\n",
            "[MLP+MFCC] Epoch [9/10], Batch [890/892], Loss: 2.3928\n",
            "[MLP+MFCC] Epoch [9/10] completed, Average Loss: 2.4059\n",
            "[MLP+MFCC] Epoch [10/10], Batch [10/892], Loss: 2.3997\n",
            "[MLP+MFCC] Epoch [10/10], Batch [20/892], Loss: 2.3866\n",
            "[MLP+MFCC] Epoch [10/10], Batch [30/892], Loss: 2.3831\n",
            "[MLP+MFCC] Epoch [10/10], Batch [40/892], Loss: 2.4078\n",
            "[MLP+MFCC] Epoch [10/10], Batch [50/892], Loss: 2.4100\n",
            "[MLP+MFCC] Epoch [10/10], Batch [60/892], Loss: 2.4208\n",
            "[MLP+MFCC] Epoch [10/10], Batch [70/892], Loss: 2.3776\n",
            "[MLP+MFCC] Epoch [10/10], Batch [80/892], Loss: 2.4222\n",
            "[MLP+MFCC] Epoch [10/10], Batch [90/892], Loss: 2.3722\n",
            "[MLP+MFCC] Epoch [10/10], Batch [100/892], Loss: 2.3878\n",
            "[MLP+MFCC] Epoch [10/10], Batch [110/892], Loss: 2.4319\n",
            "[MLP+MFCC] Epoch [10/10], Batch [120/892], Loss: 2.4184\n",
            "[MLP+MFCC] Epoch [10/10], Batch [130/892], Loss: 2.3917\n",
            "[MLP+MFCC] Epoch [10/10], Batch [140/892], Loss: 2.4072\n",
            "[MLP+MFCC] Epoch [10/10], Batch [150/892], Loss: 2.3939\n",
            "[MLP+MFCC] Epoch [10/10], Batch [160/892], Loss: 2.3712\n",
            "[MLP+MFCC] Epoch [10/10], Batch [170/892], Loss: 2.3665\n",
            "[MLP+MFCC] Epoch [10/10], Batch [180/892], Loss: 2.4169\n",
            "[MLP+MFCC] Epoch [10/10], Batch [190/892], Loss: 2.3801\n",
            "[MLP+MFCC] Epoch [10/10], Batch [200/892], Loss: 2.3913\n",
            "[MLP+MFCC] Epoch [10/10], Batch [210/892], Loss: 2.3722\n",
            "[MLP+MFCC] Epoch [10/10], Batch [220/892], Loss: 2.3660\n",
            "[MLP+MFCC] Epoch [10/10], Batch [230/892], Loss: 2.4080\n",
            "[MLP+MFCC] Epoch [10/10], Batch [240/892], Loss: 2.4114\n",
            "[MLP+MFCC] Epoch [10/10], Batch [250/892], Loss: 2.4213\n",
            "[MLP+MFCC] Epoch [10/10], Batch [260/892], Loss: 2.4212\n",
            "[MLP+MFCC] Epoch [10/10], Batch [270/892], Loss: 2.4501\n",
            "[MLP+MFCC] Epoch [10/10], Batch [280/892], Loss: 2.4135\n",
            "[MLP+MFCC] Epoch [10/10], Batch [290/892], Loss: 2.3993\n",
            "[MLP+MFCC] Epoch [10/10], Batch [300/892], Loss: 2.4153\n",
            "[MLP+MFCC] Epoch [10/10], Batch [310/892], Loss: 2.3883\n",
            "[MLP+MFCC] Epoch [10/10], Batch [320/892], Loss: 2.4012\n",
            "[MLP+MFCC] Epoch [10/10], Batch [330/892], Loss: 2.3825\n",
            "[MLP+MFCC] Epoch [10/10], Batch [340/892], Loss: 2.4151\n",
            "[MLP+MFCC] Epoch [10/10], Batch [350/892], Loss: 2.3799\n",
            "[MLP+MFCC] Epoch [10/10], Batch [360/892], Loss: 2.4073\n",
            "[MLP+MFCC] Epoch [10/10], Batch [370/892], Loss: 2.4131\n",
            "[MLP+MFCC] Epoch [10/10], Batch [380/892], Loss: 2.4238\n",
            "[MLP+MFCC] Epoch [10/10], Batch [390/892], Loss: 2.3816\n",
            "[MLP+MFCC] Epoch [10/10], Batch [400/892], Loss: 2.4167\n",
            "[MLP+MFCC] Epoch [10/10], Batch [410/892], Loss: 2.4030\n",
            "[MLP+MFCC] Epoch [10/10], Batch [420/892], Loss: 2.3673\n",
            "[MLP+MFCC] Epoch [10/10], Batch [430/892], Loss: 2.4076\n",
            "[MLP+MFCC] Epoch [10/10], Batch [440/892], Loss: 2.4284\n",
            "[MLP+MFCC] Epoch [10/10], Batch [450/892], Loss: 2.4463\n",
            "[MLP+MFCC] Epoch [10/10], Batch [460/892], Loss: 2.3949\n",
            "[MLP+MFCC] Epoch [10/10], Batch [470/892], Loss: 2.4226\n",
            "[MLP+MFCC] Epoch [10/10], Batch [480/892], Loss: 2.4066\n",
            "[MLP+MFCC] Epoch [10/10], Batch [490/892], Loss: 2.3800\n",
            "[MLP+MFCC] Epoch [10/10], Batch [500/892], Loss: 2.3787\n",
            "[MLP+MFCC] Epoch [10/10], Batch [510/892], Loss: 2.4044\n",
            "[MLP+MFCC] Epoch [10/10], Batch [520/892], Loss: 2.3936\n",
            "[MLP+MFCC] Epoch [10/10], Batch [530/892], Loss: 2.4094\n",
            "[MLP+MFCC] Epoch [10/10], Batch [540/892], Loss: 2.3623\n",
            "[MLP+MFCC] Epoch [10/10], Batch [550/892], Loss: 2.3912\n",
            "[MLP+MFCC] Epoch [10/10], Batch [560/892], Loss: 2.4075\n",
            "[MLP+MFCC] Epoch [10/10], Batch [570/892], Loss: 2.4163\n",
            "[MLP+MFCC] Epoch [10/10], Batch [580/892], Loss: 2.3878\n",
            "[MLP+MFCC] Epoch [10/10], Batch [590/892], Loss: 2.3859\n",
            "[MLP+MFCC] Epoch [10/10], Batch [600/892], Loss: 2.3621\n",
            "[MLP+MFCC] Epoch [10/10], Batch [610/892], Loss: 2.4544\n",
            "[MLP+MFCC] Epoch [10/10], Batch [620/892], Loss: 2.4085\n",
            "[MLP+MFCC] Epoch [10/10], Batch [630/892], Loss: 2.3955\n",
            "[MLP+MFCC] Epoch [10/10], Batch [640/892], Loss: 2.4307\n",
            "[MLP+MFCC] Epoch [10/10], Batch [650/892], Loss: 2.4129\n",
            "[MLP+MFCC] Epoch [10/10], Batch [660/892], Loss: 2.4021\n",
            "[MLP+MFCC] Epoch [10/10], Batch [670/892], Loss: 2.4174\n",
            "[MLP+MFCC] Epoch [10/10], Batch [680/892], Loss: 2.3705\n",
            "[MLP+MFCC] Epoch [10/10], Batch [690/892], Loss: 2.4419\n",
            "[MLP+MFCC] Epoch [10/10], Batch [700/892], Loss: 2.4336\n",
            "[MLP+MFCC] Epoch [10/10], Batch [710/892], Loss: 2.4016\n",
            "[MLP+MFCC] Epoch [10/10], Batch [720/892], Loss: 2.3135\n",
            "[MLP+MFCC] Epoch [10/10], Batch [730/892], Loss: 2.4089\n",
            "[MLP+MFCC] Epoch [10/10], Batch [740/892], Loss: 2.3727\n",
            "[MLP+MFCC] Epoch [10/10], Batch [750/892], Loss: 2.3666\n",
            "[MLP+MFCC] Epoch [10/10], Batch [760/892], Loss: 2.3816\n",
            "[MLP+MFCC] Epoch [10/10], Batch [770/892], Loss: 2.4405\n",
            "[MLP+MFCC] Epoch [10/10], Batch [780/892], Loss: 2.3925\n",
            "[MLP+MFCC] Epoch [10/10], Batch [790/892], Loss: 2.3915\n",
            "[MLP+MFCC] Epoch [10/10], Batch [800/892], Loss: 2.3917\n",
            "[MLP+MFCC] Epoch [10/10], Batch [810/892], Loss: 2.4018\n",
            "[MLP+MFCC] Epoch [10/10], Batch [820/892], Loss: 2.4053\n",
            "[MLP+MFCC] Epoch [10/10], Batch [830/892], Loss: 2.4139\n",
            "[MLP+MFCC] Epoch [10/10], Batch [840/892], Loss: 2.4012\n",
            "[MLP+MFCC] Epoch [10/10], Batch [850/892], Loss: 2.4153\n",
            "[MLP+MFCC] Epoch [10/10], Batch [860/892], Loss: 2.3798\n",
            "[MLP+MFCC] Epoch [10/10], Batch [870/892], Loss: 2.3699\n",
            "[MLP+MFCC] Epoch [10/10], Batch [880/892], Loss: 2.4223\n",
            "[MLP+MFCC] Epoch [10/10], Batch [890/892], Loss: 2.3782\n",
            "[MLP+MFCC] Epoch [10/10] completed, Average Loss: 2.3997\n"
          ]
        }
      ],
      "source": [
        "if 'model_mlp' in locals() and model_mlp is not None and dataloader_mfcc is not None:\n",
        "    model_mlp.train()\n",
        "    train_losses_mlp = []\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_idx, (mfccs, texts, mfcc_lengths, text_lengths) in enumerate(dataloader_mfcc):\n",
        "            mfccs = mfccs.to(device)\n",
        "            texts = texts.to(device)\n",
        "            mfcc_lengths = mfcc_lengths.to(device)\n",
        "            text_lengths = text_lengths.to(device)\n",
        "            \n",
        "            optimizer_mlp.zero_grad()\n",
        "            \n",
        "            logits = model_mlp(mfccs)\n",
        "            log_probs = nn.functional.log_softmax(logits, dim=2)\n",
        "            log_probs = log_probs.transpose(0, 1)\n",
        "            \n",
        "            loss = criterion_mfcc(log_probs, texts, mfcc_lengths, text_lengths)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer_mlp.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            \n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f'[MLP+MFCC] Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{batch_idx+1}/{len(dataloader_mfcc)}], Loss: {loss.item():.4f}')\n",
        "        \n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        train_losses_mlp.append(avg_loss)\n",
        "        print(f'[MLP+MFCC] Epoch [{epoch+1}/{NUM_EPOCHS}] completed, Average Loss: {avg_loss:.4f}')\n",
        "    \n",
        "    torch.save({\n",
        "        'model_state_dict': model_mlp.state_dict(),\n",
        "        'encoder': encoder_mfcc,\n",
        "        'vocab_size': vocab_size_mfcc,\n",
        "        'config': {\n",
        "            'n_mfcc': N_MFCC,\n",
        "            'hidden_size': HIDDEN_SIZE,\n",
        "            'num_layers': NUM_LAYERS,\n",
        "            'dropout': DROPOUT\n",
        "        }\n",
        "    }, 'checkpoint_mlp_mfcc.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Partie 2 - Training CNN + MLP + MelSpectrogram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if 'train_losses_mlp' in locals():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses_mlp)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Partie 1 - MLP + MFCC + CTC : Training Loss')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Chargement LibriSpeech pour Partie 2 (CNN + MLP + MelSpectrogram)...\n",
            " Chargé 28539 échantillons depuis train-clean-100\n",
            " Réutilisation de l'encoder de la Partie 1\n",
            "Modèle CNN+MLP créé: 278621 paramètres\n",
            " Vocabulaire: 29 caractères\n",
            "Dataset: 28539 échantillons\n"
          ]
        }
      ],
      "source": [
        "\n",
        "librispeech_dir = 'data/LibriSpeech'\n",
        "subset = 'train-clean-100'\n",
        "\n",
        "print(\" Chargement LibriSpeech pour Partie 2 (CNN + MLP + MelSpectrogram)...\")\n",
        "dataset = LibriSpeechDataset(librispeech_dir, None, subset=subset, n_mfcc=None)\n",
        "\n",
        "if len(dataset.data) > 0:\n",
        "    if 'encoder_mfcc' in locals():\n",
        "        encoder = encoder_mfcc\n",
        "        vocab_size = encoder.get_vocab_size()\n",
        "        print(\" Réutilisation de l'encoder de la Partie 1\")\n",
        "    else:\n",
        "        texts = [transcription for _, transcription in dataset.data]\n",
        "        encoder = CharEncoder()\n",
        "        encoder.build_vocab(texts)\n",
        "        vocab_size = encoder.get_vocab_size()\n",
        "        print(\" Nouvel encoder créé\")\n",
        "    \n",
        "    dataset.encoder = encoder\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    \n",
        "    model = CNNMLPASR(\n",
        "        input_size=N_MELS,\n",
        "        vocab_size=vocab_size,\n",
        "        conv_channels=[64, 128, 256],\n",
        "        kernel_sizes=[3, 3, 3],\n",
        "    ).to(device)\n",
        "    \n",
        "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    print(f\"Modèle CNN+MLP créé: {sum(p.numel() for p in model.parameters())} paramètres\")\n",
        "    print(f\" Vocabulaire: {vocab_size} caractères\")\n",
        "    print(f\"Dataset: {len(dataset)} échantillons\")\n",
        "else:\n",
        "    print(\" Aucun échantillon trouvé dans LibriSpeech!\")\n",
        "    print(f\"   Vérifiez que {librispeech_dir}/{subset} existe et contient des fichiers .flac\")\n",
        "    dataloader = None\n",
        "    model = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Batch [10/892], Loss: 3.5459\n",
            "Epoch [1/10], Batch [20/892], Loss: 3.1387\n",
            "Epoch [1/10], Batch [30/892], Loss: 2.9756\n",
            "Epoch [1/10], Batch [40/892], Loss: 2.8890\n",
            "Epoch [1/10], Batch [50/892], Loss: 2.9099\n",
            "Epoch [1/10], Batch [60/892], Loss: 2.8715\n",
            "Epoch [1/10], Batch [70/892], Loss: 2.8826\n",
            "Epoch [1/10], Batch [80/892], Loss: 2.8872\n",
            "Epoch [1/10], Batch [90/892], Loss: 2.8612\n",
            "Epoch [1/10], Batch [100/892], Loss: 2.8572\n",
            "Epoch [1/10], Batch [110/892], Loss: 2.8555\n",
            "Epoch [1/10], Batch [120/892], Loss: 2.8753\n",
            "Epoch [1/10], Batch [130/892], Loss: 2.8597\n",
            "Epoch [1/10], Batch [140/892], Loss: 2.8884\n",
            "Epoch [1/10], Batch [150/892], Loss: 2.8839\n",
            "Epoch [1/10], Batch [160/892], Loss: 2.8677\n",
            "Epoch [1/10], Batch [170/892], Loss: 2.8676\n",
            "Epoch [1/10], Batch [180/892], Loss: 2.8533\n",
            "Epoch [1/10], Batch [190/892], Loss: 2.8581\n",
            "Epoch [1/10], Batch [200/892], Loss: 2.8784\n",
            "Epoch [1/10], Batch [210/892], Loss: 2.8412\n",
            "Epoch [1/10], Batch [220/892], Loss: 2.8603\n",
            "Epoch [1/10], Batch [230/892], Loss: 2.8827\n",
            "Epoch [1/10], Batch [240/892], Loss: 2.8573\n",
            "Epoch [1/10], Batch [250/892], Loss: 2.8599\n",
            "Epoch [1/10], Batch [260/892], Loss: 2.8707\n",
            "Epoch [1/10], Batch [270/892], Loss: 2.8715\n",
            "Epoch [1/10], Batch [280/892], Loss: 2.8548\n",
            "Epoch [1/10], Batch [290/892], Loss: 2.8654\n",
            "Epoch [1/10], Batch [300/892], Loss: 2.8378\n",
            "Epoch [1/10], Batch [310/892], Loss: 2.8428\n",
            "Epoch [1/10], Batch [320/892], Loss: 2.8443\n",
            "Epoch [1/10], Batch [330/892], Loss: 2.8499\n",
            "Epoch [1/10], Batch [340/892], Loss: 2.8477\n",
            "Epoch [1/10], Batch [350/892], Loss: 2.8405\n",
            "Epoch [1/10], Batch [360/892], Loss: 2.8520\n",
            "Epoch [1/10], Batch [370/892], Loss: 2.8649\n",
            "Epoch [1/10], Batch [380/892], Loss: 2.8732\n",
            "Epoch [1/10], Batch [390/892], Loss: 2.8518\n",
            "Epoch [1/10], Batch [400/892], Loss: 2.8289\n",
            "Epoch [1/10], Batch [410/892], Loss: 2.8377\n",
            "Epoch [1/10], Batch [420/892], Loss: 2.8576\n",
            "Epoch [1/10], Batch [430/892], Loss: 2.8214\n",
            "Epoch [1/10], Batch [440/892], Loss: 2.8038\n",
            "Epoch [1/10], Batch [450/892], Loss: 2.7981\n",
            "Epoch [1/10], Batch [460/892], Loss: 2.7891\n",
            "Epoch [1/10], Batch [470/892], Loss: 2.7299\n",
            "Epoch [1/10], Batch [480/892], Loss: 2.7464\n",
            "Epoch [1/10], Batch [490/892], Loss: 2.7350\n",
            "Epoch [1/10], Batch [500/892], Loss: 2.7382\n",
            "Epoch [1/10], Batch [510/892], Loss: 2.6929\n",
            "Epoch [1/10], Batch [520/892], Loss: 2.7140\n",
            "Epoch [1/10], Batch [530/892], Loss: 2.6489\n",
            "Epoch [1/10], Batch [540/892], Loss: 2.6499\n",
            "Epoch [1/10], Batch [550/892], Loss: 2.5947\n",
            "Epoch [1/10], Batch [560/892], Loss: 2.6134\n",
            "Epoch [1/10], Batch [570/892], Loss: 2.5741\n",
            "Epoch [1/10], Batch [580/892], Loss: 2.5794\n",
            "Epoch [1/10], Batch [590/892], Loss: 2.5096\n",
            "Epoch [1/10], Batch [600/892], Loss: 2.5203\n",
            "Epoch [1/10], Batch [610/892], Loss: 2.5390\n",
            "Epoch [1/10], Batch [620/892], Loss: 2.5010\n",
            "Epoch [1/10], Batch [630/892], Loss: 2.4905\n",
            "Epoch [1/10], Batch [640/892], Loss: 2.4814\n",
            "Epoch [1/10], Batch [650/892], Loss: 2.5003\n",
            "Epoch [1/10], Batch [660/892], Loss: 2.4695\n",
            "Epoch [1/10], Batch [670/892], Loss: 2.4300\n",
            "Epoch [1/10], Batch [680/892], Loss: 2.4567\n",
            "Epoch [1/10], Batch [690/892], Loss: 2.3886\n",
            "Epoch [1/10], Batch [700/892], Loss: 2.3262\n",
            "Epoch [1/10], Batch [710/892], Loss: 2.3539\n",
            "Epoch [1/10], Batch [720/892], Loss: 2.3818\n",
            "Epoch [1/10], Batch [730/892], Loss: 2.3702\n",
            "Epoch [1/10], Batch [740/892], Loss: 2.3255\n",
            "Epoch [1/10], Batch [750/892], Loss: 2.3622\n",
            "Epoch [1/10], Batch [760/892], Loss: 2.3087\n",
            "Epoch [1/10], Batch [770/892], Loss: 2.3349\n",
            "Epoch [1/10], Batch [780/892], Loss: 2.3249\n",
            "Epoch [1/10], Batch [790/892], Loss: 2.2942\n",
            "Epoch [1/10], Batch [800/892], Loss: 2.2860\n",
            "Epoch [1/10], Batch [810/892], Loss: 2.2608\n",
            "Epoch [1/10], Batch [820/892], Loss: 2.2715\n",
            "Epoch [1/10], Batch [830/892], Loss: 2.2101\n",
            "Epoch [1/10], Batch [840/892], Loss: 2.2498\n",
            "Epoch [1/10], Batch [850/892], Loss: 2.2506\n",
            "Epoch [1/10], Batch [860/892], Loss: 2.2710\n",
            "Epoch [1/10], Batch [870/892], Loss: 2.2390\n",
            "Epoch [1/10], Batch [880/892], Loss: 2.2193\n",
            "Epoch [1/10], Batch [890/892], Loss: 2.2499\n",
            "Epoch [1/10] completed, Average Loss: 2.7478\n",
            "Epoch [2/10], Batch [10/892], Loss: 2.2111\n",
            "Epoch [2/10], Batch [20/892], Loss: 2.1984\n",
            "Epoch [2/10], Batch [30/892], Loss: 2.2064\n",
            "Epoch [2/10], Batch [40/892], Loss: 2.1653\n",
            "Epoch [2/10], Batch [50/892], Loss: 2.2220\n",
            "Epoch [2/10], Batch [60/892], Loss: 2.1702\n",
            "Epoch [2/10], Batch [70/892], Loss: 2.1955\n",
            "Epoch [2/10], Batch [80/892], Loss: 2.2114\n",
            "Epoch [2/10], Batch [90/892], Loss: 2.1304\n",
            "Epoch [2/10], Batch [100/892], Loss: 2.1323\n",
            "Epoch [2/10], Batch [110/892], Loss: 2.1380\n",
            "Epoch [2/10], Batch [120/892], Loss: 2.1811\n",
            "Epoch [2/10], Batch [130/892], Loss: 2.1580\n",
            "Epoch [2/10], Batch [140/892], Loss: 2.1338\n",
            "Epoch [2/10], Batch [150/892], Loss: 2.1487\n",
            "Epoch [2/10], Batch [160/892], Loss: 2.1563\n",
            "Epoch [2/10], Batch [170/892], Loss: 2.1870\n",
            "Epoch [2/10], Batch [180/892], Loss: 2.1488\n",
            "Epoch [2/10], Batch [190/892], Loss: 2.1441\n",
            "Epoch [2/10], Batch [200/892], Loss: 2.1108\n",
            "Epoch [2/10], Batch [210/892], Loss: 2.1127\n",
            "Epoch [2/10], Batch [220/892], Loss: 2.0858\n",
            "Epoch [2/10], Batch [230/892], Loss: 2.0800\n",
            "Epoch [2/10], Batch [240/892], Loss: 2.1067\n",
            "Epoch [2/10], Batch [250/892], Loss: 2.1533\n",
            "Epoch [2/10], Batch [260/892], Loss: 2.1596\n",
            "Epoch [2/10], Batch [270/892], Loss: 2.1575\n",
            "Epoch [2/10], Batch [280/892], Loss: 2.1257\n",
            "Epoch [2/10], Batch [290/892], Loss: 2.1522\n",
            "Epoch [2/10], Batch [300/892], Loss: 2.0752\n",
            "Epoch [2/10], Batch [310/892], Loss: 2.0585\n",
            "Epoch [2/10], Batch [320/892], Loss: 2.0861\n",
            "Epoch [2/10], Batch [330/892], Loss: 2.0267\n",
            "Epoch [2/10], Batch [340/892], Loss: 2.0801\n",
            "Epoch [2/10], Batch [350/892], Loss: 2.0883\n",
            "Epoch [2/10], Batch [360/892], Loss: 2.0910\n",
            "Epoch [2/10], Batch [370/892], Loss: 2.1442\n",
            "Epoch [2/10], Batch [380/892], Loss: 2.0489\n",
            "Epoch [2/10], Batch [390/892], Loss: 2.1033\n",
            "Epoch [2/10], Batch [400/892], Loss: 2.0947\n",
            "Epoch [2/10], Batch [410/892], Loss: 2.0010\n",
            "Epoch [2/10], Batch [420/892], Loss: 2.0836\n",
            "Epoch [2/10], Batch [430/892], Loss: 2.0674\n",
            "Epoch [2/10], Batch [440/892], Loss: 2.0400\n",
            "Epoch [2/10], Batch [450/892], Loss: 2.0038\n",
            "Epoch [2/10], Batch [460/892], Loss: 2.0295\n",
            "Epoch [2/10], Batch [470/892], Loss: 2.0186\n",
            "Epoch [2/10], Batch [480/892], Loss: 2.0651\n",
            "Epoch [2/10], Batch [490/892], Loss: 2.0487\n",
            "Epoch [2/10], Batch [500/892], Loss: 2.0338\n",
            "Epoch [2/10], Batch [510/892], Loss: 2.0305\n",
            "Epoch [2/10], Batch [520/892], Loss: 1.9985\n",
            "Epoch [2/10], Batch [530/892], Loss: 2.0246\n",
            "Epoch [2/10], Batch [540/892], Loss: 2.0748\n",
            "Epoch [2/10], Batch [550/892], Loss: 2.0192\n",
            "Epoch [2/10], Batch [560/892], Loss: 2.0255\n",
            "Epoch [2/10], Batch [570/892], Loss: 2.0709\n",
            "Epoch [2/10], Batch [580/892], Loss: 2.0477\n",
            "Epoch [2/10], Batch [590/892], Loss: 1.9615\n",
            "Epoch [2/10], Batch [600/892], Loss: 2.0136\n",
            "Epoch [2/10], Batch [610/892], Loss: 1.9966\n",
            "Epoch [2/10], Batch [620/892], Loss: 2.0306\n",
            "Epoch [2/10], Batch [630/892], Loss: 2.0207\n",
            "Epoch [2/10], Batch [640/892], Loss: 2.0077\n",
            "Epoch [2/10], Batch [650/892], Loss: 1.9662\n",
            "Epoch [2/10], Batch [660/892], Loss: 1.9797\n",
            "Epoch [2/10], Batch [670/892], Loss: 2.0311\n",
            "Epoch [2/10], Batch [680/892], Loss: 1.9784\n",
            "Epoch [2/10], Batch [690/892], Loss: 1.9829\n",
            "Epoch [2/10], Batch [700/892], Loss: 1.9962\n",
            "Epoch [2/10], Batch [710/892], Loss: 2.0237\n",
            "Epoch [2/10], Batch [720/892], Loss: 2.0065\n",
            "Epoch [2/10], Batch [730/892], Loss: 1.9981\n",
            "Epoch [2/10], Batch [740/892], Loss: 1.9814\n",
            "Epoch [2/10], Batch [750/892], Loss: 1.9994\n",
            "Epoch [2/10], Batch [760/892], Loss: 1.9622\n",
            "Epoch [2/10], Batch [770/892], Loss: 1.9834\n",
            "Epoch [2/10], Batch [780/892], Loss: 1.9670\n",
            "Epoch [2/10], Batch [790/892], Loss: 1.9957\n",
            "Epoch [2/10], Batch [800/892], Loss: 1.9959\n",
            "Epoch [2/10], Batch [810/892], Loss: 1.9855\n",
            "Epoch [2/10], Batch [820/892], Loss: 1.9953\n",
            "Epoch [2/10], Batch [830/892], Loss: 1.9951\n",
            "Epoch [2/10], Batch [840/892], Loss: 1.9772\n",
            "Epoch [2/10], Batch [850/892], Loss: 1.9746\n",
            "Epoch [2/10], Batch [860/892], Loss: 1.9489\n",
            "Epoch [2/10], Batch [870/892], Loss: 2.0160\n",
            "Epoch [2/10], Batch [880/892], Loss: 2.0026\n",
            "Epoch [2/10], Batch [890/892], Loss: 1.9501\n",
            "Epoch [2/10] completed, Average Loss: 2.0600\n",
            "Epoch [3/10], Batch [10/892], Loss: 1.9587\n",
            "Epoch [3/10], Batch [20/892], Loss: 1.9194\n",
            "Epoch [3/10], Batch [30/892], Loss: 1.9564\n",
            "Epoch [3/10], Batch [40/892], Loss: 2.0021\n",
            "Epoch [3/10], Batch [50/892], Loss: 1.9209\n",
            "Epoch [3/10], Batch [60/892], Loss: 1.9761\n",
            "Epoch [3/10], Batch [70/892], Loss: 1.9010\n",
            "Epoch [3/10], Batch [80/892], Loss: 1.9437\n",
            "Epoch [3/10], Batch [90/892], Loss: 1.9983\n",
            "Epoch [3/10], Batch [100/892], Loss: 1.9481\n",
            "Epoch [3/10], Batch [110/892], Loss: 1.9062\n",
            "Epoch [3/10], Batch [120/892], Loss: 1.9543\n",
            "Epoch [3/10], Batch [130/892], Loss: 1.9688\n",
            "Epoch [3/10], Batch [140/892], Loss: 1.8963\n",
            "Epoch [3/10], Batch [150/892], Loss: 1.9054\n",
            "Epoch [3/10], Batch [160/892], Loss: 1.9172\n",
            "Epoch [3/10], Batch [170/892], Loss: 1.8680\n",
            "Epoch [3/10], Batch [180/892], Loss: 1.9399\n",
            "Epoch [3/10], Batch [190/892], Loss: 1.9097\n",
            "Epoch [3/10], Batch [200/892], Loss: 1.9273\n",
            "Epoch [3/10], Batch [210/892], Loss: 1.8866\n",
            "Epoch [3/10], Batch [220/892], Loss: 1.9699\n",
            "Epoch [3/10], Batch [230/892], Loss: 1.9363\n",
            "Epoch [3/10], Batch [240/892], Loss: 1.9552\n",
            "Epoch [3/10], Batch [250/892], Loss: 1.8610\n",
            "Epoch [3/10], Batch [260/892], Loss: 1.9128\n",
            "Epoch [3/10], Batch [270/892], Loss: 1.8752\n",
            "Epoch [3/10], Batch [280/892], Loss: 1.9502\n",
            "Epoch [3/10], Batch [290/892], Loss: 1.9091\n",
            "Epoch [3/10], Batch [300/892], Loss: 1.9063\n",
            "Epoch [3/10], Batch [310/892], Loss: 2.0254\n",
            "Epoch [3/10], Batch [320/892], Loss: 1.9341\n",
            "Epoch [3/10], Batch [330/892], Loss: 1.9463\n",
            "Epoch [3/10], Batch [340/892], Loss: 1.8502\n",
            "Epoch [3/10], Batch [350/892], Loss: 1.8580\n",
            "Epoch [3/10], Batch [360/892], Loss: 1.9283\n",
            "Epoch [3/10], Batch [370/892], Loss: 1.9284\n",
            "Epoch [3/10], Batch [380/892], Loss: 1.8823\n",
            "Epoch [3/10], Batch [390/892], Loss: 1.9372\n",
            "Epoch [3/10], Batch [400/892], Loss: 1.8877\n",
            "Epoch [3/10], Batch [410/892], Loss: 1.8642\n",
            "Epoch [3/10], Batch [420/892], Loss: 1.9560\n",
            "Epoch [3/10], Batch [430/892], Loss: 1.9008\n",
            "Epoch [3/10], Batch [440/892], Loss: 1.9240\n",
            "Epoch [3/10], Batch [450/892], Loss: 1.9158\n",
            "Epoch [3/10], Batch [460/892], Loss: 1.8869\n",
            "Epoch [3/10], Batch [470/892], Loss: 1.9439\n",
            "Epoch [3/10], Batch [480/892], Loss: 1.8630\n",
            "Epoch [3/10], Batch [490/892], Loss: 1.9297\n",
            "Epoch [3/10], Batch [500/892], Loss: 1.9166\n",
            "Epoch [3/10], Batch [510/892], Loss: 1.8912\n",
            "Epoch [3/10], Batch [520/892], Loss: 1.8942\n",
            "Epoch [3/10], Batch [530/892], Loss: 1.9044\n",
            "Epoch [3/10], Batch [540/892], Loss: 1.8770\n",
            "Epoch [3/10], Batch [550/892], Loss: 1.8707\n",
            "Epoch [3/10], Batch [560/892], Loss: 1.8921\n",
            "Epoch [3/10], Batch [570/892], Loss: 1.9821\n",
            "Epoch [3/10], Batch [580/892], Loss: 1.8927\n",
            "Epoch [3/10], Batch [590/892], Loss: 1.9122\n",
            "Epoch [3/10], Batch [600/892], Loss: 1.9316\n",
            "Epoch [3/10], Batch [610/892], Loss: 1.8549\n",
            "Epoch [3/10], Batch [620/892], Loss: 1.9318\n",
            "Epoch [3/10], Batch [630/892], Loss: 1.8754\n",
            "Epoch [3/10], Batch [640/892], Loss: 1.8736\n",
            "Epoch [3/10], Batch [650/892], Loss: 1.9155\n",
            "Epoch [3/10], Batch [660/892], Loss: 1.9216\n",
            "Epoch [3/10], Batch [670/892], Loss: 1.8710\n",
            "Epoch [3/10], Batch [680/892], Loss: 1.8666\n",
            "Epoch [3/10], Batch [690/892], Loss: 1.9642\n",
            "Epoch [3/10], Batch [700/892], Loss: 1.8939\n",
            "Epoch [3/10], Batch [710/892], Loss: 1.8353\n",
            "Epoch [3/10], Batch [720/892], Loss: 1.8522\n",
            "Epoch [3/10], Batch [730/892], Loss: 1.8899\n",
            "Epoch [3/10], Batch [740/892], Loss: 1.8732\n",
            "Epoch [3/10], Batch [750/892], Loss: 1.8601\n",
            "Epoch [3/10], Batch [760/892], Loss: 1.8450\n",
            "Epoch [3/10], Batch [770/892], Loss: 1.8942\n",
            "Epoch [3/10], Batch [780/892], Loss: 1.9112\n",
            "Epoch [3/10], Batch [790/892], Loss: 1.9158\n",
            "Epoch [3/10], Batch [800/892], Loss: 1.8972\n",
            "Epoch [3/10], Batch [810/892], Loss: 1.8829\n",
            "Epoch [3/10], Batch [820/892], Loss: 1.8717\n",
            "Epoch [3/10], Batch [830/892], Loss: 1.8753\n",
            "Epoch [3/10], Batch [840/892], Loss: 1.8908\n",
            "Epoch [3/10], Batch [850/892], Loss: 1.8272\n",
            "Epoch [3/10], Batch [860/892], Loss: 1.9081\n",
            "Epoch [3/10], Batch [870/892], Loss: 1.8792\n",
            "Epoch [3/10], Batch [880/892], Loss: 1.8836\n",
            "Epoch [3/10], Batch [890/892], Loss: 1.8772\n",
            "Epoch [3/10] completed, Average Loss: 1.9120\n",
            "Epoch [4/10], Batch [10/892], Loss: 1.8361\n",
            "Epoch [4/10], Batch [20/892], Loss: 1.8475\n",
            "Epoch [4/10], Batch [30/892], Loss: 1.8836\n",
            "Epoch [4/10], Batch [40/892], Loss: 1.8340\n",
            "Epoch [4/10], Batch [50/892], Loss: 1.9051\n",
            "Epoch [4/10], Batch [60/892], Loss: 1.9108\n",
            "Epoch [4/10], Batch [70/892], Loss: 1.8519\n",
            "Epoch [4/10], Batch [80/892], Loss: 1.8737\n",
            "Epoch [4/10], Batch [90/892], Loss: 1.8390\n",
            "Epoch [4/10], Batch [100/892], Loss: 1.8232\n",
            "Epoch [4/10], Batch [110/892], Loss: 1.8671\n",
            "Epoch [4/10], Batch [120/892], Loss: 1.8375\n",
            "Epoch [4/10], Batch [130/892], Loss: 1.8737\n",
            "Epoch [4/10], Batch [140/892], Loss: 1.8223\n",
            "Epoch [4/10], Batch [150/892], Loss: 1.8268\n",
            "Epoch [4/10], Batch [160/892], Loss: 1.8488\n",
            "Epoch [4/10], Batch [170/892], Loss: 1.8687\n",
            "Epoch [4/10], Batch [180/892], Loss: 1.8912\n",
            "Epoch [4/10], Batch [190/892], Loss: 1.8470\n",
            "Epoch [4/10], Batch [200/892], Loss: 1.8552\n",
            "Epoch [4/10], Batch [210/892], Loss: 1.8475\n",
            "Epoch [4/10], Batch [220/892], Loss: 1.8873\n",
            "Epoch [4/10], Batch [230/892], Loss: 1.8377\n",
            "Epoch [4/10], Batch [240/892], Loss: 1.8003\n",
            "Epoch [4/10], Batch [250/892], Loss: 1.8883\n",
            "Epoch [4/10], Batch [260/892], Loss: 1.8393\n",
            "Epoch [4/10], Batch [270/892], Loss: 1.8232\n",
            "Epoch [4/10], Batch [280/892], Loss: 1.8216\n",
            "Epoch [4/10], Batch [290/892], Loss: 1.8641\n",
            "Epoch [4/10], Batch [300/892], Loss: 1.9013\n",
            "Epoch [4/10], Batch [310/892], Loss: 1.8286\n",
            "Epoch [4/10], Batch [320/892], Loss: 1.8627\n",
            "Epoch [4/10], Batch [330/892], Loss: 1.8649\n",
            "Epoch [4/10], Batch [340/892], Loss: 1.9036\n",
            "Epoch [4/10], Batch [350/892], Loss: 1.8331\n",
            "Epoch [4/10], Batch [360/892], Loss: 1.8105\n",
            "Epoch [4/10], Batch [370/892], Loss: 1.8210\n",
            "Epoch [4/10], Batch [380/892], Loss: 1.8448\n",
            "Epoch [4/10], Batch [390/892], Loss: 1.8585\n",
            "Epoch [4/10], Batch [400/892], Loss: 1.8644\n",
            "Epoch [4/10], Batch [410/892], Loss: 1.8972\n",
            "Epoch [4/10], Batch [420/892], Loss: 1.7991\n",
            "Epoch [4/10], Batch [430/892], Loss: 1.8941\n",
            "Epoch [4/10], Batch [440/892], Loss: 1.7750\n",
            "Epoch [4/10], Batch [450/892], Loss: 1.8499\n",
            "Epoch [4/10], Batch [460/892], Loss: 1.8762\n",
            "Epoch [4/10], Batch [470/892], Loss: 1.8213\n",
            "Epoch [4/10], Batch [480/892], Loss: 1.8602\n",
            "Epoch [4/10], Batch [490/892], Loss: 1.8869\n",
            "Epoch [4/10], Batch [500/892], Loss: 1.9234\n",
            "Epoch [4/10], Batch [510/892], Loss: 1.8319\n",
            "Epoch [4/10], Batch [520/892], Loss: 1.8447\n",
            "Epoch [4/10], Batch [530/892], Loss: 1.8513\n",
            "Epoch [4/10], Batch [540/892], Loss: 1.8296\n",
            "Epoch [4/10], Batch [550/892], Loss: 1.8490\n",
            "Epoch [4/10], Batch [560/892], Loss: 1.7921\n",
            "Epoch [4/10], Batch [570/892], Loss: 1.8031\n",
            "Epoch [4/10], Batch [580/892], Loss: 1.7863\n",
            "Epoch [4/10], Batch [590/892], Loss: 1.8668\n",
            "Epoch [4/10], Batch [600/892], Loss: 1.8206\n",
            "Epoch [4/10], Batch [610/892], Loss: 1.8504\n",
            "Epoch [4/10], Batch [620/892], Loss: 1.8541\n",
            "Epoch [4/10], Batch [630/892], Loss: 1.8561\n",
            "Epoch [4/10], Batch [640/892], Loss: 1.8779\n",
            "Epoch [4/10], Batch [650/892], Loss: 1.8606\n",
            "Epoch [4/10], Batch [660/892], Loss: 1.8529\n",
            "Epoch [4/10], Batch [670/892], Loss: 1.8821\n",
            "Epoch [4/10], Batch [680/892], Loss: 1.8476\n",
            "Epoch [4/10], Batch [690/892], Loss: 1.7900\n",
            "Epoch [4/10], Batch [700/892], Loss: 1.8160\n",
            "Epoch [4/10], Batch [710/892], Loss: 1.8396\n",
            "Epoch [4/10], Batch [720/892], Loss: 1.8190\n",
            "Epoch [4/10], Batch [730/892], Loss: 1.8505\n",
            "Epoch [4/10], Batch [740/892], Loss: 1.8277\n",
            "Epoch [4/10], Batch [750/892], Loss: 1.8485\n",
            "Epoch [4/10], Batch [760/892], Loss: 1.8577\n",
            "Epoch [4/10], Batch [770/892], Loss: 1.8862\n",
            "Epoch [4/10], Batch [780/892], Loss: 1.7999\n",
            "Epoch [4/10], Batch [790/892], Loss: 1.8447\n",
            "Epoch [4/10], Batch [800/892], Loss: 1.7224\n",
            "Epoch [4/10], Batch [810/892], Loss: 1.8140\n",
            "Epoch [4/10], Batch [820/892], Loss: 1.8319\n",
            "Epoch [4/10], Batch [830/892], Loss: 1.8214\n",
            "Epoch [4/10], Batch [840/892], Loss: 1.8205\n",
            "Epoch [4/10], Batch [850/892], Loss: 1.8309\n",
            "Epoch [4/10], Batch [860/892], Loss: 1.8125\n",
            "Epoch [4/10], Batch [870/892], Loss: 1.7967\n",
            "Epoch [4/10], Batch [880/892], Loss: 1.8645\n",
            "Epoch [4/10], Batch [890/892], Loss: 1.8109\n",
            "Epoch [4/10] completed, Average Loss: 1.8460\n",
            "Epoch [5/10], Batch [10/892], Loss: 1.7810\n",
            "Epoch [5/10], Batch [20/892], Loss: 1.8246\n",
            "Epoch [5/10], Batch [30/892], Loss: 1.8001\n",
            "Epoch [5/10], Batch [40/892], Loss: 1.8462\n",
            "Epoch [5/10], Batch [50/892], Loss: 1.8006\n",
            "Epoch [5/10], Batch [60/892], Loss: 1.8323\n",
            "Epoch [5/10], Batch [70/892], Loss: 1.8040\n",
            "Epoch [5/10], Batch [80/892], Loss: 1.8751\n",
            "Epoch [5/10], Batch [90/892], Loss: 1.8338\n",
            "Epoch [5/10], Batch [100/892], Loss: 1.7997\n",
            "Epoch [5/10], Batch [110/892], Loss: 1.7412\n",
            "Epoch [5/10], Batch [120/892], Loss: 1.8232\n",
            "Epoch [5/10], Batch [130/892], Loss: 1.8702\n",
            "Epoch [5/10], Batch [140/892], Loss: 1.8572\n",
            "Epoch [5/10], Batch [150/892], Loss: 1.8319\n",
            "Epoch [5/10], Batch [160/892], Loss: 1.7950\n",
            "Epoch [5/10], Batch [170/892], Loss: 1.7851\n",
            "Epoch [5/10], Batch [180/892], Loss: 1.7930\n",
            "Epoch [5/10], Batch [190/892], Loss: 1.7599\n",
            "Epoch [5/10], Batch [200/892], Loss: 1.7876\n",
            "Epoch [5/10], Batch [210/892], Loss: 1.7775\n",
            "Epoch [5/10], Batch [220/892], Loss: 1.8534\n",
            "Epoch [5/10], Batch [230/892], Loss: 1.7987\n",
            "Epoch [5/10], Batch [240/892], Loss: 1.8131\n",
            "Epoch [5/10], Batch [250/892], Loss: 1.8033\n",
            "Epoch [5/10], Batch [260/892], Loss: 1.8141\n",
            "Epoch [5/10], Batch [270/892], Loss: 1.7654\n",
            "Epoch [5/10], Batch [280/892], Loss: 1.8395\n",
            "Epoch [5/10], Batch [290/892], Loss: 1.7214\n",
            "Epoch [5/10], Batch [300/892], Loss: 1.8284\n",
            "Epoch [5/10], Batch [310/892], Loss: 1.7832\n",
            "Epoch [5/10], Batch [320/892], Loss: 1.8070\n",
            "Epoch [5/10], Batch [330/892], Loss: 1.8018\n",
            "Epoch [5/10], Batch [340/892], Loss: 1.8323\n",
            "Epoch [5/10], Batch [350/892], Loss: 1.8155\n",
            "Epoch [5/10], Batch [360/892], Loss: 1.8244\n",
            "Epoch [5/10], Batch [370/892], Loss: 1.8187\n",
            "Epoch [5/10], Batch [380/892], Loss: 1.8351\n",
            "Epoch [5/10], Batch [390/892], Loss: 1.8230\n",
            "Epoch [5/10], Batch [400/892], Loss: 1.7640\n",
            "Epoch [5/10], Batch [410/892], Loss: 1.8120\n",
            "Epoch [5/10], Batch [420/892], Loss: 1.7810\n",
            "Epoch [5/10], Batch [430/892], Loss: 1.8331\n",
            "Epoch [5/10], Batch [440/892], Loss: 1.8510\n",
            "Epoch [5/10], Batch [450/892], Loss: 1.7926\n",
            "Epoch [5/10], Batch [460/892], Loss: 1.7879\n",
            "Epoch [5/10], Batch [470/892], Loss: 1.8250\n",
            "Epoch [5/10], Batch [480/892], Loss: 1.8156\n",
            "Epoch [5/10], Batch [490/892], Loss: 1.8591\n",
            "Epoch [5/10], Batch [500/892], Loss: 1.8370\n",
            "Epoch [5/10], Batch [510/892], Loss: 1.7599\n",
            "Epoch [5/10], Batch [520/892], Loss: 1.8353\n",
            "Epoch [5/10], Batch [530/892], Loss: 1.8114\n",
            "Epoch [5/10], Batch [540/892], Loss: 1.8006\n",
            "Epoch [5/10], Batch [550/892], Loss: 1.8452\n",
            "Epoch [5/10], Batch [560/892], Loss: 1.7934\n",
            "Epoch [5/10], Batch [570/892], Loss: 1.8463\n",
            "Epoch [5/10], Batch [580/892], Loss: 1.8479\n",
            "Epoch [5/10], Batch [590/892], Loss: 1.7659\n",
            "Epoch [5/10], Batch [600/892], Loss: 1.7898\n",
            "Epoch [5/10], Batch [610/892], Loss: 1.8114\n",
            "Epoch [5/10], Batch [620/892], Loss: 1.8487\n",
            "Epoch [5/10], Batch [630/892], Loss: 1.7604\n",
            "Epoch [5/10], Batch [640/892], Loss: 1.7885\n",
            "Epoch [5/10], Batch [650/892], Loss: 1.7486\n",
            "Epoch [5/10], Batch [660/892], Loss: 1.7946\n",
            "Epoch [5/10], Batch [670/892], Loss: 1.7939\n",
            "Epoch [5/10], Batch [680/892], Loss: 1.8309\n",
            "Epoch [5/10], Batch [690/892], Loss: 1.7656\n",
            "Epoch [5/10], Batch [700/892], Loss: 1.8302\n",
            "Epoch [5/10], Batch [710/892], Loss: 1.8153\n",
            "Epoch [5/10], Batch [720/892], Loss: 1.8291\n",
            "Epoch [5/10], Batch [730/892], Loss: 1.8173\n",
            "Epoch [5/10], Batch [740/892], Loss: 1.8119\n",
            "Epoch [5/10], Batch [750/892], Loss: 1.8216\n",
            "Epoch [5/10], Batch [760/892], Loss: 1.7829\n",
            "Epoch [5/10], Batch [770/892], Loss: 1.7566\n",
            "Epoch [5/10], Batch [780/892], Loss: 1.7284\n",
            "Epoch [5/10], Batch [790/892], Loss: 1.7689\n",
            "Epoch [5/10], Batch [800/892], Loss: 1.7671\n",
            "Epoch [5/10], Batch [810/892], Loss: 1.8457\n",
            "Epoch [5/10], Batch [820/892], Loss: 1.7906\n",
            "Epoch [5/10], Batch [830/892], Loss: 1.7387\n",
            "Epoch [5/10], Batch [840/892], Loss: 1.7793\n",
            "Epoch [5/10], Batch [850/892], Loss: 1.7722\n",
            "Epoch [5/10], Batch [860/892], Loss: 1.7955\n",
            "Epoch [5/10], Batch [870/892], Loss: 1.8371\n",
            "Epoch [5/10], Batch [880/892], Loss: 1.8402\n",
            "Epoch [5/10], Batch [890/892], Loss: 1.8012\n",
            "Epoch [5/10] completed, Average Loss: 1.8054\n",
            "Epoch [6/10], Batch [10/892], Loss: 1.7726\n",
            "Epoch [6/10], Batch [20/892], Loss: 1.8212\n",
            "Epoch [6/10], Batch [30/892], Loss: 1.7564\n",
            "Epoch [6/10], Batch [40/892], Loss: 1.8336\n",
            "Epoch [6/10], Batch [50/892], Loss: 1.8198\n",
            "Epoch [6/10], Batch [60/892], Loss: 1.8298\n",
            "Epoch [6/10], Batch [70/892], Loss: 1.7887\n",
            "Epoch [6/10], Batch [80/892], Loss: 1.8183\n",
            "Epoch [6/10], Batch [90/892], Loss: 1.8012\n",
            "Epoch [6/10], Batch [100/892], Loss: 1.7983\n",
            "Epoch [6/10], Batch [110/892], Loss: 1.7863\n",
            "Epoch [6/10], Batch [120/892], Loss: 1.7957\n",
            "Epoch [6/10], Batch [130/892], Loss: 1.7585\n",
            "Epoch [6/10], Batch [140/892], Loss: 1.7610\n",
            "Epoch [6/10], Batch [150/892], Loss: 1.8172\n",
            "Epoch [6/10], Batch [160/892], Loss: 1.7998\n",
            "Epoch [6/10], Batch [170/892], Loss: 1.7409\n",
            "Epoch [6/10], Batch [180/892], Loss: 1.8216\n",
            "Epoch [6/10], Batch [190/892], Loss: 1.7207\n",
            "Epoch [6/10], Batch [200/892], Loss: 1.8084\n",
            "Epoch [6/10], Batch [210/892], Loss: 1.8053\n",
            "Epoch [6/10], Batch [220/892], Loss: 1.7055\n",
            "Epoch [6/10], Batch [230/892], Loss: 1.7838\n",
            "Epoch [6/10], Batch [240/892], Loss: 1.7554\n",
            "Epoch [6/10], Batch [250/892], Loss: 1.7928\n",
            "Epoch [6/10], Batch [260/892], Loss: 1.7528\n",
            "Epoch [6/10], Batch [270/892], Loss: 1.7842\n",
            "Epoch [6/10], Batch [280/892], Loss: 1.8160\n",
            "Epoch [6/10], Batch [290/892], Loss: 1.8137\n",
            "Epoch [6/10], Batch [300/892], Loss: 1.8108\n",
            "Epoch [6/10], Batch [310/892], Loss: 1.7788\n",
            "Epoch [6/10], Batch [320/892], Loss: 1.7615\n",
            "Epoch [6/10], Batch [330/892], Loss: 1.7396\n",
            "Epoch [6/10], Batch [340/892], Loss: 1.7466\n",
            "Epoch [6/10], Batch [350/892], Loss: 1.7612\n",
            "Epoch [6/10], Batch [360/892], Loss: 1.7674\n",
            "Epoch [6/10], Batch [370/892], Loss: 1.7068\n",
            "Epoch [6/10], Batch [380/892], Loss: 1.7685\n",
            "Epoch [6/10], Batch [390/892], Loss: 1.7876\n",
            "Epoch [6/10], Batch [400/892], Loss: 1.7716\n",
            "Epoch [6/10], Batch [410/892], Loss: 1.7272\n",
            "Epoch [6/10], Batch [420/892], Loss: 1.7461\n",
            "Epoch [6/10], Batch [430/892], Loss: 1.8012\n",
            "Epoch [6/10], Batch [440/892], Loss: 1.7729\n",
            "Epoch [6/10], Batch [450/892], Loss: 1.7116\n",
            "Epoch [6/10], Batch [460/892], Loss: 1.7245\n",
            "Epoch [6/10], Batch [470/892], Loss: 1.7655\n",
            "Epoch [6/10], Batch [480/892], Loss: 1.8316\n",
            "Epoch [6/10], Batch [490/892], Loss: 1.7774\n",
            "Epoch [6/10], Batch [500/892], Loss: 1.7878\n",
            "Epoch [6/10], Batch [510/892], Loss: 1.7736\n",
            "Epoch [6/10], Batch [520/892], Loss: 1.7718\n",
            "Epoch [6/10], Batch [530/892], Loss: 1.8213\n",
            "Epoch [6/10], Batch [540/892], Loss: 1.7659\n",
            "Epoch [6/10], Batch [550/892], Loss: 1.7880\n",
            "Epoch [6/10], Batch [560/892], Loss: 1.7692\n",
            "Epoch [6/10], Batch [570/892], Loss: 1.8569\n",
            "Epoch [6/10], Batch [580/892], Loss: 1.7658\n",
            "Epoch [6/10], Batch [590/892], Loss: 1.7423\n",
            "Epoch [6/10], Batch [600/892], Loss: 1.8333\n",
            "Epoch [6/10], Batch [610/892], Loss: 1.8307\n",
            "Epoch [6/10], Batch [620/892], Loss: 1.7260\n",
            "Epoch [6/10], Batch [630/892], Loss: 1.7821\n",
            "Epoch [6/10], Batch [640/892], Loss: 1.7611\n",
            "Epoch [6/10], Batch [650/892], Loss: 1.7447\n",
            "Epoch [6/10], Batch [660/892], Loss: 1.7806\n",
            "Epoch [6/10], Batch [670/892], Loss: 1.7372\n",
            "Epoch [6/10], Batch [680/892], Loss: 1.7938\n",
            "Epoch [6/10], Batch [690/892], Loss: 1.7372\n",
            "Epoch [6/10], Batch [700/892], Loss: 1.7743\n",
            "Epoch [6/10], Batch [710/892], Loss: 1.7413\n",
            "Epoch [6/10], Batch [720/892], Loss: 1.7517\n",
            "Epoch [6/10], Batch [730/892], Loss: 1.7942\n",
            "Epoch [6/10], Batch [740/892], Loss: 1.7122\n",
            "Epoch [6/10], Batch [750/892], Loss: 1.7649\n",
            "Epoch [6/10], Batch [760/892], Loss: 1.7758\n",
            "Epoch [6/10], Batch [770/892], Loss: 1.7408\n",
            "Epoch [6/10], Batch [780/892], Loss: 1.7966\n",
            "Epoch [6/10], Batch [790/892], Loss: 1.7562\n",
            "Epoch [6/10], Batch [800/892], Loss: 1.7824\n",
            "Epoch [6/10], Batch [810/892], Loss: 1.7504\n",
            "Epoch [6/10], Batch [820/892], Loss: 1.7650\n",
            "Epoch [6/10], Batch [830/892], Loss: 1.7940\n",
            "Epoch [6/10], Batch [840/892], Loss: 1.8212\n",
            "Epoch [6/10], Batch [850/892], Loss: 1.7562\n",
            "Epoch [6/10], Batch [860/892], Loss: 1.7701\n",
            "Epoch [6/10], Batch [870/892], Loss: 1.7727\n",
            "Epoch [6/10], Batch [880/892], Loss: 1.8078\n",
            "Epoch [6/10], Batch [890/892], Loss: 1.7741\n",
            "Epoch [6/10] completed, Average Loss: 1.7797\n",
            "Epoch [7/10], Batch [10/892], Loss: 1.8105\n",
            "Epoch [7/10], Batch [20/892], Loss: 1.7207\n",
            "Epoch [7/10], Batch [30/892], Loss: 1.7429\n",
            "Epoch [7/10], Batch [40/892], Loss: 1.7643\n",
            "Epoch [7/10], Batch [50/892], Loss: 1.7117\n",
            "Epoch [7/10], Batch [60/892], Loss: 1.8046\n",
            "Epoch [7/10], Batch [70/892], Loss: 1.8331\n",
            "Epoch [7/10], Batch [80/892], Loss: 1.7840\n",
            "Epoch [7/10], Batch [90/892], Loss: 1.7407\n",
            "Epoch [7/10], Batch [100/892], Loss: 1.7589\n",
            "Epoch [7/10], Batch [110/892], Loss: 1.7807\n",
            "Epoch [7/10], Batch [120/892], Loss: 1.7474\n",
            "Epoch [7/10], Batch [130/892], Loss: 1.7792\n",
            "Epoch [7/10], Batch [140/892], Loss: 1.7716\n",
            "Epoch [7/10], Batch [150/892], Loss: 1.7606\n",
            "Epoch [7/10], Batch [160/892], Loss: 1.7732\n",
            "Epoch [7/10], Batch [170/892], Loss: 1.8092\n",
            "Epoch [7/10], Batch [180/892], Loss: 1.7190\n",
            "Epoch [7/10], Batch [190/892], Loss: 1.7440\n",
            "Epoch [7/10], Batch [200/892], Loss: 1.7713\n",
            "Epoch [7/10], Batch [210/892], Loss: 1.8030\n",
            "Epoch [7/10], Batch [220/892], Loss: 1.7382\n",
            "Epoch [7/10], Batch [230/892], Loss: 1.7233\n",
            "Epoch [7/10], Batch [240/892], Loss: 1.8011\n",
            "Epoch [7/10], Batch [250/892], Loss: 1.7947\n",
            "Epoch [7/10], Batch [260/892], Loss: 1.7923\n",
            "Epoch [7/10], Batch [270/892], Loss: 1.7540\n",
            "Epoch [7/10], Batch [280/892], Loss: 1.7719\n",
            "Epoch [7/10], Batch [290/892], Loss: 1.7928\n",
            "Epoch [7/10], Batch [300/892], Loss: 1.7257\n",
            "Epoch [7/10], Batch [310/892], Loss: 1.7698\n",
            "Epoch [7/10], Batch [320/892], Loss: 1.7418\n",
            "Epoch [7/10], Batch [330/892], Loss: 1.7496\n",
            "Epoch [7/10], Batch [340/892], Loss: 1.7804\n",
            "Epoch [7/10], Batch [350/892], Loss: 1.7625\n",
            "Epoch [7/10], Batch [360/892], Loss: 1.7718\n",
            "Epoch [7/10], Batch [370/892], Loss: 1.7444\n",
            "Epoch [7/10], Batch [380/892], Loss: 1.7461\n",
            "Epoch [7/10], Batch [390/892], Loss: 1.7395\n",
            "Epoch [7/10], Batch [400/892], Loss: 1.7335\n",
            "Epoch [7/10], Batch [410/892], Loss: 1.7723\n",
            "Epoch [7/10], Batch [420/892], Loss: 1.7840\n",
            "Epoch [7/10], Batch [430/892], Loss: 1.7606\n",
            "Epoch [7/10], Batch [440/892], Loss: 1.7985\n",
            "Epoch [7/10], Batch [450/892], Loss: 1.7142\n",
            "Epoch [7/10], Batch [460/892], Loss: 1.7446\n",
            "Epoch [7/10], Batch [470/892], Loss: 1.7639\n",
            "Epoch [7/10], Batch [480/892], Loss: 1.7905\n",
            "Epoch [7/10], Batch [490/892], Loss: 1.7364\n",
            "Epoch [7/10], Batch [500/892], Loss: 1.7441\n",
            "Epoch [7/10], Batch [510/892], Loss: 1.7474\n",
            "Epoch [7/10], Batch [520/892], Loss: 1.7504\n",
            "Epoch [7/10], Batch [530/892], Loss: 1.8052\n",
            "Epoch [7/10], Batch [540/892], Loss: 1.7405\n",
            "Epoch [7/10], Batch [550/892], Loss: 1.8125\n",
            "Epoch [7/10], Batch [560/892], Loss: 1.7642\n",
            "Epoch [7/10], Batch [570/892], Loss: 1.7617\n",
            "Epoch [7/10], Batch [580/892], Loss: 1.7194\n",
            "Epoch [7/10], Batch [590/892], Loss: 1.7281\n",
            "Epoch [7/10], Batch [600/892], Loss: 1.7565\n",
            "Epoch [7/10], Batch [610/892], Loss: 1.7410\n",
            "Epoch [7/10], Batch [620/892], Loss: 1.7796\n",
            "Epoch [7/10], Batch [630/892], Loss: 1.7679\n",
            "Epoch [7/10], Batch [640/892], Loss: 1.7125\n",
            "Epoch [7/10], Batch [650/892], Loss: 1.7341\n",
            "Epoch [7/10], Batch [660/892], Loss: 1.7915\n",
            "Epoch [7/10], Batch [670/892], Loss: 1.7769\n",
            "Epoch [7/10], Batch [680/892], Loss: 1.7469\n",
            "Epoch [7/10], Batch [690/892], Loss: 1.6987\n",
            "Epoch [7/10], Batch [700/892], Loss: 1.7724\n",
            "Epoch [7/10], Batch [710/892], Loss: 1.7181\n",
            "Epoch [7/10], Batch [720/892], Loss: 1.7268\n",
            "Epoch [7/10], Batch [730/892], Loss: 1.7514\n",
            "Epoch [7/10], Batch [740/892], Loss: 1.7468\n",
            "Epoch [7/10], Batch [750/892], Loss: 1.7919\n",
            "Epoch [7/10], Batch [760/892], Loss: 1.7590\n",
            "Epoch [7/10], Batch [770/892], Loss: 1.7776\n",
            "Epoch [7/10], Batch [780/892], Loss: 1.7383\n",
            "Epoch [7/10], Batch [790/892], Loss: 1.7770\n",
            "Epoch [7/10], Batch [800/892], Loss: 1.7576\n",
            "Epoch [7/10], Batch [810/892], Loss: 1.7942\n",
            "Epoch [7/10], Batch [820/892], Loss: 1.7349\n",
            "Epoch [7/10], Batch [830/892], Loss: 1.7837\n",
            "Epoch [7/10], Batch [840/892], Loss: 1.7487\n",
            "Epoch [7/10], Batch [850/892], Loss: 1.7465\n",
            "Epoch [7/10], Batch [860/892], Loss: 1.7588\n",
            "Epoch [7/10], Batch [870/892], Loss: 1.7606\n",
            "Epoch [7/10], Batch [880/892], Loss: 1.7768\n",
            "Epoch [7/10], Batch [890/892], Loss: 1.7735\n",
            "Epoch [7/10] completed, Average Loss: 1.7625\n",
            "Epoch [8/10], Batch [10/892], Loss: 1.7802\n",
            "Epoch [8/10], Batch [20/892], Loss: 1.7790\n",
            "Epoch [8/10], Batch [30/892], Loss: 1.7406\n",
            "Epoch [8/10], Batch [40/892], Loss: 1.7450\n",
            "Epoch [8/10], Batch [50/892], Loss: 1.7431\n",
            "Epoch [8/10], Batch [60/892], Loss: 1.7446\n",
            "Epoch [8/10], Batch [70/892], Loss: 1.7409\n",
            "Epoch [8/10], Batch [80/892], Loss: 1.7743\n",
            "Epoch [8/10], Batch [90/892], Loss: 1.7694\n",
            "Epoch [8/10], Batch [100/892], Loss: 1.7004\n",
            "Epoch [8/10], Batch [110/892], Loss: 1.7313\n",
            "Epoch [8/10], Batch [120/892], Loss: 1.7926\n",
            "Epoch [8/10], Batch [130/892], Loss: 1.7322\n",
            "Epoch [8/10], Batch [140/892], Loss: 1.7334\n",
            "Epoch [8/10], Batch [150/892], Loss: 1.7225\n",
            "Epoch [8/10], Batch [160/892], Loss: 1.7792\n",
            "Epoch [8/10], Batch [170/892], Loss: 1.7558\n",
            "Epoch [8/10], Batch [180/892], Loss: 1.7984\n",
            "Epoch [8/10], Batch [190/892], Loss: 1.7498\n",
            "Epoch [8/10], Batch [200/892], Loss: 1.7987\n",
            "Epoch [8/10], Batch [210/892], Loss: 1.7973\n",
            "Epoch [8/10], Batch [220/892], Loss: 1.6707\n",
            "Epoch [8/10], Batch [230/892], Loss: 1.7393\n",
            "Epoch [8/10], Batch [240/892], Loss: 1.7427\n",
            "Epoch [8/10], Batch [250/892], Loss: 1.7862\n",
            "Epoch [8/10], Batch [260/892], Loss: 1.7805\n",
            "Epoch [8/10], Batch [270/892], Loss: 1.7364\n",
            "Epoch [8/10], Batch [280/892], Loss: 1.7078\n",
            "Epoch [8/10], Batch [290/892], Loss: 1.7956\n",
            "Epoch [8/10], Batch [300/892], Loss: 1.7598\n",
            "Epoch [8/10], Batch [310/892], Loss: 1.7239\n",
            "Epoch [8/10], Batch [320/892], Loss: 1.7407\n",
            "Epoch [8/10], Batch [330/892], Loss: 1.6784\n",
            "Epoch [8/10], Batch [340/892], Loss: 1.7267\n",
            "Epoch [8/10], Batch [350/892], Loss: 1.7218\n",
            "Epoch [8/10], Batch [360/892], Loss: 1.7432\n",
            "Epoch [8/10], Batch [370/892], Loss: 1.7414\n",
            "Epoch [8/10], Batch [380/892], Loss: 1.7584\n",
            "Epoch [8/10], Batch [390/892], Loss: 1.7752\n",
            "Epoch [8/10], Batch [400/892], Loss: 1.7289\n",
            "Epoch [8/10], Batch [410/892], Loss: 1.7877\n",
            "Epoch [8/10], Batch [420/892], Loss: 1.7130\n",
            "Epoch [8/10], Batch [430/892], Loss: 1.7131\n",
            "Epoch [8/10], Batch [440/892], Loss: 1.7193\n",
            "Epoch [8/10], Batch [450/892], Loss: 1.7686\n",
            "Epoch [8/10], Batch [460/892], Loss: 1.7376\n",
            "Epoch [8/10], Batch [470/892], Loss: 1.7266\n",
            "Epoch [8/10], Batch [480/892], Loss: 1.7581\n",
            "Epoch [8/10], Batch [490/892], Loss: 1.7433\n",
            "Epoch [8/10], Batch [500/892], Loss: 1.7730\n",
            "Epoch [8/10], Batch [510/892], Loss: 1.6847\n",
            "Epoch [8/10], Batch [520/892], Loss: 1.7068\n",
            "Epoch [8/10], Batch [530/892], Loss: 1.7568\n",
            "Epoch [8/10], Batch [540/892], Loss: 1.7685\n",
            "Epoch [8/10], Batch [550/892], Loss: 1.7555\n",
            "Epoch [8/10], Batch [560/892], Loss: 1.7182\n",
            "Epoch [8/10], Batch [570/892], Loss: 1.8006\n",
            "Epoch [8/10], Batch [580/892], Loss: 1.7106\n",
            "Epoch [8/10], Batch [590/892], Loss: 1.7307\n",
            "Epoch [8/10], Batch [600/892], Loss: 1.7481\n",
            "Epoch [8/10], Batch [610/892], Loss: 1.6881\n",
            "Epoch [8/10], Batch [620/892], Loss: 1.7217\n",
            "Epoch [8/10], Batch [630/892], Loss: 1.7463\n",
            "Epoch [8/10], Batch [640/892], Loss: 1.7163\n",
            "Epoch [8/10], Batch [650/892], Loss: 1.7140\n",
            "Epoch [8/10], Batch [660/892], Loss: 1.8005\n",
            "Epoch [8/10], Batch [670/892], Loss: 1.7637\n",
            "Epoch [8/10], Batch [680/892], Loss: 1.7766\n",
            "Epoch [8/10], Batch [690/892], Loss: 1.7575\n",
            "Epoch [8/10], Batch [700/892], Loss: 1.7192\n",
            "Epoch [8/10], Batch [710/892], Loss: 1.7729\n",
            "Epoch [8/10], Batch [720/892], Loss: 1.6969\n",
            "Epoch [8/10], Batch [730/892], Loss: 1.7139\n",
            "Epoch [8/10], Batch [740/892], Loss: 1.7612\n",
            "Epoch [8/10], Batch [750/892], Loss: 1.7538\n",
            "Epoch [8/10], Batch [760/892], Loss: 1.7323\n",
            "Epoch [8/10], Batch [770/892], Loss: 1.7217\n",
            "Epoch [8/10], Batch [780/892], Loss: 1.8195\n",
            "Epoch [8/10], Batch [790/892], Loss: 1.7392\n",
            "Epoch [8/10], Batch [800/892], Loss: 1.7001\n",
            "Epoch [8/10], Batch [810/892], Loss: 1.6998\n",
            "Epoch [8/10], Batch [820/892], Loss: 1.7381\n",
            "Epoch [8/10], Batch [830/892], Loss: 1.7299\n",
            "Epoch [8/10], Batch [840/892], Loss: 1.6805\n",
            "Epoch [8/10], Batch [850/892], Loss: 1.7006\n",
            "Epoch [8/10], Batch [860/892], Loss: 1.7461\n",
            "Epoch [8/10], Batch [870/892], Loss: 1.7351\n",
            "Epoch [8/10], Batch [880/892], Loss: 1.7451\n",
            "Epoch [8/10], Batch [890/892], Loss: 1.7249\n",
            "Epoch [8/10] completed, Average Loss: 1.7478\n",
            "Epoch [9/10], Batch [10/892], Loss: 1.8027\n",
            "Epoch [9/10], Batch [20/892], Loss: 1.7196\n",
            "Epoch [9/10], Batch [30/892], Loss: 1.7348\n",
            "Epoch [9/10], Batch [40/892], Loss: 1.7647\n",
            "Epoch [9/10], Batch [50/892], Loss: 1.7497\n",
            "Epoch [9/10], Batch [60/892], Loss: 1.7353\n",
            "Epoch [9/10], Batch [70/892], Loss: 1.7494\n",
            "Epoch [9/10], Batch [80/892], Loss: 1.7752\n",
            "Epoch [9/10], Batch [90/892], Loss: 1.6967\n",
            "Epoch [9/10], Batch [100/892], Loss: 1.6747\n",
            "Epoch [9/10], Batch [110/892], Loss: 1.7350\n",
            "Epoch [9/10], Batch [120/892], Loss: 1.7870\n",
            "Epoch [9/10], Batch [130/892], Loss: 1.6997\n",
            "Epoch [9/10], Batch [140/892], Loss: 1.7744\n",
            "Epoch [9/10], Batch [150/892], Loss: 1.7337\n",
            "Epoch [9/10], Batch [160/892], Loss: 1.7679\n",
            "Epoch [9/10], Batch [170/892], Loss: 1.7889\n",
            "Epoch [9/10], Batch [180/892], Loss: 1.7321\n",
            "Epoch [9/10], Batch [190/892], Loss: 1.7390\n",
            "Epoch [9/10], Batch [200/892], Loss: 1.7151\n",
            "Epoch [9/10], Batch [210/892], Loss: 1.7109\n",
            "Epoch [9/10], Batch [220/892], Loss: 1.7328\n",
            "Epoch [9/10], Batch [230/892], Loss: 1.7660\n",
            "Epoch [9/10], Batch [240/892], Loss: 1.7938\n",
            "Epoch [9/10], Batch [250/892], Loss: 1.7445\n",
            "Epoch [9/10], Batch [260/892], Loss: 1.7318\n",
            "Epoch [9/10], Batch [270/892], Loss: 1.7317\n",
            "Epoch [9/10], Batch [280/892], Loss: 1.8303\n",
            "Epoch [9/10], Batch [290/892], Loss: 1.7663\n",
            "Epoch [9/10], Batch [300/892], Loss: 1.7560\n",
            "Epoch [9/10], Batch [310/892], Loss: 1.7113\n",
            "Epoch [9/10], Batch [320/892], Loss: 1.7256\n",
            "Epoch [9/10], Batch [330/892], Loss: 1.7576\n",
            "Epoch [9/10], Batch [340/892], Loss: 1.7503\n",
            "Epoch [9/10], Batch [350/892], Loss: 1.7277\n",
            "Epoch [9/10], Batch [360/892], Loss: 1.8012\n",
            "Epoch [9/10], Batch [370/892], Loss: 1.7588\n",
            "Epoch [9/10], Batch [380/892], Loss: 1.7624\n",
            "Epoch [9/10], Batch [390/892], Loss: 1.7248\n",
            "Epoch [9/10], Batch [400/892], Loss: 1.7752\n",
            "Epoch [9/10], Batch [410/892], Loss: 1.8027\n",
            "Epoch [9/10], Batch [420/892], Loss: 1.7310\n",
            "Epoch [9/10], Batch [430/892], Loss: 1.6861\n",
            "Epoch [9/10], Batch [440/892], Loss: 1.7857\n",
            "Epoch [9/10], Batch [450/892], Loss: 1.7598\n",
            "Epoch [9/10], Batch [460/892], Loss: 1.7227\n",
            "Epoch [9/10], Batch [470/892], Loss: 1.7514\n",
            "Epoch [9/10], Batch [480/892], Loss: 1.7345\n",
            "Epoch [9/10], Batch [490/892], Loss: 1.7196\n",
            "Epoch [9/10], Batch [500/892], Loss: 1.7002\n",
            "Epoch [9/10], Batch [510/892], Loss: 1.7621\n",
            "Epoch [9/10], Batch [520/892], Loss: 1.8232\n",
            "Epoch [9/10], Batch [530/892], Loss: 1.7770\n",
            "Epoch [9/10], Batch [540/892], Loss: 1.7282\n",
            "Epoch [9/10], Batch [550/892], Loss: 1.7907\n",
            "Epoch [9/10], Batch [560/892], Loss: 1.7654\n",
            "Epoch [9/10], Batch [570/892], Loss: 1.7348\n",
            "Epoch [9/10], Batch [580/892], Loss: 1.7124\n",
            "Epoch [9/10], Batch [590/892], Loss: 1.7271\n",
            "Epoch [9/10], Batch [600/892], Loss: 1.7641\n",
            "Epoch [9/10], Batch [610/892], Loss: 1.7712\n",
            "Epoch [9/10], Batch [620/892], Loss: 1.7198\n",
            "Epoch [9/10], Batch [630/892], Loss: 1.7259\n",
            "Epoch [9/10], Batch [640/892], Loss: 1.7249\n",
            "Epoch [9/10], Batch [650/892], Loss: 1.7311\n",
            "Epoch [9/10], Batch [660/892], Loss: 1.7263\n",
            "Epoch [9/10], Batch [670/892], Loss: 1.7711\n",
            "Epoch [9/10], Batch [680/892], Loss: 1.7626\n",
            "Epoch [9/10], Batch [690/892], Loss: 1.7304\n",
            "Epoch [9/10], Batch [700/892], Loss: 1.7001\n",
            "Epoch [9/10], Batch [710/892], Loss: 1.7167\n",
            "Epoch [9/10], Batch [720/892], Loss: 1.7680\n",
            "Epoch [9/10], Batch [730/892], Loss: 1.6869\n",
            "Epoch [9/10], Batch [740/892], Loss: 1.7344\n",
            "Epoch [9/10], Batch [750/892], Loss: 1.7728\n",
            "Epoch [9/10], Batch [760/892], Loss: 1.7471\n",
            "Epoch [9/10], Batch [770/892], Loss: 1.7262\n",
            "Epoch [9/10], Batch [780/892], Loss: 1.7323\n",
            "Epoch [9/10], Batch [790/892], Loss: 1.7118\n",
            "Epoch [9/10], Batch [800/892], Loss: 1.7569\n",
            "Epoch [9/10], Batch [810/892], Loss: 1.7037\n",
            "Epoch [9/10], Batch [820/892], Loss: 1.7055\n",
            "Epoch [9/10], Batch [830/892], Loss: 1.7461\n",
            "Epoch [9/10], Batch [840/892], Loss: 1.7542\n",
            "Epoch [9/10], Batch [850/892], Loss: 1.7476\n",
            "Epoch [9/10], Batch [860/892], Loss: 1.6969\n",
            "Epoch [9/10], Batch [870/892], Loss: 1.7876\n",
            "Epoch [9/10], Batch [880/892], Loss: 1.7169\n",
            "Epoch [9/10], Batch [890/892], Loss: 1.6948\n",
            "Epoch [9/10] completed, Average Loss: 1.7376\n",
            "Epoch [10/10], Batch [10/892], Loss: 1.7214\n",
            "Epoch [10/10], Batch [20/892], Loss: 1.7278\n",
            "Epoch [10/10], Batch [30/892], Loss: 1.7291\n",
            "Epoch [10/10], Batch [40/892], Loss: 1.7759\n",
            "Epoch [10/10], Batch [50/892], Loss: 1.7169\n",
            "Epoch [10/10], Batch [60/892], Loss: 1.7818\n",
            "Epoch [10/10], Batch [70/892], Loss: 1.7523\n",
            "Epoch [10/10], Batch [80/892], Loss: 1.7373\n",
            "Epoch [10/10], Batch [90/892], Loss: 1.7773\n",
            "Epoch [10/10], Batch [100/892], Loss: 1.6978\n",
            "Epoch [10/10], Batch [110/892], Loss: 1.7400\n",
            "Epoch [10/10], Batch [120/892], Loss: 1.7791\n",
            "Epoch [10/10], Batch [130/892], Loss: 1.7660\n",
            "Epoch [10/10], Batch [140/892], Loss: 1.7622\n",
            "Epoch [10/10], Batch [150/892], Loss: 1.7743\n",
            "Epoch [10/10], Batch [160/892], Loss: 1.7441\n",
            "Epoch [10/10], Batch [170/892], Loss: 1.7461\n",
            "Epoch [10/10], Batch [180/892], Loss: 1.7589\n",
            "Epoch [10/10], Batch [190/892], Loss: 1.6774\n",
            "Epoch [10/10], Batch [200/892], Loss: 1.7258\n",
            "Epoch [10/10], Batch [210/892], Loss: 1.7127\n",
            "Epoch [10/10], Batch [220/892], Loss: 1.7195\n",
            "Epoch [10/10], Batch [230/892], Loss: 1.7601\n",
            "Epoch [10/10], Batch [240/892], Loss: 1.7174\n",
            "Epoch [10/10], Batch [250/892], Loss: 1.6847\n",
            "Epoch [10/10], Batch [260/892], Loss: 1.6747\n",
            "Epoch [10/10], Batch [270/892], Loss: 1.7069\n",
            "Epoch [10/10], Batch [280/892], Loss: 1.7189\n",
            "Epoch [10/10], Batch [290/892], Loss: 1.7292\n",
            "Epoch [10/10], Batch [300/892], Loss: 1.6956\n",
            "Epoch [10/10], Batch [310/892], Loss: 1.7746\n",
            "Epoch [10/10], Batch [320/892], Loss: 1.7071\n",
            "Epoch [10/10], Batch [330/892], Loss: 1.6765\n",
            "Epoch [10/10], Batch [340/892], Loss: 1.7134\n",
            "Epoch [10/10], Batch [350/892], Loss: 1.7153\n",
            "Epoch [10/10], Batch [360/892], Loss: 1.7134\n",
            "Epoch [10/10], Batch [370/892], Loss: 1.7494\n",
            "Epoch [10/10], Batch [380/892], Loss: 1.7751\n",
            "Epoch [10/10], Batch [390/892], Loss: 1.7525\n",
            "Epoch [10/10], Batch [400/892], Loss: 1.7267\n",
            "Epoch [10/10], Batch [410/892], Loss: 1.6937\n",
            "Epoch [10/10], Batch [420/892], Loss: 1.7119\n",
            "Epoch [10/10], Batch [430/892], Loss: 1.7412\n",
            "Epoch [10/10], Batch [440/892], Loss: 1.7114\n",
            "Epoch [10/10], Batch [450/892], Loss: 1.7572\n",
            "Epoch [10/10], Batch [460/892], Loss: 1.7627\n",
            "Epoch [10/10], Batch [470/892], Loss: 1.7241\n",
            "Epoch [10/10], Batch [480/892], Loss: 1.7800\n",
            "Epoch [10/10], Batch [490/892], Loss: 1.7346\n",
            "Epoch [10/10], Batch [500/892], Loss: 1.7655\n",
            "Epoch [10/10], Batch [510/892], Loss: 1.7410\n",
            "Epoch [10/10], Batch [520/892], Loss: 1.7225\n",
            "Epoch [10/10], Batch [530/892], Loss: 1.7770\n",
            "Epoch [10/10], Batch [540/892], Loss: 1.7845\n",
            "Epoch [10/10], Batch [550/892], Loss: 1.7261\n",
            "Epoch [10/10], Batch [560/892], Loss: 1.7730\n",
            "Epoch [10/10], Batch [570/892], Loss: 1.7625\n",
            "Epoch [10/10], Batch [580/892], Loss: 1.7406\n",
            "Epoch [10/10], Batch [590/892], Loss: 1.7531\n",
            "Epoch [10/10], Batch [600/892], Loss: 1.7227\n",
            "Epoch [10/10], Batch [610/892], Loss: 1.7237\n",
            "Epoch [10/10], Batch [620/892], Loss: 1.7602\n",
            "Epoch [10/10], Batch [630/892], Loss: 1.7288\n",
            "Epoch [10/10], Batch [640/892], Loss: 1.7557\n",
            "Epoch [10/10], Batch [650/892], Loss: 1.7074\n",
            "Epoch [10/10], Batch [660/892], Loss: 1.7185\n",
            "Epoch [10/10], Batch [670/892], Loss: 1.7549\n",
            "Epoch [10/10], Batch [680/892], Loss: 1.6550\n",
            "Epoch [10/10], Batch [690/892], Loss: 1.7160\n",
            "Epoch [10/10], Batch [700/892], Loss: 1.7606\n",
            "Epoch [10/10], Batch [710/892], Loss: 1.7518\n",
            "Epoch [10/10], Batch [720/892], Loss: 1.7434\n",
            "Epoch [10/10], Batch [730/892], Loss: 1.6925\n",
            "Epoch [10/10], Batch [740/892], Loss: 1.7093\n",
            "Epoch [10/10], Batch [750/892], Loss: 1.6553\n",
            "Epoch [10/10], Batch [760/892], Loss: 1.7303\n",
            "Epoch [10/10], Batch [770/892], Loss: 1.6806\n",
            "Epoch [10/10], Batch [780/892], Loss: 1.6738\n",
            "Epoch [10/10], Batch [790/892], Loss: 1.6379\n",
            "Epoch [10/10], Batch [800/892], Loss: 1.6980\n",
            "Epoch [10/10], Batch [810/892], Loss: 1.7432\n",
            "Epoch [10/10], Batch [820/892], Loss: 1.7304\n",
            "Epoch [10/10], Batch [830/892], Loss: 1.7775\n",
            "Epoch [10/10], Batch [840/892], Loss: 1.7165\n",
            "Epoch [10/10], Batch [850/892], Loss: 1.7192\n",
            "Epoch [10/10], Batch [860/892], Loss: 1.6825\n",
            "Epoch [10/10], Batch [870/892], Loss: 1.7639\n",
            "Epoch [10/10], Batch [880/892], Loss: 1.6903\n",
            "Epoch [10/10], Batch [890/892], Loss: 1.6983\n",
            "Epoch [10/10] completed, Average Loss: 1.7296\n"
          ]
        }
      ],
      "source": [
        "if 'model' in locals() and 'dataloader' in locals() and model is not None and dataloader is not None:\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_idx, (melspectrograms, texts, mel_lengths, text_lengths) in enumerate(dataloader):\n",
        "            melspectrograms = melspectrograms.to(device)\n",
        "            texts = texts.to(device)\n",
        "            mel_lengths = mel_lengths.to(device)\n",
        "            text_lengths = text_lengths.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            logits = model(melspectrograms)\n",
        "            log_probs = nn.functional.log_softmax(logits, dim=2)\n",
        "            \n",
        "            log_probs = log_probs.transpose(0, 1)\n",
        "            \n",
        "            loss = criterion(log_probs, texts, mel_lengths, text_lengths)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            \n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
        "        \n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}] completed, Average Loss: {avg_loss:.4f}')\n",
        "    \n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'encoder': encoder,\n",
        "        'vocab_size': vocab_size,\n",
        "        'config': {\n",
        "            'n_mels': N_MELS,\n",
        "            'hidden_size': HIDDEN_SIZE,\n",
        "            'num_layers': NUM_LAYERS,\n",
        "            'dropout': DROPOUT\n",
        "        }\n",
        "    }, 'checkpoint.pth')\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaIRJREFUeJzt3Xd4VGX6xvF7JpnMpFcCCYTQBUFAadKxgCLCYldQLKtrCdhWd8W1gA3QXSuK4g/FVVgVXSzYFhtNEBVBUKnSIUASUkhIMsmc3x/JDIQEUkhypnw/15WLzJlzZp7JnITced/3ORbDMAwBAAAAAI7LanYBAAAAAODtCE4AAAAAUA2CEwAAAABUg+AEAAAAANUgOAEAAABANQhOAAAAAFANghMAAAAAVIPgBAAAAADVIDgBAAAAQDUITkCAmTRpkiwWi9llAI1uyJAhGjJkiNllwI+1atVK1113XZ2O5fwEvB/BCWhEs2fPlsVi8Xw4HA516NBB48eP1759++rteQoKCjRp0iR9++239faYx/PDDz9o/Pjx6ty5s8LDw9WyZUtdfvnl2rhxY4M955YtW3TzzTerTZs2cjgcioqKUv/+/fXcc8/p8OHDnv1atWoli8WiCRMmVHqMb7/9VhaLRe+9955nm/v9cTgc2r17d6VjhgwZoi5dujTMizoOd9C1Wq3auXNnpftzc3MVGhoqi8Wi8ePHe7Zv27ZNFotF//znP0/4+O6vkfsjMTFRAwcO1Pz58+v9tZzI0d8bS5curXS/YRhKSUmRxWLRhRdeWG/Pe+DAAd1xxx3q2LGjQkNDlZiYqN69e+vvf/+7Dh06VG/PU1cvvfSSZs+ebXYZPs39vV6Tj0DVqlWrev2+AvxVsNkFAIHokUceUevWrVVYWKilS5dqxowZ+vTTT7Vu3TqFhYWd9OMXFBRo8uTJklTpL5gPPPCA7rvvvpN+Drdp06Zp2bJluuyyy9S1a1elp6dr+vTpOuOMM7RixYp6DxqffPKJLrvsMtntdo0bN05dunRRcXGxli5dqnvvvVe//vqrZs6cWeGYV199VRMnTlRycnKNnqOoqEhTp07VCy+8UK+1nwy73a7//Oc/+tvf/lZh+3//+9+Tfuzu3bvrr3/9qyRpz549euWVV3TxxRdrxowZuuWWW0768WvD4XBo7ty5GjBgQIXtixYt0q5du2S32+vtubKystSzZ0/l5ubqhhtuUMeOHZWZmalffvlFM2bM0K233qqIiIh6e766eOmll5SQkFDnUQxInTp10ptvvllh28SJExUREaF//OMf9fpcGzZskNVat79J/+9//6vXWgDUP4ITYILhw4erZ8+ekqQbb7xR8fHxevrpp/Xhhx/qqquuqvPjulwuFRcXn3Cf4OBgBQfX37f+3Xffrblz5yokJMSz7YorrtBpp52mqVOn6q233qq359q6dauuvPJKpaam6uuvv1ZSUpLnvrS0NG3evFmffPJJhWM6d+6sDRs2aOrUqXr++edr9Dzdu3evddiqjsVi0euvv17nX4AvuOCCKoPT3LlzNWLECL3//vt1rq158+a6+uqrPbfHjRundu3a6Zlnnql1cHJPVZo0aVKdarngggs0b948Pf/88xXO07lz56pHjx7KyMio0+NWZdasWdqxY4eWLVumfv36VbgvNze3wjntC/Lz8xUeHt4oz1VYWKiQkJA6h4TG1LRp0wrntyRNnTpVCQkJlbYfzf3z1OFw1Pi5TibY+9r5BgQi7/+JBwSAs88+W1JZMJCkf/7zn+rXr5/i4+MVGhqqHj16VJhS5uaenjVnzhx17txZdrtdL7/8spo0aSJJmjx5smcKivsX2eOtcXrrrbfUo0cPhYaGKi4uTldeeWWVU8OO1a9fv0r/4bdv316dO3fW77//XquvQ3WefPJJHTp0SLNmzaoQmtzatWunO+64o8K2Vq1aady4cXr11Ve1Z8+eGj3P/fffr9LSUk2dOrVe6q4PY8aM0erVq7V+/XrPtvT0dH399dcaM2ZMvT5Xs2bN1KlTJ8/52JiuuuoqZWZmauHChZ5txcXFeu+99477Ol0ul5599ll17txZDodDTZs21c0336yDBw+e8Lm2bNmioKAgnXnmmZXui4qKqvALs3ua5k8//aR+/fopNDRUrVu31ssvv1zp2KKiIj388MNq166d7Ha7UlJS9Le//U1FRUWV9n3rrbfUu3dvhYWFKTY2VoMGDfKMPLRq1Uq//vqrFi1a5Pk+do8gu6c2Llq0SLfddpsSExPVokULz+O+9NJLnp8JycnJSktLU3Z2dqXnf/HFF9WmTRuFhoaqd+/eWrJkSaW1Nu6pbm+//bYeeOABNW/eXGFhYcrNzVVWVpbuuecenXbaaYqIiFBUVJSGDx+uNWvWVHge92O8++67mjx5spo3b67IyEhdeumlysnJUVFRke68804lJiYqIiJC119/fZVfr2MVFBRo/fr19RKoq/p5+vnnn0uq+c/kY9c4ud+nZcuW6e6771aTJk0UHh6uiy66SAcOHKhw7PG+7u+++64ef/xxtWjRQg6HQ+ecc442b95c6blr8l6ejJKSEj366KNq27at7Ha7WrVqpfvvv7/S+/Tjjz/qvPPOU0JCguf75IYbbqiwz9tvv60ePXooMjJSUVFROu200/Tcc8/VS51AQyI4AV5gy5YtkqT4+HhJ0nPPPafTTz9djzzyiJ544gkFBwfrsssuqzSaIklff/217rrrLl1xxRV67rnn1KtXL82YMUOSdNFFF+nNN9/Um2++qYsvvvi4z//4449r3Lhxat++vZ5++mndeeed+uqrrzRo0KAqf9mqjmEY2rdvnxISEmp97Il8/PHHatOmTaXRger84x//UElJSY2DUOvWrWsdthraoEGD1KJFC82dO9ez7Z133lFERIRGjBhRr8/ldDq1c+dOz/nYmFq1aqW+ffvqP//5j2fbZ599ppycHF155ZVVHnPzzTfr3nvv9axzu/766zVnzhydd955cjqdx32u1NRUlZaWVprGdTwHDx7UBRdcoB49eujJJ59UixYtdOutt+q1117z7ONyuTRq1Cj985//1MiRI/XCCy9o9OjReuaZZ3TFFVdUeLzJkyfrmmuukc1m0yOPPKLJkycrJSVFX3/9tSTp2WefVYsWLdSxY0fP9/GxU8tuu+02/fbbb3rooYc8U3AnTZqktLQ0JScn61//+pcuueQSvfLKKxo2bFiFr8eMGTM0fvx4tWjRQk8++aQGDhyo0aNHa9euXVW+/kcffVSffPKJ7rnnHj3xxBMKCQnRH3/8oQ8++EAXXnihnn76ad17771au3atBg8eXOX3zpQpU/TFF1/ovvvu0w033KD//ve/uuWWW3TDDTdo48aNmjRpki6++GLNnj1b06ZNq/Y9WblypTp16qTp06dXu29NHPvztFWrVpJq9zO5KhMmTNCaNWv08MMP69Zbb9XHH39cYU3iiUydOlXz58/XPffco4kTJ2rFihUaO3ZshX1q+17WxY033qiHHnpIZ5xxhp555hkNHjxYU6ZMqfB9uX//fg0bNkzbtm3TfffdpxdeeEFjx47VihUrPPssXLhQV111lWJjYzVt2jRNnTpVQ4YM0bJly+qtVqDBGAAazeuvv25IMr788kvjwIEDxs6dO423337biI+PN0JDQ41du3YZhmEYBQUFFY4rLi42unTpYpx99tkVtksyrFar8euvv1bYfuDAAUOS8fDDD1eq4eGHHzaO/tbftm2bERQUZDz++OMV9lu7dq0RHBxcaXtNvPnmm4YkY9asWbU+9nhycnIMScaf/vSnGh+TmppqjBgxwjAMw7j++usNh8Nh7NmzxzAMw/jmm28MSca8efM8+7vfnx9++MHYsmWLERwcbNx+++2e+wcPHmx07ty5TvVLMl5//fVaH+d+vw4cOGDcc889Rrt27Tz39erVy7j++us9j5+Wlua5b+vWrYYk46mnnjrh46emphrDhg0zDhw4YBw4cMBYs2aNceWVVxqSjAkTJtS63tTU1CrPu+oc/bWfPn26ERkZ6fk+uOyyy4yzzjrL8/ju99QwDGPJkiWGJGPOnDkVHu/zzz+vtH3w4MHG4MGDPbfT09ONJk2aGJKMjh07Grfccosxd+5cIzs7u1J9gwcPNiQZ//rXvzzbioqKjO7duxuJiYlGcXGxYRhl577VajWWLFlS4fiXX37ZkGQsW7bMMAzD2LRpk2G1Wo2LLrrIKC0trbCvy+XyfN65c+cKNR/79RowYIBRUlLi2b5//34jJCTEGDZsWIXHnT59uiHJeO211zy1x8fHG7169TKcTqdnv9mzZxuSKjyn+3ulTZs2lX42FRYWVqp/69atht1uNx555JFKj9GlSxfP18owDOOqq64yLBaLMXz48AqP0bdvXyM1NbXS6z6W+3Fre85V9XU93s9Tw6j5z+TU1FTj2muv9dx2v0/nnntuhff1rrvuMoKCgiqca8een+7X1qlTJ6OoqMiz/bnnnjMkGWvXrjUMo3bv5fEc+311rNWrVxuSjBtvvLHC9nvuuceQZHz99deGYRjG/PnzPd/Hx3PHHXcYUVFRFc5bwFcw4gSY4Nxzz1WTJk2UkpKiK6+8UhEREZo/f76aN28uSQoNDfXse/DgQeXk5GjgwIFatWpVpccaPHiwTj311DrX8t///lcul0uXX365MjIyPB/NmjVT+/bt9c0339Tq8davX6+0tDT17dtX1157bZ3rOlZubq4kKTIysk7HP/DAA7UadWrTpo2uueYazZw5U3v37q3VcxUUFFT4WrqnER06dKjCtuqmkh1rzJgx2rx5s3744QfPv/UxTe9///ufmjRpoiZNmqhbt26aN2+errnmmmr/4l9UVFTpdbpcruO+/pq6/PLLdfjwYS1YsEB5eXlasGDBcV/nvHnzFB0draFDh1Z4vh49eigiIuKE52/Tpk21Zs0a3XLLLTp48KBefvlljRkzRomJiXr00UdlGEaF/YODg3XzzTd7boeEhOjmm2/W/v379dNPP3nq6dSpkzp27FihHvd0XHc9H3zwgVwulx566KFK64Rq093tpptuUlBQkOf2l19+qeLiYt15550VHvemm25SVFSUZ4Tkxx9/VGZmpm666aYKa8nGjh2r2NjYKp/r2muvrfCzSSpb0+N+ntLSUmVmZioiIkKnnHJKlT+vxo0bJ5vN5rndp08fGYZRaSpXnz59tHPnTpWUlJzw9Q8ZMkSGYdR5Td2xjvfztDY/k6vyl7/8pcL7OnDgQJWWlmr79u3VHnv99ddXmA49cOBASdIff/whqW7vZW19+umnksrWtB7N3VTGfV7FxMRIkhYsWHDc0d6YmBjl5+dXmI4L+AqaQwAmePHFF9WhQwcFBweradOmOuWUUyr8krNgwQI99thjWr16dYX541X9QtW6deuTqmXTpk0yDEPt27ev8v6jf8mpTnp6ukaMGKHo6Gi99957FX6hq8rhw4eVk5NTYVuzZs2q3DcqKkqSlJeXV+N6jnZ0EKppV8EHHnhAb775pqZOnVqr+fdPPvmkp6vh0SZMmFChNXpqaqq2bdtW48c9/fTT1bFjR82dO1cxMTFq1qyZ5xfyk9GnTx899thjslgsCgsLU6dOnTy/AJ3If/7zH11//fWVtj/11FN66qmnKmw7NoScSJMmTXTuuedq7ty5KigoUGlpqS699NIq9920aZNycnKUmJhY5f379+8/4XMlJSVpxowZeumll7Rp0yZ98cUXmjZtmh566CElJSXpxhtv9OybnJxcqflChw4dJJW1fz/zzDO1adMm/f777551hserZ8uWLbJarSf1Rw+p8ve/+xfxU045pcL2kJAQtWnTxnO/+9927dpV2C84ONgzPa2655LKpiY+99xzeumll7R161aVlpZ67qtqqmfLli0r3I6OjpYkpaSkVNrucrmUk5PTqFNGj/fztDY/k6ty7Ot2B5qa/PGkumPr8l7W1vbt22W1Wis9R7NmzRQTE+OpYfDgwbrkkks0efJkPfPMMxoyZIhGjx6tMWPGeBpn3HbbbXr33Xc1fPhwNW/eXMOGDdPll1+u888/v15qBRoSwQkwQe/evT1d9Y61ZMkSjRo1SoMGDdJLL72kpKQk2Ww2vf766xXWt7gd+xfg2nK5XLJYLPrss8+qDDo1bceck5Oj4cOHKzs7W0uWLKlRN7p33nmn0i/ex/sFOyoqSsnJyVq3bl2N6qnKP/7xD7355puaNm2aRo8eXe3+bdq00dVXX12rsCWV/VX92HbaQ4cO1b333qthw4Z5ttXlvRszZoxmzJihyMhIXXHFFfXS1SwhIUHnnnturY8777zzKv3V+Oqrr9awYcM0bty4k6ppzJgxuummm5Senq7hw4cfN8i5XC4lJiZqzpw5Vd5/vABzLIvFog4dOqhDhw4aMWKE2rdvrzlz5lQITjXhcrl02mmn6emnn67y/mMDwsk62e//k32uJ554Qg8++KBuuOEGPfroo4qLi5PVatWdd94pl8tVaf/j/THleNtrE7jrQ1WvsbY/k6tyMq/PW742UvVB0X1tvBUrVujjjz/WF198oRtuuEH/+te/tGLFCkVERCgxMVGrV6/WF198oc8++0yfffaZXn/9dY0bN05vvPFGI70SoG4IToCXef/99+VwOPTFF19UaG37+uuv1/gxajPVp23btjIMQ61bt/b89by2CgsLNXLkSG3cuFFffvlljf+KXtUv3idy4YUXaubMmVq+fLn69u1b6zrbtm2rq6++Wq+88or69OlTo2MeeOABvfXWWzVaqO7Wpk0btWnTptL2U089tU4B5WhjxozRQw89pL1799a4qUFDSUpKqtTd0OFwqE2bNif9Oi+66CLdfPPNWrFihd55553j7te2bVt9+eWX6t+/f72FiDZt2ig2NrbSFM09e/ZUavntvtCz+y/7bdu21Zo1a3TOOeec8Puwbdu2crlc+u2339S9e/fj7lfbi7KmpqZKKrue0NHnYHFxsbZu3ep5X9z7bd68WWeddZZnv5KSEm3btk1du3at0fO99957OuusszRr1qwK27Ozs+u9OYxZ6uNnckOqr/eyuudwuVzatGmTOnXq5Nm+b98+ZWdne2pwO/PMM3XmmWfq8ccf19y5czV27Fi9/fbbnj9EhISEaOTIkRo5cqRcLpduu+02vfLKK3rwwQcrjWoB3oQ1ToCXCQoKksViqTDlZdu2bfrggw9q/Bjui+jWpCPexRdfrKCgIE2ePLnSXzANw1BmZuYJjy8tLdUVV1yh5cuXa968ebUKNElJSTr33HMrfJzI3/72N4WHh+vGG2/Uvn37Kt2/ZcuWaqfUPfDAA3I6nXryySdrVOPRYSs9Pb1GxzSktm3b6tlnn9WUKVPUu3dvs8tpMBEREZoxY4YmTZqkkSNHHne/yy+/XKWlpXr00Ucr3VdSUnLC74Hvv/9e+fn5lbavXLlSmZmZlaa7lZSU6JVXXvHcLi4u1iuvvKImTZqoR48ennp2796tV199tdLjHj582PN8o0ePltVq1SOPPFJpZObo78Pw8PBadbY899xzFRISoueff77C48yaNUs5OTmeDow9e/ZUfHy8Xn311QrriObMmVOrtXdBQUGVfm7MmzdPu3fvrvFjnIz6bEd+PPXxM7kh1dd7eSIXXHCBpLJOj0dzj6y6z6uDBw9WOh/cfxhwT3E89v8Uq9XqCXc1aUEPmIkRJ8DLjBgxQk8//bTOP/98jRkzRvv379eLL76odu3a6ZdffqnRY4SGhurUU0/VO++8ow4dOiguLk5dunRRly5dKu3btm1bPfbYY5o4caK2bdum0aNHKzIyUlu3btX8+fP1l7/8Rffcc89xn+uvf/2rPvroI40cOVJZWVmVLnh7ogtM1lbbtm01d+5cXXHFFerUqZPGjRunLl26qLi4WN99953mzZtX7QVm3UGoNlNC3FP8NmzYoM6dO5/kqzh5x16r6kS++uorFRYWVto+evToKs8Hb1KT5iKDBw/WzTffrClTpmj16tUaNmyYbDabNm3apHnz5um555477vqoN998U3PmzNFFF12kHj16KCQkRL///rtee+01ORwO3X///RX2T05O1rRp07Rt2zZ16NBB77zzjlavXq2ZM2d61gJec801evfdd3XLLbfom2++Uf/+/VVaWqr169fr3Xff1RdffKGePXuqXbt2+sc//qFHH31UAwcO1MUXXyy73a4ffvhBycnJmjJliiSpR48emjFjhh577DG1a9dOiYmJJ1zX1qRJE02cOFGTJ0/W+eefr1GjRmnDhg166aWX1KtXL8/3Y0hIiCZNmqQJEybo7LPP1uWXX65t27Zp9uzZatu2bY1Hui688EI98sgjuv7669WvXz+tXbtWc+bMqXLEtSGsXLlSZ511lh5++OF6axBxrPr4mdyQ6uu93Lx5sx577LFK208//XSNGDFC1157rWbOnKns7GwNHjxYK1eu1BtvvKHRo0d7RrreeOMNvfTSS7rooovUtm1b5eXl6dVXX1VUVJQnfN14443KysrS2WefrRYtWmj79u164YUX1L179wqjWYBXavxGfkDgOrrl8onMmjXLaN++vWG3242OHTsar7/+eqU24oZRuQX10b777jujR48eRkhISIV2vVU9jmEYxvvvv28MGDDACA8PN8LDw42OHTsaaWlpxoYNG05Yq7tN8/E+GsLGjRuNm266yWjVqpUREhJiREZGGv379zdeeOEFo7Cw0LPf8Vrsbtq0yQgKCjphO/JjXXvttYYkU9uRV/f4VbUjP97Hm2++aRhG9W2Ia6s+2pFX9/hV1Ttz5kyjR48eRmhoqBEZGWmcdtppxt/+9jdP+3nDqNzu+ZdffjHuvfde44wzzjDi4uKM4OBgIykpybjsssuMVatWVXh8dyv6H3/80ejbt6/hcDiM1NRUY/r06ZVqKS4uNqZNm2Z07tzZsNvtRmxsrNGjRw9j8uTJRk5OToV9X3vtNeP000/37Dd48GBj4cKFnvvT09ONESNGGJGRkRVaS1f39Zo+fbrRsWNHw2azGU2bNjVuvfVW4+DBg5X2e/75543U1FTDbrcbvXv3NpYtW2b06NHDOP/88z37VNW6362wsND461//aiQlJRmhoaFG//79jeXLlx+3tfaxj3G811HT876+25Ef7+dpTX8mH68d+bGvz133N99849lW06+Z+3v72J8nNXkvjyc1NfW4Pyv+/Oc/G4ZhGE6n05g8ebLRunVrw2azGSkpKcbEiRMr/MxdtWqVcdVVVxktW7Y07Ha7kZiYaFx44YXGjz/+6NnnvffeM4YNG2YkJiYaISEhRsuWLY2bb77Z2Lt3b7V1AmazGIYJqwsBAPAhQ4YMUUZGxkk1J/EFLpdLTZo00cUXX1zldEP4Dt5LoP6xxgkAgABUWFhYaT3Kv//9b2VlZWnIkCHmFIU64b0EGgdrnAAACEArVqzQXXfdpcsuu0zx8fFatWqVZs2apS5duuiyyy4zuzzUAu8l0DgITgAABKBWrVopJSVFzz//vLKyshQXF6dx48Zp6tSpCgkJMbs81ALvJdA4WOMEAAAAANVgjRMAAAAAVIPgBAAAAADVCLg1Ti6XS3v27FFkZGSNLwoHAAAAwP8YhqG8vDwlJyfLaj3xmFLABac9e/YoJSXF7DIAAAAAeImdO3eqRYsWJ9wn4IJTZGSkpLIvTlRUlMnVSE6nU//73/80bNgw2Ww2s8uBn+N8Q2PjnENj4nxDY+Oc8325ublKSUnxZIQTCbjg5J6eFxUV5TXBKSwsTFFRUXzDocFxvqGxcc6hMXG+obFxzvmPmizhoTkEAAAAAFSD4AQAAAAA1SA4AQAAAEA1CE4AAAAAUA2CEwAAAABUg+AEAAAAANUgOAEAAABANQhOAAAAAFANghMAAAAAVIPgBAAAAADVIDgBAAAAQDUITgAAAABQDYITAAAAAFSD4AQAAAAA1SA4AQAAAEA1CE4AAAAAUI1gswsIZDmHnVq8YZ+25pldCQAAAIATYcTJRK8u/kMT3l6jxXt5GwAAAABvxm/sJhrYPkGStCHHIpfLMLkaAAAAAMdDcDLRGamxCrcHKb/Eot/2Ml8PAAAA8FYEJxPZgqzq2zpOkrR0c4bJ1QAAAAA4HoKTyQa0i5ckLdmcaXIlAAAAAI6H4GSyAeXrnFbtyNahohKTqwEAAABQFYKTyVLjwpRgN1TiMrRiC6NOAAAAgDciOHmBU2LKOuot3nTA5EoAAAAAVIXg5AU6uYPTRoITAAAA4I0ITl6gfZShYKtF2zILtCOzwOxyAAAAAByD4OQFHMFS95RoSUzXAwAAALwRwclLDGxX1l2P6XoAAACA9yE4eQn39ZyWb8mUs9RlcjUAAAAAjkZw8hKdk6MUG2ZTXlGJVu/MNrscAAAAAEchOHmJIKtFA9o3kSQtYboeAAAA4FUITl5kYPuydU6LNmWYXAkAAACAoxGcvMig8hGnX3Zl62B+scnVAAAAAHAjOHmRZtEOndI0UoYhLdvCqBMAAADgLQhOXsY9XY+25AAAAID3IDh5mUEdyqbrLd6YIcMwTK4GAAAAgERw8jq9W8fJHmxVem6hNu8/ZHY5AAAAAERw8joOW5B6t46TJC1iuh4AAADgFQhOXmhw+XS9JbQlBwAAALwCwckLDSxvS/791kwVOktNrgYAAAAAwckLdWgaoWZRDhU6XfphW5bZ5QAAAAABj+DkhSwWi6ctOdP1AAAAAPMRnLzUQE9bchpEAAAAAGYjOHmpge0SZLFI69PztC+30OxyAAAAgIBGcPJSseEh6to8WhLT9QAAAACzEZy8mLu7HtP1AAAAAHMRnLzYoPJ1Tks3Z8jlMkyuBgAAAAhcBCcvdnrLGEXYg5WVX6xf9+SaXQ4AAAAQsAhOXswWZFXftvGSpMWbmK4HAAAAmIXg5OUG0ZYcAAAAMB3BycsNKr8Q7k/bD+pQUYnJ1QAAAACBieDk5VLjw5UaH6YSl6HlWzLNLgcAAAAISAQnHzCovC35EtY5AQAAAKYgOPmAgeXT9VjnBAAAAJiD4OQD+raNV7DVom2ZBdqRWWB2OQAAAEDAITj5gEiHTWe0jJVEW3IAAADADAQnHzGoA9P1AAAAALMQnHyE+3pOy7dkylnqMrkaAAAAILAQnHxE5+RoxYbZlFdUotU7s80uBwAAAAgoBCcfEWS1aEB5W3Km6wEAAACNi+DkQwa525JvyjC5EgAAACCwEJx8yMDyEadfdmXrYH6xydUAAAAAgYPg5EOaRTt0StNIGYa0dDOjTgAAAEBjITj5GHdb8iVczwkAAABoNAQnHzPQ0yAiQ4ZhmFwNAAAAEBgITj6md+s42YOtSs8t1Kb9h8wuBwAAAAgIBCcf47AFqXfrOEm0JQcAAAAaC8HJBw3uUD5dj7bkAAAAQKMgOPmgQeXB6fs/MlXoLDW5GgAAAMD/EZx8UPvECDWLcqioxKUftmWZXQ4AAADg9whOPshisWhg+7K25KxzAgAAABoewclHuafrLWGdEwAAANDgCE4+akC7BFks0vr0PO3LLTS7HAAAAMCvEZx8VGx4iLo2j5bEdD0AAACgoZkanKZMmaJevXopMjJSiYmJGj16tDZs2FDtcdnZ2UpLS1NSUpLsdrs6dOigTz/9tBEq9i5M1wMAAAAah6nBadGiRUpLS9OKFSu0cOFCOZ1ODRs2TPn5+cc9pri4WEOHDtW2bdv03nvvacOGDXr11VfVvHnzRqzcOwxsXxaclm7OkMtlmFwNAAAA4L+CzXzyzz//vMLt2bNnKzExUT/99JMGDRpU5TGvvfaasrKy9N1338lms0mSWrVq1dCleqXTW8Yowh6srPxi/bonV6e1iDa7JAAAAMAvmRqcjpWTkyNJiouLO+4+H330kfr27au0tDR9+OGHatKkicaMGaO///3vCgoKqrR/UVGRioqKPLdzc3MlSU6nU06ns55fQe25a6hrLWe2jtWX6w/om/Xp6tg0rD5Lgx862fMNqC3OOTQmzjc0Ns4531eb985iGIZXzPFyuVwaNWqUsrOztXTp0uPu17FjR23btk1jx47Vbbfdps2bN+u2227T7bffrocffrjS/pMmTdLkyZMrbZ87d67Cwnw/aCxNt2je1iC1jTR0e5dSs8sBAAAAfEZBQYHGjBmjnJwcRUVFnXBfrwlOt956qz777DMtXbpULVq0OO5+HTp0UGFhobZu3eoZYXr66af11FNPae/evZX2r2rEKSUlRRkZGdV+cRqD0+nUwoULNXToUM/Uw9rYkVWgc55ZqmCrRT/cf5Yi7F41iAgvc7LnG1BbnHNoTJxvaGycc74vNzdXCQkJNQpOXvFb9vjx47VgwQItXrz4hKFJkpKSkmSz2SpMy+vUqZPS09NVXFyskJCQCvvb7XbZ7fZKj2Oz2bzqBK9rPW2bRis1PkzbMwv0445cDT21aQNUB3/jbec//B/nHBoT5xsaG+ec76rN+2ZqVz3DMDR+/HjNnz9fX3/9tVq3bl3tMf3799fmzZvlcrk82zZu3KikpKRKoSlQDCrvrsf1nAAAAICGYWpwSktL01tvvaW5c+cqMjJS6enpSk9P1+HDhz37jBs3ThMnTvTcvvXWW5WVlaU77rhDGzdu1CeffKInnnhCaWlpZrwEr3Dkek4EJwAAAKAhmDpVb8aMGZKkIUOGVNj++uuv67rrrpMk7dixQ1brkXyXkpKiL774QnfddZe6du2q5s2b64477tDf//73xirb65zZJk7BVou2ZRZoR2aBWsb7ftMLAAAAwJuYGpxq0pfi22+/rbStb9++WrFiRQNU5JsiHTadkRqrlVuztGjTAV0Tn2p2SQAAAIBfMXWqHurPoPYJkqQlrHMCAAAA6h3ByU+41zl9tyVTzlJXNXsDAAAAqA2Ck5/okhyt2DCbDhWVaPXObLPLAQAAAPwKwclPWK0WDaAtOQAAANAgCE5+xL3OieAEAAAA1C+Ckx9xr3P6ZXeODuYXm1wNAAAA4D8ITn6kaZRDpzSNlGFISzdnmF0OAAAA4DcITn5mUAem6wEAAAD1jeDkZ9zT9ZZsyqjRBYYBAAAAVI/g5Gd6tYqTPdiq9NxCbdp/yOxyAAAAAL9AcPIzDluQ+rSJl8R0PQAAAKC+EJz8kKct+SYaRAAAAAD1geDkh9zrnL7/I1OFzlKTqwEAAAB8H8HJD7VPjFCzKIeKSlz6YVuW2eUAAAAAPo/g5IcsFosGtqctOQAAAFBfCE5+yj1db/FG1jkBAAAAJ4vg5KcGtEuQxSJt2JenfbmFZpcDAAAA+DSCk5+KDQ9R1+bRkpiuBwAAAJwsgpMf80zXoy05AAAAcFIITn7MHZyWbjogl8swuRoAAADAdxGc/Fj3lBhF2IN1sMCpdXtyzC4HAAAA8FkEJz9mC7KqX9t4SdISpusBAAAAdUZw8nMDy6frLaJBBAAAAFBnBCc/N7h9WXBatf2g8gqdJlcDAAAA+CaCk59rGR+mVvFhKnEZWvFHltnlAAAAAD6J4BQABpaPOnE9JwAAAKBuCE4B4Mj1nAhOAAAAQF0QnAJA37bxCrZatD2zQNsz880uBwAAAPA5BKcAEGEP1hmpsZKkxbQlBwAAAGqN4BQgBpdP11vCOicAAACg1ghOAWJg+wRJ0ndbMuUsdZlcDQAAAOBbCE4BoktytOLCQ3SoqEQ/78g2uxwAAADApxCcAoTVatGAdmWjTkvorgcAAADUCsEpgLin63E9JwAAAKB2CE4BxH09p1925ygrv9jkagAAAADfQXAKIE2jHOrYLFKGIS3bTFtyAAAAoKYITgGG6XoAAABA7RGcAox7ut7iTQdkGIbJ1QAAAAC+geAUYHq1ipM92Kp9uUXatP+Q2eUAAAAAPoHgFGActiD1aRMviel6AAAAQE0RnALQIPc6p000iAAAAABqguAUgNzrnL7/I1OFzlKTqwEAAAC8H8EpALVPjFCzKIeKSlxauTXL7HIAAAAAr0dwCkAWi0WDOpRN11uyiXVOAAAAQHUITgFqYPvytuQbWecEAAAAVIfgFKAGtEuQxSJt2Jen9JxCs8sBAAAAvBrBKUDFhoeoa4sYSUzXAwAAAKpDcApgtCUHAAAAaobgFMDcbcmXbjogl8swuRoAAADAexGcAlj3lBhF2oN1sMCpdXtyzC4HAAAA8FoEpwBmC7Kqb9t4SdLijaxzAgAAAI6H4BTg3NP1WOcEAAAAHB/BKcANKr+e06rtB5VX6DS5GgAAAMA7EZwCXMv4MLWKD1OJy9DyLZlmlwMAAAB4JYITPNP1ljBdDwAAAKgSwQka2N69zokGEQAAAEBVCE5Q37bxCrZatD2zQNsz880uBwAAAPA6BCcowh6sHqmxkuiuBwAAAFSF4ARJR7Ul53pOAAAAQCUEJ0g60pZ8+ZZMOUtdJlcDAAAAeBeCEyRJnZOjFBceokNFJfp5R7bZ5QAAAABeheAESZLVatGAdgmSmK4HAAAAHIvgBI8j13MiOAEAAABHIzjBY2D7shGnX3bnKCu/2ORqAAAAAO9BcIJH0yiHOjaLlGFISzfTlhwAAABwIzihAs90PdY5AQAAAB4EJ1Tgnq63eNMBGYZhcjUAAACAdyA4oYJereLksFm1L7dIm/YfMrscAAAAwCsQnFCBwxakPq3jJdGWHAAAAHAjOKES93S9RQQnAAAAQBLBCVUYXN4gYuXWLBU6S02uBgAAADCfqcFpypQp6tWrlyIjI5WYmKjRo0drw4YNNT7+7bfflsVi0ejRoxuuyADULjFCzaIcKipxaeXWLLPLAQAAAExnanBatGiR0tLStGLFCi1cuFBOp1PDhg1Tfn5+tcdu27ZN99xzjwYOHNgIlQYWi8WiQR3Ku+sxXQ8AAABQsJlP/vnnn1e4PXv2bCUmJuqnn37SoEGDjntcaWmpxo4dq8mTJ2vJkiXKzs5u4EoDz6AOTfTuj7u0ZBMXwgUAAABMDU7HysnJkSTFxcWdcL9HHnlEiYmJ+vOf/6wlS5accN+ioiIVFRV5bufm5kqSnE6nnE7nSVZ88tw1eEMtR+udGi2LRdqwL087M/PULMphdkmoB956vsF/cc6hMXG+obFxzvm+2rx3FsNLrnLqcrk0atQoZWdna+nSpcfdb+nSpbryyiu1evVqJSQk6LrrrlN2drY++OCDKvefNGmSJk+eXGn73LlzFRYWVl/l+6Wn1wZp+yGLrmpbqjMTveI0AQAAAOpNQUGBxowZo5ycHEVFRZ1wX68ZcUpLS9O6detOGJry8vJ0zTXX6NVXX1VCQkKNHnfixIm6++67Pbdzc3OVkpKiYcOGVfvFaQxOp1MLFy7U0KFDZbPZzC6ngo32zXrx2z+UG9ZcF1zQ1exyUA+8+XyDf+KcQ2PifENj45zzfe7ZaDXhFcFp/PjxWrBggRYvXqwWLVocd78tW7Zo27ZtGjlypGeby+WSJAUHB2vDhg1q27ZthWPsdrvsdnulx7LZbF51gntbPZI0pGNTvfjtH/puS6asQcEKslrMLgn1xBvPN/g3zjk0Js43NDbOOd9Vm/fN1OBkGIYmTJig+fPn69tvv1Xr1q1PuH/Hjh21du3aCtseeOAB5eXl6bnnnlNKSkpDlhtwuqfEKNIerIMFTv26J0ddW8SYXRIAAABgClODU1pamubOnasPP/xQkZGRSk9PlyRFR0crNDRUkjRu3Dg1b95cU6ZMkcPhUJcuXSo8RkxMjCRV2o6TZwuyql+7eH3x6z4t3niA4AQAAICAZep1nGbMmKGcnBwNGTJESUlJno933nnHs8+OHTu0d+9eE6sMbAPbN5EkLd5IW3IAAAAELtOn6lXn22+/PeH9s2fPrp9iUKXBHcqC06odB5VX6FSkg/m7AAAACDymjjjB+6XEhalVfJhKXIaWb8k0uxwAAADAFAQnVGtQ+ajT4k0HTK4EAAAAMAfBCdUaVL7Oackm1jkBAAAgMBGcUK0z28Yr2GrR9swCbc/MN7scAAAAoNERnFCtCHuweqTGSpIWM+oEAACAAERwQo141jltZJ0TAAAAAg/BCTXiXue0fEumnKUuk6sBAAAAGhfBCTXSOTlKceEhOlRUop93ZJtdDgAAANCoCE6oEavVogHtEiQxXQ8AAACBh+CEGuN6TgAAAAhUBCfU2KD2ZSNOa3fnKCu/2ORqAAAAgMZDcEKNJUY51LFZpAxDWrqZtuQAAAAIHAQn1Ip7ut4S1jkBAAAggBCcUCvutuSLNx2QYRgmVwMAAAA0DoITaqVnq1g5bFbtyy3Sxn2HzC4HAAAAaBQEJ9SKwxakPq3jJUlL6K4HAACAAEFwQq251zktYp0TAAAAAgTBCbXmbku+cmuWCp2lJlcDAAAANDyCE2qtXWKEkqIdKipxaeXWLLPLAQAAABocwQm1ZrFYNLB81Gkx0/UAAAAQAAhOqBP3OqfFNIgAAABAACA4oU4GtEuQxSJt3HdI6TmFZpcDAAAANCiCE+okJixEXVvESGLUCQAAAP6P4IQ6G1y+zmnJpgyTKwEAAAAaFsEJdeZe57R00wGVugyTqwEAAAAaDsEJddYtJUaR9mAdLHBq3e4cs8sBAAAAGgzBCXVmC7KqX7t4SdIS1jkBAADAjxGccFI8bck3ss4JAAAA/ovghJMyqH1ZcFq146DyCp0mVwMAAAA0DIITTkpKXJhaJ4SrxGVo+ZZMs8sBAAAAGgTBCSdtYHlbcq7nBAAAAH9FcMJJc0/X43pOAAAA8FcEJ5y0vm3jZQuyaHtmgbZn5ptdDgAAAFDvCE44aeH2YJ3RMlaStHgj0/UAAADgfwhOqBeetuRM1wMAAIAfIjihXgwuD07Lt2TKWeoyuRoAAACgfhGcUC9OTYpSfHiIDhWVaNX2g2aXAwAAANQrghPqhdVq0YDytuR01wMAAIC/ITih3gxs717nRIMIAAAA+BeCE+rNoPIRp7W7c5SVX2xyNQAAAED9ITih3iRGOdSxWaQMQ1q6mel6AAAA8B8EJ9QrT1tyrucEAAAAP0JwQr0aVL7OacmmAzIMw+RqAAAAgPpBcEK96tkqVg6bVftyi7Rx3yGzywEAAADqBcEJ9cphC1Kf1vGSmK4HAAAA/0FwQr3zrHOiLTkAAAD8BMEJ9W5wh7K25Cu3ZqnQWWpyNQAAAMDJIzih3rVtEqGkaIeKSlz6fmuW2eUAAAAAJ61OwWnnzp3atWuX5/bKlSt15513aubMmfVWGHyXxWI50l2PdU4AAADwA3UKTmPGjNE333wjSUpPT9fQoUO1cuVK/eMf/9AjjzxSrwXCNw0sn67HOicAAAD4gzoFp3Xr1ql3796SpHfffVddunTRd999pzlz5mj27Nn1WR981IB2CbJapI37Dik9p9DscgAAAICTUqfg5HQ6ZbfbJUlffvmlRo0aJUnq2LGj9u7dW3/VwWfFhIWoa4sYSYw6AQAAwPfVKTh17txZL7/8spYsWaKFCxfq/PPPlyTt2bNH8fHx9VogfNeg9uXT9VjnBAAAAB9Xp+A0bdo0vfLKKxoyZIiuuuoqdevWTZL00UcfeabwAe7rOS3dnKFSl2FyNQAAAEDdBdfloCFDhigjI0O5ubmKjY31bP/LX/6isLCweisOvq17Sowi7cHKLnBq3e4cdUuJMbskAAAAoE7qNOJ0+PBhFRUVeULT9u3b9eyzz2rDhg1KTEys1wLhu4KDrOrXrmzqJtP1AAAA4MvqFJz+9Kc/6d///rckKTs7W3369NG//vUvjR49WjNmzKjXAuHb3NP1lmzKMLkSAAAAoO7qFJxWrVqlgQMHSpLee+89NW3aVNu3b9e///1vPf/88/VaIHyb+0K4q3YcVF6h0+RqAAAAgLqpU3AqKChQZGSkJOl///ufLr74YlmtVp155pnavn17vRYI35YSF6bWCeEqcRn6bkum2eUAAAAAdVKn4NSuXTt98MEH2rlzp7744gsNGzZMkrR//35FRUXVa4Hwfe625Eu4nhMAAAB8VJ2C00MPPaR77rlHrVq1Uu/evdW3b19JZaNPp59+er0WCN83sHy63uKNrHMCAACAb6pTO/JLL71UAwYM0N69ez3XcJKkc845RxdddFG9FQf/0LdtvGxBFu3IKtD2zHylxoebXRIAAABQK3UacZKkZs2a6fTTT9eePXu0a9cuSVLv3r3VsWPHeisO/iHcHqweqWWt62lLDgAAAF9Up+Dkcrn0yCOPKDo6WqmpqUpNTVVMTIweffRRuVyu+q4RfsA9XW8R0/UAAADgg+o0Ve8f//iHZs2apalTp6p///6SpKVLl2rSpEkqLCzU448/Xq9FwvcN7tBET32xQcu3ZMhZ6pItqM6DnQAAAECjq1NweuONN/R///d/GjVqlGdb165d1bx5c912220EJ1RyalKU4sNDlJlfrFXbD6pPm3izSwIAAABqrE5/9s/KyqpyLVPHjh2VlZV10kXB/1itFg0ob0u+mLbkAAAA8DF1Ck7dunXT9OnTK22fPn26unbtetJFwT8NKl/ntGQT65wAAADgW+o0Ve/JJ5/UiBEj9OWXX3qu4bR8+XLt3LlTn376ab0WCP8xsHzEae3uHGXlFysuPMTkigAAAICaqdOI0+DBg7Vx40ZddNFFys7OVnZ2ti6++GL9+uuvevPNN+u7RviJxCiHOjaLlGFISzcz6gQAAADfUacRJ0lKTk6u1ARizZo1mjVrlmbOnHnShcE/De7QROvT87R44wGN6pZsdjkAAABAjdATGo1qoGed0wEZhmFyNQAAAEDNmBqcpkyZol69eikyMlKJiYkaPXq0NmzYcMJjXn31VQ0cOFCxsbGKjY3Vueeeq5UrVzZSxThZPVvFymGzal9ukTbuO2R2OQAAAECNmBqcFi1apLS0NK1YsUILFy6U0+nUsGHDlJ+ff9xjvv32W1111VX65ptvtHz5cqWkpGjYsGHavXt3I1aOunLYgnRm+TWcFm+kLTkAAAB8Q63WOF188cUnvD87O7tWT/75559XuD179mwlJibqp59+0qBBg6o8Zs6cORVu/9///Z/ef/99ffXVVxo3blytnh/mGNi+ib7dcECLNx3QTYPamF0OAAAAUK1aBafo6Ohq7z+Z8JKTkyNJiouLq/ExBQUFcjqdxz2mqKhIRUVFntu5ubmSJKfTKafTWeda64u7Bm+opbH0ax0jSfp+a5byCgrlsAWZW1AACcTzDebinENj4nxDY+Oc8321ee8shpes0He5XBo1apSys7O1dOnSGh9322236YsvvtCvv/4qh8NR6f5JkyZp8uTJlbbPnTtXYWFhJ1Uz6sYwpEmrgpRdbNEtnUrVKcYrTkEAAAAEmIKCAo0ZM0Y5OTmKioo64b51bkde39LS0rRu3bpahaapU6fq7bff1rfffltlaJKkiRMn6u677/bczs3N9ayLqu6L0xicTqcWLlyooUOHymazmV1Oo1nm/FXzftqtotg2umD4KWaXEzAC9XyDeTjn0Jg439DYOOd8n3s2Wk14RXAaP368FixYoMWLF6tFixY1Ouaf//ynpk6dqi+//FJdu3Y97n52u112u73SdpvN5lUnuLfV09CGnNJU837arWVbMgPqdXuLQDvfYD7OOTQmzjc0Ns4531Wb983UrnqGYWj8+PGaP3++vv76a7Vu3bpGxz355JN69NFH9fnnn6tnz54NXCUaQv928bJapI37DmlvzmGzywEAAABOyNTglJaWprfeektz585VZGSk0tPTlZ6ersOHj/wiPW7cOE2cONFze9q0aXrwwQf12muvqVWrVp5jDh3imkC+JCYsRF1bxEiSlmzKMLcYAAAAoBqmBqcZM2YoJydHQ4YMUVJSkufjnXfe8eyzY8cO7d27t8IxxcXFuvTSSysc889//tOMl4CTMKhDE0lczwkAAADez9Q1TjVp6Pftt99WuL1t27aGKQaNblD7BD3/1SYt3ZyhUpehIKvF7JIAAACAKpk64oTA1j0lRpGOYGUXOLVud47Z5QAAAADHRXCCaYKDrOrfNkES0/UAAADg3QhOMNXADuXBaRPBCQAAAN6L4ARTDWpf1iBi1Y5s5RU6Ta4GAAAAqBrBCaZKiQtT64RwlboMfbcl0+xyAAAAgCoRnGC6Qe3LpustYboeAAAAvBTBCaY7cj0nLoQLAAAA70RwgunObBMvW5BFO7IKtC0j3+xyAAAAgEoITjBduD1YPVJjJTFdDwAAAN6J4ASv4J6ut4jpegAAAPBCBCd4BXdb8uVbMlRc4jK5GgAAAKAighO8wqlJUYoPD1F+cal+3nHQ7HIAAACACghO8ApWq0UDy9uSL2adEwAAALwMwQleY2B72pIDAADAOxGc4DXcI07r9uQo81CRydUAAAAARxCc4DUSoxzq2CxShiEt3cyoEwAAALwHwQleZXB5W/IlmwhOAAAA8B4EJ3iVQZ7gdECGYZhcDQAAAFCG4ASv0iM1Vg6bVftyi7RhX57Z5QAAAACSCE7wMg5bkM5sEy9JWkJ3PQAAAHgJghO8ziB3W3Ku5wQAAAAvQXCC1xnUoawt+fdbs3S4uNTkagAAAACCE7xQ2yYRSo52qLjEpZXbsswuBwAAACA4wftYLBZPd73FG5muBwAAAPMRnOCVBrY/0pYcAAAAMBvBCV6pf7t4WS3Sxn2HtDfnsNnlAAAAIMARnOCVYsJC1LVFjCTakgMAAMB8BCd4Lc86J6brAQAAwGQEJ3itweVtyZduzlCpyzC5GgAAAAQyghO8VrcWMYp0BCu7wKm1u3PMLgcAAAABjOAErxUcZFX/tmWjTktoSw4AAAATEZzg1VjnBAAAAG9AcIJXG9i+bMRp1Y5s5RY6Ta4GAAAAgYrgBK+WEhemNgnhKnUZWr4l0+xyAAAAEKAITvB67lGnxaxzAgAAgEkITvB67nVOSzZxIVwAAACYg+AEr3dmm3jZgizakVWgbRn5ZpcDAACAAERwgtcLtwerR2qsJLrrAQAAwBwEJ/gET1vyjUzXAwAAQOMjOMEnDGpfFpyWb8lQcYnL5GoAAAAQaAhO8AmnJkUpPjxE+cWlWrXjoNnlAAAAIMAQnOATrFaLpy35EtY5AQAAoJERnOAzWOcEAAAAsxCc4DMGlI84rduTo8xDRSZXAwAAgEBCcILPSIx0qFNSlAxDWrqZUScAAAA0HoITfMqg8lEnpusBAACgMRGc4FPc65yWbDogwzBMrgYAAACBguAEn9KzVawcNqv25xVpw748s8sBAABAgCA4wafYg4N0Zpt4SdLijbQlBwAAQOMgOMHnDGpfNl3vm/VM1wMAAEDjIDjB5ww5pSw4Lf8jU3+dt0aHi0tNrggAAAD+juAEn9OmSYQeuvBUWS3Sf1ft1sUzvtP2zHyzywIAAIAfIzjBJ90woLXeurGPEiJC9PveXI18Yam++n2f2WUBAADATxGc4LP6tU3QggkDdUbLGOUWlujPb/yop/+3QaUu1j0BAACgfhGc4NOaRTv09l/66rp+rSRJz3+9Wde9vlIH84vNLQwAAAB+heAEnxcSbNWkUZ317BXd5bBZtWRThi58Yal+2ZVtdmkAAADwEwQn+I3RpzfXB2n91So+TLuzD+vSGcv19sodZpcFAAAAP0Bwgl/p2CxKH00YoKGnNlVxqUv3/Xet/vbeGhU6aVkOAACAuiM4we9EOWx65eoeuve8U2S1SO/+uEuXvvyddmYVmF0aAAAAfBTBCX7JarUo7ax2evPPfRQXHqJ1u3M1cvpSfbthv9mlAQAAwAcRnODX+rdL0IIJA9QtJUbZBU5dP/sHPfflJrloWQ4AAIBaIDjB7yXHhOrdm8/U2D4tZRjSM19u1J/f+EE5BU6zSwMAAICPIDghINiDg/T4Rafpn5d1kz3Yqm82HNCF05do3e4cs0sDAACADyA4IaBc2qOF/ntbP7WMC9POrMO6ZMZ3mvfjTrPLAgAAgJcjOCHgdE6O1sfjB+jsjokqKnHp3vd+0f3z16qohJblAAAAqBrBCQEpOsym/xvXU38d2kEWizT3+x26/OXl2p192OzSAAAA4IUITghYVqtFE85pr9nX91ZMmE1rduXowueXaOmmDLNLAwAAgJchOCHgDe7QRB+PH6DTmkfrYIFT4177Xi9+s5mW5QAAAPAgOAGSUuLCNO+WvrqyV4pchvTUFxv0lzd/VM5hWpYDAACA4AR4OGxBmnpJV0275DSFBFv15e/7NWr6Uv2+N9fs0gAAAGAyghNwjCt6tdT7t/RT85hQbc8s0EUvLdP8n3eZXRYAAABMZGpwmjJlinr16qXIyEglJiZq9OjR2rBhQ7XHzZs3Tx07dpTD4dBpp52mTz/9tBGqRSA5rUW0FkwYoMEdmqjQ6dJd76zRQx+uU3GJy+zSAAAAYAJTg9OiRYuUlpamFStWaOHChXI6nRo2bJjy8/OPe8x3332nq666Sn/+85/1888/a/To0Ro9erTWrVvXiJUjEMSGh+i163rpjnPaS5L+vXy7rpi5XHtzaFkOAAAQaEwNTp9//rmuu+46de7cWd26ddPs2bO1Y8cO/fTTT8c95rnnntP555+ve++9V506ddKjjz6qM844Q9OnT2/EyhEogqwW3TW0g16/rpeiHMH6eUe2Lnx+qb7bQstyAACAQBJsdgFHy8nJkSTFxcUdd5/ly5fr7rvvrrDtvPPO0wcffFDl/kVFRSoqKvLczs0tW+jvdDrldJrfMc1dgzfUguMb0DZW8289U+P/s0a/p+fp6v/7Xn8d2l43DWgli8Vidnk1xvmGxsY5h8bE+YbGxjnn+2rz3lkMw/CKi9W4XC6NGjVK2dnZWrp06XH3CwkJ0RtvvKGrrrrKs+2ll17S5MmTtW/fvkr7T5o0SZMnT660fe7cuQoLC6uf4hEwikuleVutWnmgbLC2a5xLY9u65PCqP0EAAACgJgoKCjRmzBjl5OQoKirqhPt6za97aWlpWrdu3QlDU11MnDixwghVbm6uUlJSNGzYsGq/OI3B6XRq4cKFGjp0qGw2m9nloAb+ZBh6+8ddevST9foly6o8S4RevKq72jeNMLu0anG+obFxzqExcb6hsXHO+T73bLSa8IrgNH78eC1YsECLFy9WixYtTrhvs2bNKo0s7du3T82aNatyf7vdLrvdXmm7zWbzqhPc2+rBiY3r10bdUuJ061s/aWtmgS555XtNu7SrRnVLNru0GuF8Q2PjnENj4nxDY+Oc8121ed9MbQ5hGIbGjx+v+fPn6+uvv1br1q2rPaZv37766quvKmxbuHCh+vbt21BlAlXqlhKjBbcP1IB2CTrsLNXt//lZkz/+Vc5SWpYDAAD4G1ODU1pamt566y3NnTtXkZGRSk9PV3p6ug4fPtLuedy4cZo4caLn9h133KHPP/9c//rXv7R+/XpNmjRJP/74o8aPH2/GS0CAiwsP0Rs39Nb4s9pJkl5ftk1XzVyh/bmFJlcGAACA+mRqcJoxY4ZycnI0ZMgQJSUleT7eeecdzz47duzQ3r17Pbf79eunuXPnaubMmerWrZvee+89ffDBB+rSpYsZLwFQkNWie847Ra+O66lIe7B+3H5QFzy/VN//kWl2aQAAAKgnpq5xqklDv2+//bbStssuu0yXXXZZA1QE1N3QU5vq4wkDdMtbP2l9ep7G/N/3mji8o/48oLVPtSwHAABAZaaOOAH+plVCuObf1l8Xnd5cpS5Dj33yu8bP/VmHikrMLg0AAAAngeAE1LPQkCA9fXk3PfKnzrIFWfTJ2r360/Sl2rw/z+zSAAAAUEcEJ6ABWCwWjevbSu/c3FfNohzaciBff5q+TJ+u3Vv9wQAAAPA6BCegAZ3RMlYLbh+gvm3ilV9cqtvmrNLjn/ymElqWAwAA+BSCE9DAEiLsevPPvXXz4DaSpFeXbNXY//te+/NoWQ4AAOArCE5AIwgOsmri8E56+eozFGEP1vdbs3Th80v147Yss0sDAABADRCcgEZ0fpckfTi+vzo0jdD+vCJdOXOFXl+2tUat+QEAAGAeghPQyNo2idD82/prZLdklbgMTf74N93x9moVFNOyHAAAwFsRnAAThNuD9fyV3fXwyFMVbLXoozV7NPrFZfrjwCGzSwMAAEAVCE6ASSwWi67v31r/+cuZSoy0a+O+Qxo1fZk+X5dudmkAAAA4BsEJMFmvVnFacPsA9W4dp0NFJbrlrZ809bP1tCwHAADwIgQnwAskRjo058Y+unFAa0nSy4u26JpZK5VxqMjkygAAACARnACvYQuy6oELT9WLY85QWEiQlv+RqQufX6pVOw6aXRoAAEDAIzgBXmZE1yR9NL6/2jYJV3puoa54ZbneXL6NluUAAAAmIjgBXqhdYqQ+HD9AF5zWTM5SQw9++Kv++u4aHS4uNbs0AACAgERwArxUhD1YL445Qw+M6KQgq0X//Xm3LnppmbZl5JtdGgAAQMAhOAFezGKx6MaBbTTnxj5KiLBrfXqeRk5fqi9/22d2aQAAAAGF4AT4gDPbxOuT2weoR2qs8gpLdOO/f9Q/v9igUhfrngAAABoDwQnwEU2jHPrPTWfqun6tJEnTv9ms615fqaz8YnMLAwAACAAEJ8CHhARbNWlUZz13ZXeF2oK0ZFOGRr6wVGt2ZptdGgAAgF8jOAE+6E/dm+uDtP5qnRCu3dmHddnLyzX3+x20LAcAAGggBCfAR53SLFIfju+v8zo3VXGpS/fPX6u/vfeLCp20LAcAAKhvBCfAh0U5bHr56h66b3hHWS3SvJ926ZIZ32lnVoHZpQEAAPgVghPg4ywWi24Z3FZv/bmP4sND9OueXF34wlJ9s36/2aUBAAD4DYIT4Cf6tUvQgtsHqHtKjHIOO3XDGz/omYUb5aJlOQAAwEkjOAF+JCk6VO/cfKauOTNVhiE999UmXT/7B2UX0LIcAADgZBCcAD9jDw7So6O76OnLu8lhs2rRxgO68IWlWrc7x+zSAAAAfBbBCfBTF5/RQvNv66/U+DDtOnhYF8/4Tu+t2m12WQAAAD6J4AT4sU5JUfpo/ACd2ylRxSUuTZz/q+ZstmrTvkNmlwYAAOBTCE6An4sOtWnmNT11z7AOsliklQesumD6dzr/2cV68ZvNtC4HAACogWCzCwDQ8KxWi8af3V7dW0Rp6vyV2pAbpPXpeVqfvkFPfbFBp7eM0ahuyRrRNUmJkQ6zywUAAPA6BCcggPRpHaebOrrU/6xz9NWGDH20Zo+Wb8nUzzuy9fOObD264Df1bRuvUd2SdX7nJEWH2cwuGQAAwCsQnIAAFB1q0xW9WuqKXi21P69Qn/yyVx+t2aOfd2Rr2eZMLducqQc+WKfBHRI1qnuyzu2UqLAQflwAAIDAxW9CQIBLjHTo+v6tdX3/1tqZVaCP1uzRx2v2aH16nr78fZ++/H2fwkKCdG6npvpT92QNbN9EIcEsjwQAAIGF4ATAIyUuTGlntVPaWe20IT1PH63ZrY/X7NWO8kD10Zo9ig616YLTmmlkt2T1aR2vIKvF7LIBAAAaHMEJQJVOaRape5t11D3DTtHqndn6aM0eLfhlrw7kFek/K3fqPyt3KjHSrgu7JmtU92R1axEti4UQBQAA/BPBCcAJWSwWnd4yVqe3jNUDI07V939k6qM1e/TZunTtzyvSa8u26rVlW5UaH6aR5SGqQ9NIs8sGAACoVwQnADUWZLWoX7sE9WuXoEf+1EWLNx7QR2v2aOFv+7Q9s0DTv9ms6d9sVsdmkRrZLVmjuiUrJS7M7LIBAABOGsEJQJ2EBFt17qlNde6pTVVQXKIvf9+vj1bv0aKN+7lGFAAA8DsEJwAnLSwkWKPKR5hyCpz6/Ne9+nD1Hi3/g2tEAQAA/0BwAlCvosOOukZUbqEWlF8javXOI9eIevCDXzX4lCYa1S1Z53ZqqtCQILPLBgAAOCGCE4AGkxjl0A0DWuuGAa21I7NAH/+yRx+t3qMN+/K08Ld9Wvhb2TWihp7aVKO6cY0oAADgvQhOABpFy/jK14j6aM0e7cw6rA9X79GHq/coJsym4V24RhQAAPA+BCcAjY5rRAEAAF9DcAJgGq4RBQAAfAXBCYBX4BpRAADAmxGcAHidY68RtfC3ffp4zR4t2nig0jWi/tQtWSO6JqtJpN3ssgEAgB8jOAHwamEhwfpT9+b6U/fmyi4o1ufr0vXRmorXiHpkwW/q1zZBo7ol67wuzRQdyjWiAABA/SI4AfAZMWEhurJ3S13Zu/I1opZuztDSzRl64IN1XCMKAADUO4ITAJ/ENaIAAEBjIjgB8HlcIwoAADQ0ghMAv1KTa0Q1jbJrxGnJ+lP3ZHXlGlEAAKAGCE4A/FJV14j6cPUefbZur/blVrxG1Kjy9ubtuUYUAAA4DoITAL9X4RpRoztr8cYMfbRmj74sv0bUC19v1gtfl10jalT3ZI3syjWiAABARQQnAAHFHlzWMGLo8a4R9fkGPfn5Bp3RMkajuEYUAAAoR3ACELBOdI2oVTuyteqoa0SN7Jakvm0SlBIXypooAAACEMEJAFSza0RJUlx4iLq1iFb3lFh1bxmj7i1iFB3GBXcBAPB3BCcAOEZV14j632/79PueXGXlF+ubDQf0zYYDnv1bJ4Sre0qM56NTUhTXjAIAwM8QnADgBI6+RlRRSal+25OrNTuztbr8Y1tmgbZm5GtrRr7m/7xbkhQSZNWpyVHqnhKj01vGqFuLGKXGhzHFDwAAH0ZwAoAasgcHeVqcux3ML9aaXUeC1Jqd2TpY4PTcnv1d2X6xYTZ1Kx+R6pZSNsUvNjzEpFcCAABqi+AEACchNjxEQ05J1JBTEiVJhmFoR1aBVu/M1s87ysLTb3tydbDAqW83HNC3R03xaxUfdiRIpcTo1OQo2YODzHopAADgBAhOAFCPLBaLUuPDlRofrj91by5JKi5x6fe9uZ5RqNU7s7U1I1/bMgu0LbNAH6zeI6lsil+n5Ch1bxFd1ngiJVatmOIHAIBXIDgBQAMLCbaqW/nI0rXl27ILirVmV45W78jW6p0HtWZXjrLyi7WmfLrfG8u3S5KiQ49M8Tu9/DHimOIHAECjIzgBgAliwkI0uEMTDe7QRFLZFL+dWYf1886DnrVS6/bkKuewU4s3HtDijUem+LWMCzvSxa9ljE5NipLDxhQ/AAAaEsEJALyAxWJRy/gwtYwPqzDFb316xSl+fxzI146sAu3IKtBHa8qm+NmCLOqUFFWhJXqr+HBZrUzxAwCgvhCcAMBLhQRb1bVFjLq2iNG4vmXbcgqcWrMru0JL9Mz8Yv2yK0e/7MrRv8un+EU5gtWtfHpf9/KW6PERdhNfDQAAvo3gBAA+JDrMpkEdmmjQUVP8dh08rJ93HglT63bnKLewREs2ZWjJpgzPsSlxoeqeEls+KhWtzsnRTPEDAKCGCE4A4MMsFotS4sKUEhemUd2SJUnOUpfW783T6l3ZnuYTWw7ka2fWYe3MOqyPy6f4BVuPTPFzN6Bok8AUPwAAqkJwAgA/Ywuy6rQW0TqtRbSuOTNVkpRz2Km1u3K0urz5xOqd2co4VKy1u3O0dneO3lxRNsUv0hFcFqRaHGk+kcAUPwAACE4AEAiiQ20a0D5BA9onSDoyxW+NZ1QqW2t35yiviil+LWJDj6yXSolRl+ZM8QMABB6CEwAEoKOn+F3Y9cgUvw3peRW6+G05cEi7Dh7WroOH9ckveyWVTfE7pVmkp4Pf6S1j1CYhgil+AAC/RnACAEgqm+LXpXm0ujSP1tXlU/xyC91T/LL18w73FL8i/bonV7/uydWc73dIkiLtweqaEl0epmLVLSVaiZEOM18OAAD1ytTgtHjxYj311FP66aeftHfvXs2fP1+jR48+4TFz5szRk08+qU2bNik6OlrDhw/XU089pfj4+MYpGgACSJTDpv7tEtS/3ZEpfntyCj1NJ9bszNEvu7OVV1SiZZsztWxzpufY5jGh6to8SsqxyLl6j1onRik1Pkzx4SGyWBidAgD4FlODU35+vrp166YbbrhBF198cbX7L1u2TOPGjdMzzzyjkSNHavfu3brlllt000036b///W8jVAwAgc1isah5TKiax4RqRNckSVJJqUsb9pVN8XO3RN+0/5B2Zx/W7uzDkoL02a51nscIDwlSSlyYUuPDlBofXvZ5+e3kmFDZgqwmvToAAI7P1OA0fPhwDR8+vMb7L1++XK1atdLtt98uSWrdurVuvvlmTZs2raFKBABUIzjIqs7JZdeFGtunbIpfXvkUv1Xbs7Rk9QYZ4fHadfCw9uYWKr+4VOvT87Q+Pa/SYwVZLUqOcSg1Llwt48sCVcu4sLLP48MVYWeGOQDAHD71P1Dfvn11//3369NPP9Xw4cO1f/9+vffee7rggguOe0xRUZGKioo8t3NzcyVJTqdTTqezwWuujrsGb6gF/o/zDY3FEST1So1W9+QwpRz6XUOHdpfNZlORs1S7sgu182CBdmQd1o6sAu3IKii7xtTBwyoqcXmuN6XNlR83LtymlNgwtYwLLQtUcaGe24mRdqYABjh+xqGxcc75vtq8dxbDMIwGrKXGLBZLjdY4zZs3TzfccIMKCwtVUlKikSNH6v3335fNZqty/0mTJmny5MmVts+dO1dhYWH1UToAoB64DCm3WMookjILLcootCijUMosKvs3v+TEochmNRRvlxIchuIdUoK9/F9H2fZgZgACAI5RUFCgMWPGKCcnR1FRUSfc16eC02+//aZzzz1Xd911l8477zzt3btX9957r3r16qVZs2ZVeUxVI04pKSnKyMio9ovTGJxOpxYuXKihQ4ceN/wB9YXzDY2tPs+5vEKndpSPTJWNVB3WzqwC7Th4WHuyD8t1gv/NLBYpKcqhFPdIVWzZv+7b0aF8P/gDfsahsXHO+b7c3FwlJCTUKDj51FS9KVOmqH///rr33nslSV27dlV4eLgGDhyoxx57TElJSZWOsdvtstsrX/XeZrN51QnubfXAv3G+obHVxzkXZ7MpLjJM3VMr3+csdWn3wcPaXj71b0dmvrZnFnimAhYUl2pPTqH25BTq+60HKx0fHWo7spaqvFFFWQOLcCVFObhGlY/hZxwaG+ec76rN++ZTwamgoEDBwRVLDgoqu3q9lwycAQBMYAuyqlVCuFolhFe6zzAMZRwq1o6sfO3IKigLVJkFnpB1IK9IOYedWrs7R2t351Q6PiTIqhblI1OpcWFqGR9e/m9Z4wqHLagxXiIAwGSmBqdDhw5p8+Yjq3+3bt2q1atXKy4uTi1bttTEiRO1e/du/fvf/5YkjRw5UjfddJNmzJjhmap35513qnfv3kpOTjbrZQAAvJjFYlGTSLuaRNrVIzWu0v0FxSWeQLWz/N/tWWWf7zpYoOJSl/44kK8/DuRX+fhNo+xKjQs/qsV6mKfFehzXrAIAv2FqcPrxxx911llneW7ffffdkqRrr71Ws2fP1t69e7Vjxw7P/dddd53y8vI0ffp0/fWvf1VMTIzOPvts2pEDAOosLCRYHZtFqWOzynPbS12G9mQf6f5XNv0v3zNqlVdUon25RdqXW6SV27IqHR9hDy7v/lcWqMqmAoarZVyYkmMcCuaaVQDgM0wNTkOGDDnhFLvZs2dX2jZhwgRNmDChAasCAKBMkNWilLiyEaT+x9xnGIayC5yV1lW5R6v25hTqUFGJftubq9/25lZ67GCrRc1jQysGq7jw8n/DFM41qwDAq/BTGQCAOrBYLIoND1FseIi6p8RUur/QWapdBws8TSqO/JuvnQcPq7jEVRa0MguqfPyYMJsSIuxKiAhRk0iHEiJClBBhV5MIuxIiQ9QkwqGEyBDFh9sVQq91AGhwBCcAABqAwxakdomRapcYWek+l8vQvrxCz5S/HVnlzSoyyxpYHCxwKrv8Y/P+6p8rOtRWHrDs5WHLXv55xW3xESGyB9PMAgDqguAEAEAjs1otSooOVVJ0qM5sE1/p/pzDTu3LLVRGXpEOHCrSgbwiZRwqVsahImV4bhcp81CxSlyGcg47lXPYqS3HaWBxtChHsBIi3SNX5f+6R7PcIas8dBGyAOAIghMAAF4mOtSm6FCbOjStPFp1NFd5aHKHqQOHjgpYnttFysgr21biMpRbWKLcwpLjdgk8WqQj2BOmThSwEiLstGUH4PcITgAA+Cir9cg6q/bVhCzDKAtZFQKW+/PyEayjR7WcpYbyCkuUV5uQFeEOVEevx7J71mq5QxchC4AvIjgBABAALBaLYsJCFBNW85BVNpJVXGF64NEBy72tQsjKqEHIsgdXGK2qMIoVEXLUFEK7QkMIWQC8A8EJAABUcHTIapd44n0Nw1Du4RLPtMAKAcsdujyjWsUqLnUpr6hEeUUl2lqDkBVhD646YEUe2ZZYvi2Yaw0DaEAEJwAAUGcWi0XRYTZFh9nULjHihPsaRtkaqwoBK6/yCFbGoWIdOFSk4hKXDhWV6FBRibYdp2370cJDghRqCdJrO79XfIRdMWE2xYWVTWWMCw9RbFiIYsNsZZ+Hhygm1MZFiAHUGMEJAAA0CovF4ml80bZJ9SErr6ikLEwdp6vggfJ1WhmHilRU4lJ+canyZVHGrpwa1xTlCPYEKXfIig2zVbgd595WPgoXZGVoCwhEBCcAAOB1LBaLohw2RTlqHrLSD+ZrwZeL1LFrT+UVlyor36mDBcXKyi9Wdvm/BwucysovVs5hpyR5ugzWZESrrK6yrocVRq/CysJVTFiI4sJtx9wOUXSojbAF+AGCEwAA8GnukBWaEK62UdLQUxNls9lOeExJqUs5h93BynkkXBUU62B+2baKt4uVW1giw5Dn4sRba1yfFOMOW56gZTvyuWdkyx3IysKWlbAFeBWCEwAACDjBQVbFR9gVH2Gv8TElpS5lH3Z6gtTBgiMjWAfzy0JWtvt2+QhXXnnYOljg1MECp1SDhhiSZLVIMWFHpghWnDpoKxvNOmYqYZSDsAU0JIITAABADQQHWT2d/GrKWepSdsGRKYMHy6cLVrxdrKyCskB2ML9YeUUlchlSVnlAk2oeto4OWTFHNcKo6nZseIiiHMGyWAhbQE0QnAAAABqILciqJpFlbdRrqrjEpezDxTqY7zxqZOvIFMIKtwvK9jtUHrYy84uVmV9c4+cKsloUG1Y2ghXlCFa4PViRjmBF2IMVYbcpwh6kCEf5547gstt2myLK9wu3l+0bEkx3Qvg/ghMAAIAXCQm2KjHSocRIR42PKS5xedZklY1kOY8JV5VHuvKLS1XqMso7FtY8bB2v5kj7kSAV4Qg+crv884hjb7uDWvnnEfZghYcEM90QXovgBAAA4ONCgq1KjHIoMarmYauopPTImqzyKYKHCkuUX1yivMKy62cdKixRfvkFi933HSo8cvuws1RSWXDLLKndaNfxhIcEeYJUxFGhqjYjYJGOYNmDrUxDRL0iOAEAAAQge3CQmkYFqWktwtaxSkrLrp/lDlnuCxaXfe7UoaLSoz4vKb9d9nne0UGssEQlLkOSyq7HVVyqfSo6qdcXbLVUCFKVRrzsx0xNrGIEzL2PjQslQwQnAAAA1FFwkFXRoVZFh564/Xt1DMMou4hxefByj3gdfftE93luF5boUHFZJ8MSl6Gcw07PNbtOhsNmPWbEq2zUK8xmVcY+q9Z8tkERDpscIUEKswUpLCRYoSFBCgsJUqgtqPzz4LLb7m22IKYl+hiCEwAAAExlsVjksAXJYQuqVYv4qrhchgqcpWVTDAtLqhgNO2r0q8hZ8T7PvmX3FTpdkqRCp0uFziJlHKrqGa36bt/2OtXqsFnLQpYnXJUFqiMBqyxshYWUfW3cn4e6Q9hRxx3Zp+w+pirWP4ITAAAA/IbVavFMs2sadXKP5Sw9MgrmDlV5R41uZRcUac2639WiVVsVlRo6XFyqAmdp2b/FJTpcXKrDzlIVFLu3lXrWhUnuQHby68KqYrFIYeXBKjQkSGG2YM9olzuYhZWPhFUMZUfvU0VAK38cW5Al4IIZwQkAAACogi3IqpiwEMWEhVR5v9Pp1Kc5v+mC8zrIZqvZdEXDMFTodKmguMQTpI6EqrJtBcWlKiwPXGWhq4p9i0tV4CwPZ+WBraC4VMUlrvLnObJerCEEWS2eYOYOWaHuEbRK0xSDqhxZ69UqTrHhVX9tvRHBCQAAAGgkFovFMwoU3wCPX+oyyke5SiqMcrk/P/5IWMX93fe5H8t9293Eo9RlKK+842JdvX9rX/UIj6uvl97gCE4AAACAnwg6aqpiQ3CWuo4KXCWVR8ecR0LW0VMXj3x+ZFTteCN53orgBAAAAKBGbPXUSdEX0ZQeAAAAAKpBcAIAAACAahCcAAAAAKAaBCcAAAAAqAbBCQAAAACqQXACAAAAgGoQnAAAAACgGgQnAAAAAKgGwQkAAAAAqkFwAgAAAIBqEJwAAAAAoBoEJwAAAACoBsEJAAAAAKpBcAIAAACAahCcAAAAAKAaBCcAAAAAqAbBCQAAAACqQXACAAAAgGoEm11AYzMMQ5KUm5trciVlnE6nCgoKlJubK5vNZnY58HOcb2hsnHNoTJxvaGycc77PnQncGeFEAi445eXlSZJSUlJMrgQAAACAN8jLy1N0dPQJ97EYNYlXfsTlcmnPnj2KjIyUxWIxuxzl5uYqJSVFO3fuVFRUlNnlwM9xvqGxcc6hMXG+obFxzvk+wzCUl5en5ORkWa0nXsUUcCNOVqtVLVq0MLuMSqKioviGQ6PhfENj45xDY+J8Q2PjnPNt1Y00udEcAgAAAACqQXACAAAAgGoQnExmt9v18MMPy263m10KAgDnGxob5xwaE+cbGhvnXGAJuOYQAAAAAFBbjDgBAAAAQDUITgAAAABQDYITAAAAAFSD4AQAAAAA1SA4mejFF19Uq1at5HA41KdPH61cudLskuCnpkyZol69eikyMlKJiYkaPXq0NmzYYHZZCBBTp06VxWLRnXfeaXYp8GO7d+/W1Vdfrfj4eIWGhuq0007Tjz/+aHZZ8EOlpaV68MEH1bp1a4WGhqpt27Z69NFHRb81/0dwMsk777yju+++Ww8//LBWrVqlbt266bzzztP+/fvNLg1+aNGiRUpLS9OKFSu0cOFCOZ1ODRs2TPn5+WaXBj/3ww8/6JVXXlHXrl3NLgV+7ODBg+rfv79sNps+++wz/fbbb/rXv/6l2NhYs0uDH5o2bZpmzJih6dOn6/fff9e0adP05JNP6oUXXjC7NDQw2pGbpE+fPurVq5emT58uSXK5XEpJSdGECRN03333mVwd/N2BAweUmJioRYsWadCgQWaXAz916NAhnXHGGXrppZf02GOPqXv37nr22WfNLgt+6L777tOyZcu0ZMkSs0tBALjwwgvVtGlTzZo1y7PtkksuUWhoqN566y0TK0NDY8TJBMXFxfrpp5907rnnerZZrVade+65Wr58uYmVIVDk5ORIkuLi4kyuBP4sLS1NI0aMqPCzDmgIH330kXr27KnLLrtMiYmJOv300/Xqq6+aXRb8VL9+/fTVV19p48aNkqQ1a9Zo6dKlGj58uMmVoaEFm11AIMrIyFBpaamaNm1aYXvTpk21fv16k6pCoHC5XLrzzjvVv39/denSxexy4KfefvttrVq1Sj/88IPZpSAA/PHHH5oxY4buvvtu3X///frhhx90++23KyQkRNdee63Z5cHP3HfffcrNzVXHjh0VFBSk0tJSPf744xo7dqzZpaGBEZyAAJOWlqZ169Zp6dKlZpcCP7Vz507dcccdWrhwoRwOh9nlIAC4XC717NlTTzzxhCTp9NNP17p16/Tyyy8TnFDv3n33Xc2ZM0dz585V586dtXr1at15551KTk7mfPNzBCcTJCQkKCgoSPv27auwfd++fWrWrJlJVSEQjB8/XgsWLNDixYvVokULs8uBn/rpp5+0f/9+nXHGGZ5tpaWlWrx4saZPn66ioiIFBQWZWCH8TVJSkk499dQK2zp16qT333/fpIrgz+69917dd999uvLKKyVJp512mrZv364pU6YQnPwca5xMEBISoh49euirr77ybHO5XPrqq6/Ut29fEyuDvzIMQ+PHj9f8+fP19ddfq3Xr1maXBD92zjnnaO3atVq9erXno2fPnho7dqxWr15NaEK969+/f6VLLGzcuFGpqakmVQR/VlBQIKu14q/QQUFBcrlcJlWExsKIk0nuvvtuXXvtterZs6d69+6tZ599Vvn5+br++uvNLg1+KC0tTXPnztWHH36oyMhIpaenS5Kio6MVGhpqcnXwN5GRkZXWz4WHhys+Pp51dWgQd911l/r166cnnnhCl19+uVauXKmZM2dq5syZZpcGPzRy5Eg9/vjjatmypTp37qyff/5ZTz/9tG644QazS0MDox25iaZPn66nnnpK6enp6t69u55//nn16dPH7LLghywWS5XbX3/9dV133XWNWwwC0pAhQ2hHjga1YMECTZw4UZs2bVLr1q11991366abbjK7LPihvLw8Pfjgg5o/f77279+v5ORkXXXVVXrooYcUEhJidnloQAQnAAAAAKgGa5wAAAAAoBoEJwAAAACoBsEJAAAAAKpBcAIAAACAahCcAAAAAKAaBCcAAAAAqAbBCQAAAACqQXACAAAAgGoQnAAAqAWLxaIPPvjA7DIAAI2M4AQA8BnXXXedLBZLpY/zzz/f7NIAAH4u2OwCAACojfPPP1+vv/56hW12u92kagAAgYIRJwCAT7Hb7WrWrFmFj9jYWEll0+hmzJih4cOHKzQ0VG3atNF7771X4fi1a9fq7LPPVmhoqOLj4/WXv/xFhw4dqrDPa6+9ps6dO8tutyspKUnjx4+vcH9GRoYuuugihYWFqX379vroo48a9kUDAExHcAIA+JUHH3xQl1xyidasWaOxY8fqyiuv1O+//y5Jys/P13nnnafY2Fj98MMPmjdvnr788ssKwWjGjBlKS0vTX/7yF61du1YfffSR2rVrV+E5Jk+erMsvv1y//PKLLrjgAo0dO1ZZWVmN+joBAI3LYhiGYXYRAADUxHXXXae33npLDoejwvb7779f999/vywWi2655RbNmDHDc9+ZZ56pM844Qy+99JJeffVV/f3vf9fOnTsVHh4uSfr00081cuRI7dmzR02bNlXz5s11/fXX67HHHquyBovFogceeECPPvqopLIwFhERoc8++4y1VgDgx1jjBADwKWeddVaFYCRJcXFxns/79u1b4b6+fftq9erVkqTff/9d3bp184QmSerfv79cLpc2bNggi8WiPXv26JxzzjlhDV27dvV8Hh4erqioKO3fv7+uLwkA4AMITgAAnxIeHl5p6lx9CQ0NrdF+Nputwm2LxSKXy9UQJQEAvARrnAAAfmXFihWVbnfq1EmS1KlTJ61Zs0b5+fme+5ctWyar1apTTjlFkZGRatWqlb766qtGrRkA4P0YcQIA+JSioiKlp6dX2BYcHKyEhARJ0rx589SzZ08NGDBAc+bM0cqVKzVr1ixJ0tixY/Xwww/r2muv1aRJk3TgwAFNmDBB11xzjZo2bSpJmjRpkm655RYlJiZq+PDhysvL07JlyzRhwoTGfaEAAK9CcAIA+JTPP/9cSUlJFbadcsopWr9+vaSyjndvv/22brvtNiUlJek///mPTj31VElSWFiYvvjiC91xxx3q1auXwsLCdMkll+jpp5/2PNa1116rwsJCPfPMM7rnnnuUkJCgSy+9tPFeIADAK9FVDwDgNywWi+bPn6/Ro0ebXQoAwM+wxgkAAAAAqkFwAgAAAIBqsMYJAOA3mH0OAGgojDgBAAAAQDUITgAAAABQDYITAAAAAFSD4AQAAAAA1SA4AQAAAEA1CE4AAAAAUA2CEwAAAABUg+AEAAAAANX4fyP7orvSh+K5AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if 'train_losses' in locals():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Partie 2 - CNN + MLP + MelSpectrogram : Training Loss')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if 'train_losses_mlp' in locals() and 'train_losses' in locals():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses_mlp, label='MLP + MFCC (Partie 1)', linewidth=2, marker='o')\n",
        "    plt.plot(train_losses, label='CNN + MLP + MelSpec (Partie 2)', linewidth=2, marker='s')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Comparaison: MLP pur vs CNN+MLP')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Avec le CNN, il a converge plus vite et a fini avec une loss plus basse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partie 3 — RNN (LSTM / GRU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture RNN avec LSTM/GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "class CNNRNNASR(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        vocab_size,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT,\n",
        "        rnn_type='LSTM',\n",
        "        bidirectional=False,\n",
        "        conv_channels=[64, 128, 256],\n",
        "        kernel_sizes=[3, 3, 3],\n",
        "    ):\n",
        "        super(CNNRNNASR, self).__init__()\n",
        "        \n",
        "        self.rnn_type = rnn_type\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        in_channels = input_size\n",
        "\n",
        "        for out_channels, kernel_size in zip(conv_channels, kernel_sizes):\n",
        "            self.conv_layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(\n",
        "                        in_channels, out_channels, kernel_size, padding=kernel_size // 2\n",
        "                    ),\n",
        "                    nn.BatchNorm1d(out_channels),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(dropout),\n",
        "                )\n",
        "            )\n",
        "            in_channels = out_channels\n",
        "\n",
        "        conv_output_size = conv_channels[-1]\n",
        "        \n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(\n",
        "                input_size=conv_output_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional,\n",
        "                batch_first=True\n",
        "            )\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(\n",
        "                input_size=conv_output_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional,\n",
        "                batch_first=True\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"rnn_type doit être 'LSTM' ou 'GRU', reçu: {rnn_type}\")\n",
        "        \n",
        "        rnn_output_size = hidden_size * (2 if bidirectional else 1)\n",
        "        \n",
        "        self.fc = nn.Linear(rnn_output_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, features = x.size()\n",
        "        x = x.transpose(1, 2)\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = conv_layer(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        rnn_out, _ = self.rnn(x)\n",
        "        output = self.fc(rnn_out)\n",
        "        \n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fonction d'entraînement réutilisable avec métriques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=NUM_EPOCHS, model_name=\"Model\", max_grad_norm=1.0):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    batch_losses = []\n",
        "    sequence_lengths = []\n",
        "    epoch_times = []\n",
        "    \n",
        "    total_start_time = time.time()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "        skipped_batches = 0\n",
        "        \n",
        "        for batch_idx, (melspectrograms, texts, mel_lengths, text_lengths) in enumerate(dataloader):\n",
        "            melspectrograms = melspectrograms.to(device)\n",
        "            texts = texts.to(device)\n",
        "            mel_lengths = mel_lengths.to(device)\n",
        "            text_lengths = text_lengths.to(device)\n",
        "            \n",
        "            if (mel_lengths <= 0).any() or (text_lengths <= 0).any():\n",
        "                skipped_batches += 1\n",
        "                continue\n",
        "            \n",
        "            if (mel_lengths < text_lengths).any():\n",
        "                skipped_batches += 1\n",
        "                continue\n",
        "            \n",
        "            sequence_lengths.extend(mel_lengths.cpu().numpy().tolist())\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            try:\n",
        "                logits = model(melspectrograms)\n",
        "                log_probs = nn.functional.log_softmax(logits, dim=2)\n",
        "                log_probs = log_probs.transpose(0, 1)\n",
        "                \n",
        "                loss = criterion(log_probs, texts, mel_lengths, text_lengths)\n",
        "                \n",
        "                if not torch.isfinite(loss) or loss.item() > 1000:\n",
        "                    skipped_batches += 1\n",
        "                    if (batch_idx + 1) % 10 == 0:\n",
        "                        print(f'[{model_name}] Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f} (SKIPPED - invalid loss)')\n",
        "                    continue\n",
        "                \n",
        "                loss.backward()\n",
        "                \n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                \n",
        "                optimizer.step()\n",
        "                \n",
        "                loss_value = loss.item()\n",
        "                epoch_loss += loss_value\n",
        "                num_batches += 1\n",
        "                batch_losses.append(loss_value)\n",
        "                \n",
        "                if (batch_idx + 1) % 10 == 0:\n",
        "                    print(f'[{model_name}] Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss_value:.4f}')\n",
        "            \n",
        "            except RuntimeError as e:\n",
        "                if \"CTC\" in str(e) or \"length\" in str(e).lower():\n",
        "                    skipped_batches += 1\n",
        "                    if (batch_idx + 1) % 10 == 0:\n",
        "                        print(f'[{model_name}] Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], SKIPPED - CTC error: {str(e)[:50]}')\n",
        "                    continue\n",
        "                else:\n",
        "                    raise\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        epoch_times.append(epoch_time)\n",
        "        avg_loss = epoch_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f'[{model_name}] Epoch [{epoch+1}/{num_epochs}] completed, Average Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s, Skipped batches: {skipped_batches}')\n",
        "    \n",
        "    total_time = time.time() - total_start_time\n",
        "    \n",
        "    metrics = {\n",
        "        'train_losses': train_losses,\n",
        "        'batch_losses': batch_losses,\n",
        "        'sequence_lengths': sequence_lengths,\n",
        "        'epoch_times': epoch_times,\n",
        "        'total_time': total_time,\n",
        "        'final_loss': train_losses[-1] if train_losses else None,\n",
        "    }\n",
        "    \n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calcul des métriques d'analyse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(metrics_dict, model_name=\"Model\"):\n",
        "    train_losses = metrics_dict['train_losses']\n",
        "    batch_losses = metrics_dict['batch_losses']\n",
        "    epoch_times = metrics_dict['epoch_times']\n",
        "    sequence_lengths = metrics_dict['sequence_lengths']\n",
        "    \n",
        "    avg_epoch_time = np.mean(epoch_times)\n",
        "    total_time = metrics_dict['total_time']\n",
        "    final_loss = metrics_dict['final_loss']\n",
        "    \n",
        "    num_batches_per_epoch = len(batch_losses) // len(train_losses) if train_losses else len(batch_losses)\n",
        "    batch_losses_array = np.array(batch_losses)\n",
        "    \n",
        "    loss_variance = np.var(batch_losses_array)\n",
        "    loss_std = np.std(batch_losses_array)\n",
        "    \n",
        "    epoch_variances = []\n",
        "    for i in range(len(train_losses)):\n",
        "        start_idx = i * num_batches_per_epoch\n",
        "        end_idx = start_idx + num_batches_per_epoch\n",
        "        if end_idx <= len(batch_losses):\n",
        "            epoch_variances.append(np.var(batch_losses_array[start_idx:end_idx]))\n",
        "    \n",
        "    avg_epoch_variance = np.mean(epoch_variances) if epoch_variances else 0\n",
        "    \n",
        "    if sequence_lengths:\n",
        "        max_seq_len = max(sequence_lengths)\n",
        "        min_seq_len = min(sequence_lengths)\n",
        "        avg_seq_len = np.mean(sequence_lengths)\n",
        "        median_seq_len = np.median(sequence_lengths)\n",
        "    else:\n",
        "        max_seq_len = min_seq_len = avg_seq_len = median_seq_len = 0\n",
        "    \n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'vitesse': {\n",
        "            'temps_total': total_time,\n",
        "            'temps_moyen_par_epoch': avg_epoch_time,\n",
        "            'temps_par_epoch': epoch_times,\n",
        "        },\n",
        "        'stabilite_ctc': {\n",
        "            'variance_globale': loss_variance,\n",
        "            'ecart_type_global': loss_std,\n",
        "            'variance_moyenne_par_epoch': avg_epoch_variance,\n",
        "            'variances_par_epoch': epoch_variances,\n",
        "        },\n",
        "        'capacite_temporelle': {\n",
        "            'longueur_max': max_seq_len,\n",
        "            'longueur_min': min_seq_len,\n",
        "            'longueur_moyenne': avg_seq_len,\n",
        "            'longueur_mediane': median_seq_len,\n",
        "            'longueurs': sequence_lengths,\n",
        "        },\n",
        "        'performance': {\n",
        "            'perte_finale': final_loss,\n",
        "            'pertes_par_epoch': train_losses,\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entraînement et comparaison des architectures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset disponible, début de l'entraînement des modèles RNN...\n"
          ]
        }
      ],
      "source": [
        "all_results = {}\n",
        "\n",
        "if 'dataloader' in locals() and dataloader is not None:\n",
        "    print(\"Dataset disponible, début de l'entraînement des modèles RNN...\")\n",
        "else:\n",
        "    print(\"Chargement du dataset...\")\n",
        "    transcriptions_file = 'data/ss-corpus-fr.tsv'\n",
        "    audio_dir = 'data/audios'\n",
        "    \n",
        "    if os.path.exists(transcriptions_file):\n",
        "        with open(transcriptions_file, 'r', encoding='utf-8') as f:\n",
        "            next(f)\n",
        "            texts = [line.split('\\t')[6].strip() for line in f if line.strip() and len(line.split('\\t')) >= 7 and line.split('\\t')[6].strip()]\n",
        "        \n",
        "        if 'encoder' not in locals():\n",
        "            encoder = CharEncoder()\n",
        "            encoder.build_vocab(texts)\n",
        "            vocab_size = encoder.get_vocab_size()\n",
        "        else:\n",
        "            vocab_size = encoder.get_vocab_size()\n",
        "        \n",
        "        dataset = ASRDataset(transcriptions_file, audio_dir, encoder)\n",
        "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Entraînement LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LSTM] Epoch [1/10], Batch [10/892], Loss: 3.7253\n",
            "[LSTM] Epoch [1/10], Batch [20/892], Loss: 3.0663\n",
            "[LSTM] Epoch [1/10], Batch [30/892], Loss: 2.9604\n",
            "[LSTM] Epoch [1/10], Batch [40/892], Loss: 2.9194\n",
            "[LSTM] Epoch [1/10], Batch [50/892], Loss: 2.8861\n",
            "[LSTM] Epoch [1/10], Batch [60/892], Loss: 2.8973\n",
            "[LSTM] Epoch [1/10], Batch [70/892], Loss: 2.8894\n",
            "[LSTM] Epoch [1/10], Batch [80/892], Loss: 2.8732\n",
            "[LSTM] Epoch [1/10], Batch [90/892], Loss: 2.8901\n",
            "[LSTM] Epoch [1/10], Batch [100/892], Loss: 2.8769\n",
            "[LSTM] Epoch [1/10], Batch [110/892], Loss: 2.8769\n",
            "[LSTM] Epoch [1/10], Batch [120/892], Loss: 2.8679\n",
            "[LSTM] Epoch [1/10], Batch [130/892], Loss: 2.8782\n",
            "[LSTM] Epoch [1/10], Batch [140/892], Loss: 2.8724\n",
            "[LSTM] Epoch [1/10], Batch [150/892], Loss: 2.8847\n",
            "[LSTM] Epoch [1/10], Batch [160/892], Loss: 2.8564\n",
            "[LSTM] Epoch [1/10], Batch [170/892], Loss: 2.8797\n",
            "[LSTM] Epoch [1/10], Batch [180/892], Loss: 2.8553\n",
            "[LSTM] Epoch [1/10], Batch [190/892], Loss: 2.8879\n",
            "[LSTM] Epoch [1/10], Batch [200/892], Loss: 2.8743\n",
            "[LSTM] Epoch [1/10], Batch [210/892], Loss: 2.8686\n",
            "[LSTM] Epoch [1/10], Batch [220/892], Loss: 2.8948\n",
            "[LSTM] Epoch [1/10], Batch [230/892], Loss: 2.8657\n",
            "[LSTM] Epoch [1/10], Batch [240/892], Loss: 2.8884\n",
            "[LSTM] Epoch [1/10], Batch [250/892], Loss: 2.8799\n",
            "[LSTM] Epoch [1/10], Batch [260/892], Loss: 2.8790\n",
            "[LSTM] Epoch [1/10], Batch [270/892], Loss: 2.8723\n",
            "[LSTM] Epoch [1/10], Batch [280/892], Loss: 2.8940\n",
            "[LSTM] Epoch [1/10], Batch [290/892], Loss: 2.8835\n",
            "[LSTM] Epoch [1/10], Batch [300/892], Loss: 2.8657\n",
            "[LSTM] Epoch [1/10], Batch [310/892], Loss: 2.8686\n",
            "[LSTM] Epoch [1/10], Batch [320/892], Loss: 2.8631\n",
            "[LSTM] Epoch [1/10], Batch [330/892], Loss: 2.8725\n",
            "[LSTM] Epoch [1/10], Batch [340/892], Loss: 2.8768\n",
            "[LSTM] Epoch [1/10], Batch [350/892], Loss: 2.8797\n",
            "[LSTM] Epoch [1/10], Batch [360/892], Loss: 2.8789\n",
            "[LSTM] Epoch [1/10], Batch [370/892], Loss: 2.8731\n",
            "[LSTM] Epoch [1/10], Batch [380/892], Loss: 2.8670\n",
            "[LSTM] Epoch [1/10], Batch [390/892], Loss: 2.8614\n",
            "[LSTM] Epoch [1/10], Batch [400/892], Loss: 2.8716\n",
            "[LSTM] Epoch [1/10], Batch [410/892], Loss: 2.8703\n",
            "[LSTM] Epoch [1/10], Batch [420/892], Loss: 2.8751\n",
            "[LSTM] Epoch [1/10], Batch [430/892], Loss: 2.8706\n",
            "[LSTM] Epoch [1/10], Batch [440/892], Loss: 2.8709\n",
            "[LSTM] Epoch [1/10], Batch [450/892], Loss: 2.8516\n",
            "[LSTM] Epoch [1/10], Batch [460/892], Loss: 2.8746\n",
            "[LSTM] Epoch [1/10], Batch [470/892], Loss: 2.8700\n",
            "[LSTM] Epoch [1/10], Batch [480/892], Loss: 2.8529\n",
            "[LSTM] Epoch [1/10], Batch [490/892], Loss: 2.8657\n",
            "[LSTM] Epoch [1/10], Batch [500/892], Loss: 2.8746\n",
            "[LSTM] Epoch [1/10], Batch [510/892], Loss: 2.8497\n",
            "[LSTM] Epoch [1/10], Batch [520/892], Loss: 2.8722\n",
            "[LSTM] Epoch [1/10], Batch [530/892], Loss: 2.8515\n",
            "[LSTM] Epoch [1/10], Batch [540/892], Loss: 2.8418\n",
            "[LSTM] Epoch [1/10], Batch [550/892], Loss: 2.8565\n",
            "[LSTM] Epoch [1/10], Batch [560/892], Loss: 2.8422\n",
            "[LSTM] Epoch [1/10], Batch [570/892], Loss: 2.8592\n",
            "[LSTM] Epoch [1/10], Batch [580/892], Loss: 2.8657\n",
            "[LSTM] Epoch [1/10], Batch [590/892], Loss: 2.8802\n",
            "[LSTM] Epoch [1/10], Batch [600/892], Loss: 2.8493\n",
            "[LSTM] Epoch [1/10], Batch [610/892], Loss: 2.8424\n",
            "[LSTM] Epoch [1/10], Batch [620/892], Loss: 2.8469\n",
            "[LSTM] Epoch [1/10], Batch [630/892], Loss: 2.8420\n",
            "[LSTM] Epoch [1/10], Batch [640/892], Loss: 2.8338\n",
            "[LSTM] Epoch [1/10], Batch [650/892], Loss: 2.8593\n",
            "[LSTM] Epoch [1/10], Batch [660/892], Loss: 2.8457\n",
            "[LSTM] Epoch [1/10], Batch [670/892], Loss: 2.8345\n",
            "[LSTM] Epoch [1/10], Batch [680/892], Loss: 2.8321\n",
            "[LSTM] Epoch [1/10], Batch [690/892], Loss: 2.8097\n",
            "[LSTM] Epoch [1/10], Batch [700/892], Loss: 2.7958\n",
            "[LSTM] Epoch [1/10], Batch [710/892], Loss: 2.7894\n",
            "[LSTM] Epoch [1/10], Batch [720/892], Loss: 2.7836\n",
            "[LSTM] Epoch [1/10], Batch [730/892], Loss: 2.7653\n",
            "[LSTM] Epoch [1/10], Batch [740/892], Loss: 2.7275\n",
            "[LSTM] Epoch [1/10], Batch [750/892], Loss: 2.7190\n",
            "[LSTM] Epoch [1/10], Batch [760/892], Loss: 2.6827\n",
            "[LSTM] Epoch [1/10], Batch [770/892], Loss: 2.6511\n",
            "[LSTM] Epoch [1/10], Batch [780/892], Loss: 2.6245\n",
            "[LSTM] Epoch [1/10], Batch [790/892], Loss: 2.5762\n",
            "[LSTM] Epoch [1/10], Batch [800/892], Loss: 2.5306\n",
            "[LSTM] Epoch [1/10], Batch [810/892], Loss: 2.4842\n",
            "[LSTM] Epoch [1/10], Batch [820/892], Loss: 2.4713\n",
            "[LSTM] Epoch [1/10], Batch [830/892], Loss: 2.4325\n",
            "[LSTM] Epoch [1/10], Batch [840/892], Loss: 2.4194\n",
            "[LSTM] Epoch [1/10], Batch [850/892], Loss: 2.3745\n",
            "[LSTM] Epoch [1/10], Batch [860/892], Loss: 2.3080\n",
            "[LSTM] Epoch [1/10], Batch [870/892], Loss: 2.2976\n",
            "[LSTM] Epoch [1/10], Batch [880/892], Loss: 2.3341\n",
            "[LSTM] Epoch [1/10], Batch [890/892], Loss: 2.2893\n",
            "[LSTM] Epoch [1/10] completed, Average Loss: 2.8730, Time: 273.54s\n",
            "[LSTM] Epoch [2/10], Batch [10/892], Loss: 2.2568\n",
            "[LSTM] Epoch [2/10], Batch [20/892], Loss: 2.2007\n",
            "[LSTM] Epoch [2/10], Batch [30/892], Loss: 2.1810\n",
            "[LSTM] Epoch [2/10], Batch [40/892], Loss: 2.1547\n",
            "[LSTM] Epoch [2/10], Batch [50/892], Loss: 2.1636\n",
            "[LSTM] Epoch [2/10], Batch [60/892], Loss: 2.1109\n",
            "[LSTM] Epoch [2/10], Batch [70/892], Loss: 2.1031\n",
            "[LSTM] Epoch [2/10], Batch [80/892], Loss: 2.1049\n",
            "[LSTM] Epoch [2/10], Batch [90/892], Loss: 2.1041\n",
            "[LSTM] Epoch [2/10], Batch [100/892], Loss: 2.0544\n",
            "[LSTM] Epoch [2/10], Batch [110/892], Loss: 1.9915\n",
            "[LSTM] Epoch [2/10], Batch [120/892], Loss: 1.9334\n",
            "[LSTM] Epoch [2/10], Batch [130/892], Loss: 1.9853\n",
            "[LSTM] Epoch [2/10], Batch [140/892], Loss: 1.9800\n",
            "[LSTM] Epoch [2/10], Batch [150/892], Loss: 1.9279\n",
            "[LSTM] Epoch [2/10], Batch [160/892], Loss: 1.9802\n",
            "[LSTM] Epoch [2/10], Batch [170/892], Loss: 1.9705\n",
            "[LSTM] Epoch [2/10], Batch [180/892], Loss: 1.9117\n",
            "[LSTM] Epoch [2/10], Batch [190/892], Loss: 1.9566\n",
            "[LSTM] Epoch [2/10], Batch [200/892], Loss: 1.8503\n",
            "[LSTM] Epoch [2/10], Batch [210/892], Loss: 1.9403\n",
            "[LSTM] Epoch [2/10], Batch [220/892], Loss: 1.8007\n",
            "[LSTM] Epoch [2/10], Batch [230/892], Loss: 1.9027\n",
            "[LSTM] Epoch [2/10], Batch [240/892], Loss: 1.8460\n",
            "[LSTM] Epoch [2/10], Batch [250/892], Loss: 1.8021\n",
            "[LSTM] Epoch [2/10], Batch [260/892], Loss: 1.8341\n",
            "[LSTM] Epoch [2/10], Batch [270/892], Loss: 1.7813\n",
            "[LSTM] Epoch [2/10], Batch [280/892], Loss: 1.7885\n",
            "[LSTM] Epoch [2/10], Batch [290/892], Loss: 1.8361\n",
            "[LSTM] Epoch [2/10], Batch [300/892], Loss: 1.8284\n",
            "[LSTM] Epoch [2/10], Batch [310/892], Loss: 1.8031\n",
            "[LSTM] Epoch [2/10], Batch [320/892], Loss: 1.7941\n",
            "[LSTM] Epoch [2/10], Batch [330/892], Loss: 1.7090\n",
            "[LSTM] Epoch [2/10], Batch [340/892], Loss: 1.7182\n",
            "[LSTM] Epoch [2/10], Batch [350/892], Loss: 1.7000\n",
            "[LSTM] Epoch [2/10], Batch [360/892], Loss: 1.6825\n",
            "[LSTM] Epoch [2/10], Batch [370/892], Loss: 1.7223\n",
            "[LSTM] Epoch [2/10], Batch [380/892], Loss: 1.7038\n",
            "[LSTM] Epoch [2/10], Batch [390/892], Loss: 1.6760\n",
            "[LSTM] Epoch [2/10], Batch [400/892], Loss: 1.7164\n",
            "[LSTM] Epoch [2/10], Batch [410/892], Loss: 1.6802\n",
            "[LSTM] Epoch [2/10], Batch [420/892], Loss: 1.6692\n",
            "[LSTM] Epoch [2/10], Batch [430/892], Loss: 1.6410\n",
            "[LSTM] Epoch [2/10], Batch [440/892], Loss: 1.7093\n",
            "[LSTM] Epoch [2/10], Batch [450/892], Loss: 1.6742\n",
            "[LSTM] Epoch [2/10], Batch [460/892], Loss: 1.6388\n",
            "[LSTM] Epoch [2/10], Batch [470/892], Loss: 1.5843\n",
            "[LSTM] Epoch [2/10], Batch [480/892], Loss: 1.5801\n",
            "[LSTM] Epoch [2/10], Batch [490/892], Loss: 1.5545\n",
            "[LSTM] Epoch [2/10], Batch [500/892], Loss: 1.5487\n",
            "[LSTM] Epoch [2/10], Batch [510/892], Loss: 1.5559\n",
            "[LSTM] Epoch [2/10], Batch [520/892], Loss: 1.6128\n",
            "[LSTM] Epoch [2/10], Batch [530/892], Loss: 1.6823\n",
            "[LSTM] Epoch [2/10], Batch [540/892], Loss: 1.5831\n",
            "[LSTM] Epoch [2/10], Batch [550/892], Loss: 1.5968\n",
            "[LSTM] Epoch [2/10], Batch [560/892], Loss: 1.5929\n",
            "[LSTM] Epoch [2/10], Batch [570/892], Loss: 1.5942\n",
            "[LSTM] Epoch [2/10], Batch [580/892], Loss: 1.6645\n",
            "[LSTM] Epoch [2/10], Batch [590/892], Loss: 1.5793\n",
            "[LSTM] Epoch [2/10], Batch [600/892], Loss: 1.5968\n",
            "[LSTM] Epoch [2/10], Batch [610/892], Loss: 1.5975\n",
            "[LSTM] Epoch [2/10], Batch [620/892], Loss: 1.5254\n",
            "[LSTM] Epoch [2/10], Batch [630/892], Loss: 1.5745\n",
            "[LSTM] Epoch [2/10], Batch [640/892], Loss: 1.6082\n",
            "[LSTM] Epoch [2/10], Batch [650/892], Loss: 1.5800\n",
            "[LSTM] Epoch [2/10], Batch [660/892], Loss: 1.5589\n",
            "[LSTM] Epoch [2/10], Batch [670/892], Loss: 1.5472\n",
            "[LSTM] Epoch [2/10], Batch [680/892], Loss: 1.5349\n",
            "[LSTM] Epoch [2/10], Batch [690/892], Loss: 1.4858\n",
            "[LSTM] Epoch [2/10], Batch [700/892], Loss: 1.4725\n",
            "[LSTM] Epoch [2/10], Batch [710/892], Loss: 1.5508\n",
            "[LSTM] Epoch [2/10], Batch [720/892], Loss: 1.5484\n",
            "[LSTM] Epoch [2/10], Batch [730/892], Loss: 1.5478\n",
            "[LSTM] Epoch [2/10], Batch [740/892], Loss: 1.5157\n",
            "[LSTM] Epoch [2/10], Batch [750/892], Loss: 1.5335\n",
            "[LSTM] Epoch [2/10], Batch [760/892], Loss: 1.4678\n",
            "[LSTM] Epoch [2/10], Batch [770/892], Loss: 1.5125\n",
            "[LSTM] Epoch [2/10], Batch [780/892], Loss: 1.5369\n",
            "[LSTM] Epoch [2/10], Batch [790/892], Loss: 1.5129\n",
            "[LSTM] Epoch [2/10], Batch [800/892], Loss: 1.5095\n",
            "[LSTM] Epoch [2/10], Batch [810/892], Loss: 1.5205\n",
            "[LSTM] Epoch [2/10], Batch [820/892], Loss: 1.5585\n",
            "[LSTM] Epoch [2/10], Batch [830/892], Loss: 1.4733\n",
            "[LSTM] Epoch [2/10], Batch [840/892], Loss: 1.4522\n",
            "[LSTM] Epoch [2/10], Batch [850/892], Loss: 1.5143\n",
            "[LSTM] Epoch [2/10], Batch [860/892], Loss: 1.4647\n",
            "[LSTM] Epoch [2/10], Batch [870/892], Loss: 1.4261\n",
            "[LSTM] Epoch [2/10], Batch [880/892], Loss: 1.5327\n",
            "[LSTM] Epoch [2/10], Batch [890/892], Loss: 1.4265\n",
            "[LSTM] Epoch [2/10] completed, Average Loss: 1.7213, Time: 281.53s\n",
            "[LSTM] Epoch [3/10], Batch [10/892], Loss: 1.4717\n",
            "[LSTM] Epoch [3/10], Batch [20/892], Loss: 1.4092\n",
            "[LSTM] Epoch [3/10], Batch [30/892], Loss: 1.4652\n",
            "[LSTM] Epoch [3/10], Batch [40/892], Loss: 1.5115\n",
            "[LSTM] Epoch [3/10], Batch [50/892], Loss: 1.4691\n",
            "[LSTM] Epoch [3/10], Batch [60/892], Loss: 1.4652\n",
            "[LSTM] Epoch [3/10], Batch [70/892], Loss: 1.4334\n",
            "[LSTM] Epoch [3/10], Batch [80/892], Loss: 1.4856\n",
            "[LSTM] Epoch [3/10], Batch [90/892], Loss: 1.4248\n",
            "[LSTM] Epoch [3/10], Batch [100/892], Loss: 1.4156\n",
            "[LSTM] Epoch [3/10], Batch [110/892], Loss: 1.3658\n",
            "[LSTM] Epoch [3/10], Batch [120/892], Loss: 1.4417\n",
            "[LSTM] Epoch [3/10], Batch [130/892], Loss: 1.4948\n",
            "[LSTM] Epoch [3/10], Batch [140/892], Loss: 1.4152\n",
            "[LSTM] Epoch [3/10], Batch [150/892], Loss: 1.4207\n",
            "[LSTM] Epoch [3/10], Batch [160/892], Loss: 1.3951\n",
            "[LSTM] Epoch [3/10], Batch [170/892], Loss: 1.4857\n",
            "[LSTM] Epoch [3/10], Batch [180/892], Loss: 1.4041\n",
            "[LSTM] Epoch [3/10], Batch [190/892], Loss: 1.4308\n",
            "[LSTM] Epoch [3/10], Batch [200/892], Loss: 1.4153\n",
            "[LSTM] Epoch [3/10], Batch [210/892], Loss: 1.3625\n",
            "[LSTM] Epoch [3/10], Batch [220/892], Loss: 1.4316\n",
            "[LSTM] Epoch [3/10], Batch [230/892], Loss: 1.4305\n",
            "[LSTM] Epoch [3/10], Batch [240/892], Loss: 1.3818\n",
            "[LSTM] Epoch [3/10], Batch [250/892], Loss: 1.3959\n",
            "[LSTM] Epoch [3/10], Batch [260/892], Loss: 1.3522\n",
            "[LSTM] Epoch [3/10], Batch [270/892], Loss: 1.3264\n",
            "[LSTM] Epoch [3/10], Batch [280/892], Loss: 1.3839\n",
            "[LSTM] Epoch [3/10], Batch [290/892], Loss: 1.3542\n",
            "[LSTM] Epoch [3/10], Batch [300/892], Loss: 1.3838\n",
            "[LSTM] Epoch [3/10], Batch [310/892], Loss: 1.3614\n",
            "[LSTM] Epoch [3/10], Batch [320/892], Loss: 1.3686\n",
            "[LSTM] Epoch [3/10], Batch [330/892], Loss: 1.4390\n",
            "[LSTM] Epoch [3/10], Batch [340/892], Loss: 1.3941\n",
            "[LSTM] Epoch [3/10], Batch [350/892], Loss: 1.3718\n",
            "[LSTM] Epoch [3/10], Batch [360/892], Loss: 1.3690\n",
            "[LSTM] Epoch [3/10], Batch [370/892], Loss: 1.3011\n",
            "[LSTM] Epoch [3/10], Batch [380/892], Loss: 1.2872\n",
            "[LSTM] Epoch [3/10], Batch [390/892], Loss: 1.3225\n",
            "[LSTM] Epoch [3/10], Batch [400/892], Loss: 1.3149\n",
            "[LSTM] Epoch [3/10], Batch [410/892], Loss: 1.3909\n",
            "[LSTM] Epoch [3/10], Batch [420/892], Loss: 1.4024\n",
            "[LSTM] Epoch [3/10], Batch [430/892], Loss: 1.3318\n",
            "[LSTM] Epoch [3/10], Batch [440/892], Loss: 1.3668\n",
            "[LSTM] Epoch [3/10], Batch [450/892], Loss: 1.3742\n",
            "[LSTM] Epoch [3/10], Batch [460/892], Loss: 1.3098\n",
            "[LSTM] Epoch [3/10], Batch [470/892], Loss: 1.3361\n",
            "[LSTM] Epoch [3/10], Batch [480/892], Loss: 1.3535\n",
            "[LSTM] Epoch [3/10], Batch [490/892], Loss: 1.3038\n",
            "[LSTM] Epoch [3/10], Batch [500/892], Loss: 1.3082\n",
            "[LSTM] Epoch [3/10], Batch [510/892], Loss: 1.3234\n",
            "[LSTM] Epoch [3/10], Batch [520/892], Loss: 1.3265\n",
            "[LSTM] Epoch [3/10], Batch [530/892], Loss: 1.2837\n",
            "[LSTM] Epoch [3/10], Batch [540/892], Loss: 1.3759\n",
            "[LSTM] Epoch [3/10], Batch [550/892], Loss: 1.3078\n",
            "[LSTM] Epoch [3/10], Batch [560/892], Loss: 1.2653\n",
            "[LSTM] Epoch [3/10], Batch [570/892], Loss: 1.3580\n",
            "[LSTM] Epoch [3/10], Batch [580/892], Loss: 1.3676\n",
            "[LSTM] Epoch [3/10], Batch [590/892], Loss: 1.3005\n",
            "[LSTM] Epoch [3/10], Batch [600/892], Loss: 1.3084\n",
            "[LSTM] Epoch [3/10], Batch [610/892], Loss: 1.3805\n",
            "[LSTM] Epoch [3/10], Batch [620/892], Loss: 1.3315\n",
            "[LSTM] Epoch [3/10], Batch [630/892], Loss: 1.2951\n",
            "[LSTM] Epoch [3/10], Batch [640/892], Loss: 1.2905\n",
            "[LSTM] Epoch [3/10], Batch [650/892], Loss: 1.2994\n",
            "[LSTM] Epoch [3/10], Batch [660/892], Loss: 1.2876\n",
            "[LSTM] Epoch [3/10], Batch [670/892], Loss: 1.2674\n",
            "[LSTM] Epoch [3/10], Batch [680/892], Loss: 1.3797\n",
            "[LSTM] Epoch [3/10], Batch [690/892], Loss: 1.3144\n",
            "[LSTM] Epoch [3/10], Batch [700/892], Loss: 1.3192\n",
            "[LSTM] Epoch [3/10], Batch [710/892], Loss: 1.3109\n",
            "[LSTM] Epoch [3/10], Batch [720/892], Loss: 1.2873\n",
            "[LSTM] Epoch [3/10], Batch [730/892], Loss: 1.2792\n",
            "[LSTM] Epoch [3/10], Batch [740/892], Loss: 1.3117\n",
            "[LSTM] Epoch [3/10], Batch [750/892], Loss: 1.2846\n",
            "[LSTM] Epoch [3/10], Batch [760/892], Loss: 1.3223\n",
            "[LSTM] Epoch [3/10], Batch [770/892], Loss: 1.2439\n",
            "[LSTM] Epoch [3/10], Batch [780/892], Loss: 1.2948\n",
            "[LSTM] Epoch [3/10], Batch [790/892], Loss: 1.3067\n",
            "[LSTM] Epoch [3/10], Batch [800/892], Loss: 1.3101\n",
            "[LSTM] Epoch [3/10], Batch [810/892], Loss: 1.2379\n",
            "[LSTM] Epoch [3/10], Batch [820/892], Loss: 1.2633\n",
            "[LSTM] Epoch [3/10], Batch [830/892], Loss: 1.2515\n",
            "[LSTM] Epoch [3/10], Batch [840/892], Loss: 1.2583\n",
            "[LSTM] Epoch [3/10], Batch [850/892], Loss: 1.3357\n",
            "[LSTM] Epoch [3/10], Batch [860/892], Loss: 1.3112\n",
            "[LSTM] Epoch [3/10], Batch [870/892], Loss: 1.2387\n",
            "[LSTM] Epoch [3/10], Batch [880/892], Loss: 1.2586\n",
            "[LSTM] Epoch [3/10], Batch [890/892], Loss: 1.3543\n",
            "[LSTM] Epoch [3/10] completed, Average Loss: 1.3571, Time: 277.65s\n",
            "[LSTM] Epoch [4/10], Batch [10/892], Loss: 1.2654\n",
            "[LSTM] Epoch [4/10], Batch [20/892], Loss: 1.2335\n",
            "[LSTM] Epoch [4/10], Batch [30/892], Loss: 1.2844\n",
            "[LSTM] Epoch [4/10], Batch [40/892], Loss: 1.3058\n",
            "[LSTM] Epoch [4/10], Batch [50/892], Loss: 1.2475\n",
            "[LSTM] Epoch [4/10], Batch [60/892], Loss: 1.2842\n",
            "[LSTM] Epoch [4/10], Batch [70/892], Loss: 1.3158\n",
            "[LSTM] Epoch [4/10], Batch [80/892], Loss: 1.2985\n",
            "[LSTM] Epoch [4/10], Batch [90/892], Loss: 1.3413\n",
            "[LSTM] Epoch [4/10], Batch [100/892], Loss: 1.2899\n",
            "[LSTM] Epoch [4/10], Batch [110/892], Loss: 1.2737\n",
            "[LSTM] Epoch [4/10], Batch [120/892], Loss: 1.2570\n",
            "[LSTM] Epoch [4/10], Batch [130/892], Loss: 1.2325\n",
            "[LSTM] Epoch [4/10], Batch [140/892], Loss: 1.1981\n",
            "[LSTM] Epoch [4/10], Batch [150/892], Loss: 1.2074\n",
            "[LSTM] Epoch [4/10], Batch [160/892], Loss: 1.2590\n",
            "[LSTM] Epoch [4/10], Batch [170/892], Loss: 1.2378\n",
            "[LSTM] Epoch [4/10], Batch [180/892], Loss: 1.2542\n",
            "[LSTM] Epoch [4/10], Batch [190/892], Loss: 1.3057\n",
            "[LSTM] Epoch [4/10], Batch [200/892], Loss: 1.3113\n",
            "[LSTM] Epoch [4/10], Batch [210/892], Loss: 1.2833\n",
            "[LSTM] Epoch [4/10], Batch [220/892], Loss: 1.3135\n",
            "[LSTM] Epoch [4/10], Batch [230/892], Loss: 1.2266\n",
            "[LSTM] Epoch [4/10], Batch [240/892], Loss: 1.3210\n",
            "[LSTM] Epoch [4/10], Batch [250/892], Loss: 1.2429\n",
            "[LSTM] Epoch [4/10], Batch [260/892], Loss: 1.2674\n",
            "[LSTM] Epoch [4/10], Batch [270/892], Loss: 1.1677\n",
            "[LSTM] Epoch [4/10], Batch [280/892], Loss: 1.2565\n",
            "[LSTM] Epoch [4/10], Batch [290/892], Loss: 1.2304\n",
            "[LSTM] Epoch [4/10], Batch [300/892], Loss: 1.2343\n",
            "[LSTM] Epoch [4/10], Batch [310/892], Loss: 1.2787\n",
            "[LSTM] Epoch [4/10], Batch [320/892], Loss: 1.1738\n",
            "[LSTM] Epoch [4/10], Batch [330/892], Loss: 1.2788\n",
            "[LSTM] Epoch [4/10], Batch [340/892], Loss: 1.1917\n",
            "[LSTM] Epoch [4/10], Batch [350/892], Loss: 1.2533\n",
            "[LSTM] Epoch [4/10], Batch [360/892], Loss: 1.1712\n",
            "[LSTM] Epoch [4/10], Batch [370/892], Loss: 1.2088\n",
            "[LSTM] Epoch [4/10], Batch [380/892], Loss: 1.2332\n",
            "[LSTM] Epoch [4/10], Batch [390/892], Loss: 1.2031\n",
            "[LSTM] Epoch [4/10], Batch [400/892], Loss: 1.2669\n",
            "[LSTM] Epoch [4/10], Batch [410/892], Loss: 1.3176\n",
            "[LSTM] Epoch [4/10], Batch [420/892], Loss: 1.1948\n",
            "[LSTM] Epoch [4/10], Batch [430/892], Loss: 1.2358\n",
            "[LSTM] Epoch [4/10], Batch [440/892], Loss: 1.1851\n",
            "[LSTM] Epoch [4/10], Batch [450/892], Loss: 1.2357\n",
            "[LSTM] Epoch [4/10], Batch [460/892], Loss: 1.2200\n",
            "[LSTM] Epoch [4/10], Batch [470/892], Loss: 1.1739\n",
            "[LSTM] Epoch [4/10], Batch [480/892], Loss: 1.2127\n",
            "[LSTM] Epoch [4/10], Batch [490/892], Loss: 1.1835\n",
            "[LSTM] Epoch [4/10], Batch [500/892], Loss: 1.1944\n",
            "[LSTM] Epoch [4/10], Batch [510/892], Loss: 1.2026\n",
            "[LSTM] Epoch [4/10], Batch [520/892], Loss: 1.2435\n",
            "[LSTM] Epoch [4/10], Batch [530/892], Loss: 1.2105\n",
            "[LSTM] Epoch [4/10], Batch [540/892], Loss: 1.1790\n",
            "[LSTM] Epoch [4/10], Batch [550/892], Loss: 1.1728\n",
            "[LSTM] Epoch [4/10], Batch [560/892], Loss: 1.2606\n",
            "[LSTM] Epoch [4/10], Batch [570/892], Loss: 1.2023\n",
            "[LSTM] Epoch [4/10], Batch [580/892], Loss: 1.1973\n",
            "[LSTM] Epoch [4/10], Batch [590/892], Loss: 1.2500\n",
            "[LSTM] Epoch [4/10], Batch [600/892], Loss: 1.1996\n",
            "[LSTM] Epoch [4/10], Batch [610/892], Loss: 1.3423\n",
            "[LSTM] Epoch [4/10], Batch [620/892], Loss: 1.2065\n",
            "[LSTM] Epoch [4/10], Batch [630/892], Loss: 1.2309\n",
            "[LSTM] Epoch [4/10], Batch [640/892], Loss: 1.2047\n",
            "[LSTM] Epoch [4/10], Batch [650/892], Loss: 1.1351\n",
            "[LSTM] Epoch [4/10], Batch [660/892], Loss: 1.2203\n",
            "[LSTM] Epoch [4/10], Batch [670/892], Loss: 1.1784\n",
            "[LSTM] Epoch [4/10], Batch [680/892], Loss: 1.1892\n",
            "[LSTM] Epoch [4/10], Batch [690/892], Loss: 1.1366\n",
            "[LSTM] Epoch [4/10], Batch [700/892], Loss: 1.1377\n",
            "[LSTM] Epoch [4/10], Batch [710/892], Loss: 1.1127\n",
            "[LSTM] Epoch [4/10], Batch [720/892], Loss: 1.2966\n",
            "[LSTM] Epoch [4/10], Batch [730/892], Loss: 1.1829\n",
            "[LSTM] Epoch [4/10], Batch [740/892], Loss: 1.1132\n",
            "[LSTM] Epoch [4/10], Batch [750/892], Loss: 1.1639\n",
            "[LSTM] Epoch [4/10], Batch [760/892], Loss: 1.1728\n",
            "[LSTM] Epoch [4/10], Batch [770/892], Loss: 1.1697\n",
            "[LSTM] Epoch [4/10], Batch [780/892], Loss: 1.1921\n",
            "[LSTM] Epoch [4/10], Batch [790/892], Loss: 1.1827\n",
            "[LSTM] Epoch [4/10], Batch [800/892], Loss: 1.1782\n",
            "[LSTM] Epoch [4/10], Batch [810/892], Loss: 1.2004\n",
            "[LSTM] Epoch [4/10], Batch [820/892], Loss: 1.1150\n",
            "[LSTM] Epoch [4/10], Batch [830/892], Loss: 1.1287\n",
            "[LSTM] Epoch [4/10], Batch [840/892], Loss: 1.2143\n",
            "[LSTM] Epoch [4/10], Batch [850/892], Loss: 1.1733\n",
            "[LSTM] Epoch [4/10], Batch [860/892], Loss: 1.1798\n",
            "[LSTM] Epoch [4/10], Batch [870/892], Loss: 1.1604\n",
            "[LSTM] Epoch [4/10], Batch [880/892], Loss: 1.2159\n",
            "[LSTM] Epoch [4/10], Batch [890/892], Loss: 1.2385\n",
            "[LSTM] Epoch [4/10] completed, Average Loss: 1.2131, Time: 279.84s\n",
            "[LSTM] Epoch [5/10], Batch [10/892], Loss: 1.1586\n",
            "[LSTM] Epoch [5/10], Batch [20/892], Loss: 1.0918\n",
            "[LSTM] Epoch [5/10], Batch [30/892], Loss: 1.1913\n",
            "[LSTM] Epoch [5/10], Batch [40/892], Loss: 1.1418\n",
            "[LSTM] Epoch [5/10], Batch [50/892], Loss: 1.1276\n",
            "[LSTM] Epoch [5/10], Batch [60/892], Loss: 1.1155\n",
            "[LSTM] Epoch [5/10], Batch [70/892], Loss: 1.2306\n",
            "[LSTM] Epoch [5/10], Batch [80/892], Loss: 1.1305\n",
            "[LSTM] Epoch [5/10], Batch [90/892], Loss: 1.1057\n",
            "[LSTM] Epoch [5/10], Batch [100/892], Loss: 1.1725\n",
            "[LSTM] Epoch [5/10], Batch [110/892], Loss: 1.1660\n",
            "[LSTM] Epoch [5/10], Batch [120/892], Loss: 1.1694\n",
            "[LSTM] Epoch [5/10], Batch [130/892], Loss: 1.1289\n",
            "[LSTM] Epoch [5/10], Batch [140/892], Loss: 1.1047\n",
            "[LSTM] Epoch [5/10], Batch [150/892], Loss: 1.1641\n",
            "[LSTM] Epoch [5/10], Batch [160/892], Loss: 1.0861\n",
            "[LSTM] Epoch [5/10], Batch [170/892], Loss: 1.1122\n",
            "[LSTM] Epoch [5/10], Batch [180/892], Loss: 1.2149\n",
            "[LSTM] Epoch [5/10], Batch [190/892], Loss: 1.1868\n",
            "[LSTM] Epoch [5/10], Batch [200/892], Loss: 1.1097\n",
            "[LSTM] Epoch [5/10], Batch [210/892], Loss: 1.1590\n",
            "[LSTM] Epoch [5/10], Batch [220/892], Loss: 1.0749\n",
            "[LSTM] Epoch [5/10], Batch [230/892], Loss: 1.0300\n",
            "[LSTM] Epoch [5/10], Batch [240/892], Loss: 1.1243\n",
            "[LSTM] Epoch [5/10], Batch [250/892], Loss: 1.1564\n",
            "[LSTM] Epoch [5/10], Batch [260/892], Loss: 1.1639\n",
            "[LSTM] Epoch [5/10], Batch [270/892], Loss: 1.1592\n",
            "[LSTM] Epoch [5/10], Batch [280/892], Loss: 1.1656\n",
            "[LSTM] Epoch [5/10], Batch [290/892], Loss: 1.1784\n",
            "[LSTM] Epoch [5/10], Batch [300/892], Loss: 1.1261\n",
            "[LSTM] Epoch [5/10], Batch [310/892], Loss: 1.1221\n",
            "[LSTM] Epoch [5/10], Batch [320/892], Loss: 1.1640\n",
            "[LSTM] Epoch [5/10], Batch [330/892], Loss: 1.0738\n",
            "[LSTM] Epoch [5/10], Batch [340/892], Loss: 1.1069\n",
            "[LSTM] Epoch [5/10], Batch [350/892], Loss: 1.1073\n",
            "[LSTM] Epoch [5/10], Batch [360/892], Loss: 1.0863\n",
            "[LSTM] Epoch [5/10], Batch [370/892], Loss: 1.1037\n",
            "[LSTM] Epoch [5/10], Batch [380/892], Loss: 1.1332\n",
            "[LSTM] Epoch [5/10], Batch [390/892], Loss: 1.0453\n",
            "[LSTM] Epoch [5/10], Batch [400/892], Loss: 1.1840\n",
            "[LSTM] Epoch [5/10], Batch [410/892], Loss: 1.0430\n",
            "[LSTM] Epoch [5/10], Batch [420/892], Loss: 1.0825\n",
            "[LSTM] Epoch [5/10], Batch [430/892], Loss: 1.0765\n",
            "[LSTM] Epoch [5/10], Batch [440/892], Loss: 1.1361\n",
            "[LSTM] Epoch [5/10], Batch [450/892], Loss: 1.1375\n",
            "[LSTM] Epoch [5/10], Batch [460/892], Loss: 1.1736\n",
            "[LSTM] Epoch [5/10], Batch [470/892], Loss: 1.1597\n",
            "[LSTM] Epoch [5/10], Batch [480/892], Loss: 1.0421\n",
            "[LSTM] Epoch [5/10], Batch [490/892], Loss: 1.1421\n",
            "[LSTM] Epoch [5/10], Batch [500/892], Loss: 1.1620\n",
            "[LSTM] Epoch [5/10], Batch [510/892], Loss: 1.1145\n",
            "[LSTM] Epoch [5/10], Batch [520/892], Loss: 1.1113\n",
            "[LSTM] Epoch [5/10], Batch [530/892], Loss: 1.1493\n",
            "[LSTM] Epoch [5/10], Batch [540/892], Loss: 1.1236\n",
            "[LSTM] Epoch [5/10], Batch [550/892], Loss: 1.0833\n",
            "[LSTM] Epoch [5/10], Batch [560/892], Loss: 1.1212\n",
            "[LSTM] Epoch [5/10], Batch [570/892], Loss: 1.0898\n",
            "[LSTM] Epoch [5/10], Batch [580/892], Loss: 1.1160\n",
            "[LSTM] Epoch [5/10], Batch [590/892], Loss: 1.0823\n",
            "[LSTM] Epoch [5/10], Batch [600/892], Loss: 1.1402\n",
            "[LSTM] Epoch [5/10], Batch [610/892], Loss: 1.1951\n",
            "[LSTM] Epoch [5/10], Batch [620/892], Loss: 1.1082\n",
            "[LSTM] Epoch [5/10], Batch [630/892], Loss: 1.0631\n",
            "[LSTM] Epoch [5/10], Batch [640/892], Loss: 1.1341\n",
            "[LSTM] Epoch [5/10], Batch [650/892], Loss: 1.1104\n",
            "[LSTM] Epoch [5/10], Batch [660/892], Loss: 1.0862\n",
            "[LSTM] Epoch [5/10], Batch [670/892], Loss: 1.1636\n",
            "[LSTM] Epoch [5/10], Batch [680/892], Loss: 1.1002\n",
            "[LSTM] Epoch [5/10], Batch [690/892], Loss: 1.0261\n",
            "[LSTM] Epoch [5/10], Batch [700/892], Loss: 1.1613\n",
            "[LSTM] Epoch [5/10], Batch [710/892], Loss: 1.0839\n",
            "[LSTM] Epoch [5/10], Batch [720/892], Loss: 1.0425\n",
            "[LSTM] Epoch [5/10], Batch [730/892], Loss: 1.0351\n",
            "[LSTM] Epoch [5/10], Batch [740/892], Loss: 1.0727\n",
            "[LSTM] Epoch [5/10], Batch [750/892], Loss: 1.0998\n",
            "[LSTM] Epoch [5/10], Batch [760/892], Loss: 1.1471\n",
            "[LSTM] Epoch [5/10], Batch [770/892], Loss: 1.0703\n",
            "[LSTM] Epoch [5/10], Batch [780/892], Loss: 1.1421\n",
            "[LSTM] Epoch [5/10], Batch [790/892], Loss: 1.0649\n",
            "[LSTM] Epoch [5/10], Batch [800/892], Loss: 1.1073\n",
            "[LSTM] Epoch [5/10], Batch [810/892], Loss: 1.0949\n",
            "[LSTM] Epoch [5/10], Batch [820/892], Loss: 1.1165\n",
            "[LSTM] Epoch [5/10], Batch [830/892], Loss: 1.0525\n",
            "[LSTM] Epoch [5/10], Batch [840/892], Loss: 1.0575\n",
            "[LSTM] Epoch [5/10], Batch [850/892], Loss: 1.0431\n",
            "[LSTM] Epoch [5/10], Batch [860/892], Loss: 1.2156\n",
            "[LSTM] Epoch [5/10], Batch [870/892], Loss: 1.0786\n",
            "[LSTM] Epoch [5/10], Batch [880/892], Loss: 0.9932\n",
            "[LSTM] Epoch [5/10], Batch [890/892], Loss: 1.0678\n",
            "[LSTM] Epoch [5/10] completed, Average Loss: 1.1161, Time: 282.54s\n",
            "[LSTM] Epoch [6/10], Batch [10/892], Loss: 1.0256\n",
            "[LSTM] Epoch [6/10], Batch [20/892], Loss: 1.1058\n",
            "[LSTM] Epoch [6/10], Batch [30/892], Loss: 1.2357\n",
            "[LSTM] Epoch [6/10], Batch [40/892], Loss: 0.9906\n",
            "[LSTM] Epoch [6/10], Batch [50/892], Loss: 1.0757\n",
            "[LSTM] Epoch [6/10], Batch [60/892], Loss: 1.0248\n",
            "[LSTM] Epoch [6/10], Batch [70/892], Loss: 1.0727\n",
            "[LSTM] Epoch [6/10], Batch [80/892], Loss: 1.0177\n",
            "[LSTM] Epoch [6/10], Batch [90/892], Loss: 1.1701\n",
            "[LSTM] Epoch [6/10], Batch [100/892], Loss: 1.1135\n",
            "[LSTM] Epoch [6/10], Batch [110/892], Loss: 1.0081\n",
            "[LSTM] Epoch [6/10], Batch [120/892], Loss: 1.0959\n",
            "[LSTM] Epoch [6/10], Batch [130/892], Loss: 1.1201\n",
            "[LSTM] Epoch [6/10], Batch [140/892], Loss: 1.0705\n",
            "[LSTM] Epoch [6/10], Batch [150/892], Loss: 1.0556\n",
            "[LSTM] Epoch [6/10], Batch [160/892], Loss: 1.1284\n",
            "[LSTM] Epoch [6/10], Batch [170/892], Loss: 1.0188\n",
            "[LSTM] Epoch [6/10], Batch [180/892], Loss: 1.0522\n",
            "[LSTM] Epoch [6/10], Batch [190/892], Loss: 1.0256\n",
            "[LSTM] Epoch [6/10], Batch [200/892], Loss: 1.0960\n",
            "[LSTM] Epoch [6/10], Batch [210/892], Loss: 1.0629\n",
            "[LSTM] Epoch [6/10], Batch [220/892], Loss: 1.0715\n",
            "[LSTM] Epoch [6/10], Batch [230/892], Loss: 1.0567\n",
            "[LSTM] Epoch [6/10], Batch [240/892], Loss: 0.9573\n",
            "[LSTM] Epoch [6/10], Batch [250/892], Loss: 1.0027\n",
            "[LSTM] Epoch [6/10], Batch [260/892], Loss: 1.0476\n",
            "[LSTM] Epoch [6/10], Batch [270/892], Loss: 1.0636\n",
            "[LSTM] Epoch [6/10], Batch [280/892], Loss: 1.0664\n",
            "[LSTM] Epoch [6/10], Batch [290/892], Loss: 1.0774\n",
            "[LSTM] Epoch [6/10], Batch [300/892], Loss: 1.0164\n",
            "[LSTM] Epoch [6/10], Batch [310/892], Loss: 1.1026\n",
            "[LSTM] Epoch [6/10], Batch [320/892], Loss: 1.0346\n",
            "[LSTM] Epoch [6/10], Batch [330/892], Loss: 1.0484\n",
            "[LSTM] Epoch [6/10], Batch [340/892], Loss: 1.0548\n",
            "[LSTM] Epoch [6/10], Batch [350/892], Loss: 1.0839\n",
            "[LSTM] Epoch [6/10], Batch [360/892], Loss: 1.1261\n",
            "[LSTM] Epoch [6/10], Batch [370/892], Loss: 1.0149\n",
            "[LSTM] Epoch [6/10], Batch [380/892], Loss: 1.0607\n",
            "[LSTM] Epoch [6/10], Batch [390/892], Loss: 1.0149\n",
            "[LSTM] Epoch [6/10], Batch [400/892], Loss: 1.1033\n",
            "[LSTM] Epoch [6/10], Batch [410/892], Loss: 1.0951\n",
            "[LSTM] Epoch [6/10], Batch [420/892], Loss: 1.0588\n",
            "[LSTM] Epoch [6/10], Batch [430/892], Loss: 1.0362\n",
            "[LSTM] Epoch [6/10], Batch [440/892], Loss: 1.0299\n",
            "[LSTM] Epoch [6/10], Batch [450/892], Loss: 1.0525\n",
            "[LSTM] Epoch [6/10], Batch [460/892], Loss: 1.0092\n",
            "[LSTM] Epoch [6/10], Batch [470/892], Loss: 0.9963\n",
            "[LSTM] Epoch [6/10], Batch [480/892], Loss: 1.0891\n",
            "[LSTM] Epoch [6/10], Batch [490/892], Loss: 0.9926\n",
            "[LSTM] Epoch [6/10], Batch [500/892], Loss: 1.0720\n",
            "[LSTM] Epoch [6/10], Batch [510/892], Loss: 1.0574\n",
            "[LSTM] Epoch [6/10], Batch [520/892], Loss: 1.0530\n",
            "[LSTM] Epoch [6/10], Batch [530/892], Loss: 1.0126\n",
            "[LSTM] Epoch [6/10], Batch [540/892], Loss: 1.0147\n",
            "[LSTM] Epoch [6/10], Batch [550/892], Loss: 1.0233\n",
            "[LSTM] Epoch [6/10], Batch [560/892], Loss: 1.0163\n",
            "[LSTM] Epoch [6/10], Batch [570/892], Loss: 1.0819\n",
            "[LSTM] Epoch [6/10], Batch [580/892], Loss: 1.0481\n",
            "[LSTM] Epoch [6/10], Batch [590/892], Loss: 1.0866\n",
            "[LSTM] Epoch [6/10], Batch [600/892], Loss: 1.0122\n",
            "[LSTM] Epoch [6/10], Batch [610/892], Loss: 1.0566\n",
            "[LSTM] Epoch [6/10], Batch [620/892], Loss: 0.9829\n",
            "[LSTM] Epoch [6/10], Batch [630/892], Loss: 0.9764\n",
            "[LSTM] Epoch [6/10], Batch [640/892], Loss: 0.9734\n",
            "[LSTM] Epoch [6/10], Batch [650/892], Loss: 1.0401\n",
            "[LSTM] Epoch [6/10], Batch [660/892], Loss: 1.0651\n",
            "[LSTM] Epoch [6/10], Batch [670/892], Loss: 1.0068\n",
            "[LSTM] Epoch [6/10], Batch [680/892], Loss: 1.0977\n",
            "[LSTM] Epoch [6/10], Batch [690/892], Loss: 1.0513\n",
            "[LSTM] Epoch [6/10], Batch [700/892], Loss: 1.0021\n",
            "[LSTM] Epoch [6/10], Batch [710/892], Loss: 1.1030\n",
            "[LSTM] Epoch [6/10], Batch [720/892], Loss: 0.9538\n",
            "[LSTM] Epoch [6/10], Batch [730/892], Loss: 1.0463\n",
            "[LSTM] Epoch [6/10], Batch [740/892], Loss: 0.9352\n",
            "[LSTM] Epoch [6/10], Batch [750/892], Loss: 0.9971\n",
            "[LSTM] Epoch [6/10], Batch [760/892], Loss: 1.1017\n",
            "[LSTM] Epoch [6/10], Batch [770/892], Loss: 1.0536\n",
            "[LSTM] Epoch [6/10], Batch [780/892], Loss: 1.0711\n",
            "[LSTM] Epoch [6/10], Batch [790/892], Loss: 1.0624\n",
            "[LSTM] Epoch [6/10], Batch [800/892], Loss: 0.9524\n",
            "[LSTM] Epoch [6/10], Batch [810/892], Loss: 1.0369\n",
            "[LSTM] Epoch [6/10], Batch [820/892], Loss: 1.0323\n",
            "[LSTM] Epoch [6/10], Batch [830/892], Loss: 0.9995\n",
            "[LSTM] Epoch [6/10], Batch [840/892], Loss: 1.0707\n",
            "[LSTM] Epoch [6/10], Batch [850/892], Loss: 1.0179\n",
            "[LSTM] Epoch [6/10], Batch [860/892], Loss: 1.0716\n",
            "[LSTM] Epoch [6/10], Batch [870/892], Loss: 1.0545\n",
            "[LSTM] Epoch [6/10], Batch [880/892], Loss: 0.9973\n",
            "[LSTM] Epoch [6/10], Batch [890/892], Loss: 1.0599\n",
            "[LSTM] Epoch [6/10] completed, Average Loss: 1.0465, Time: 285.58s\n",
            "[LSTM] Epoch [7/10], Batch [10/892], Loss: 1.0195\n",
            "[LSTM] Epoch [7/10], Batch [20/892], Loss: 0.9871\n",
            "[LSTM] Epoch [7/10], Batch [30/892], Loss: 0.9562\n",
            "[LSTM] Epoch [7/10], Batch [40/892], Loss: 0.9571\n",
            "[LSTM] Epoch [7/10], Batch [50/892], Loss: 1.0144\n",
            "[LSTM] Epoch [7/10], Batch [60/892], Loss: 1.0234\n",
            "[LSTM] Epoch [7/10], Batch [70/892], Loss: 0.9494\n",
            "[LSTM] Epoch [7/10], Batch [80/892], Loss: 0.9848\n",
            "[LSTM] Epoch [7/10], Batch [90/892], Loss: 1.0083\n",
            "[LSTM] Epoch [7/10], Batch [100/892], Loss: 0.9970\n",
            "[LSTM] Epoch [7/10], Batch [110/892], Loss: 1.0167\n",
            "[LSTM] Epoch [7/10], Batch [120/892], Loss: 0.9647\n",
            "[LSTM] Epoch [7/10], Batch [130/892], Loss: 1.0413\n",
            "[LSTM] Epoch [7/10], Batch [140/892], Loss: 0.9481\n",
            "[LSTM] Epoch [7/10], Batch [150/892], Loss: 1.0152\n",
            "[LSTM] Epoch [7/10], Batch [160/892], Loss: 0.9559\n",
            "[LSTM] Epoch [7/10], Batch [170/892], Loss: 1.0064\n",
            "[LSTM] Epoch [7/10], Batch [180/892], Loss: 1.1062\n",
            "[LSTM] Epoch [7/10], Batch [190/892], Loss: 1.0133\n",
            "[LSTM] Epoch [7/10], Batch [200/892], Loss: 1.0090\n",
            "[LSTM] Epoch [7/10], Batch [210/892], Loss: 0.9843\n",
            "[LSTM] Epoch [7/10], Batch [220/892], Loss: 0.9431\n",
            "[LSTM] Epoch [7/10], Batch [230/892], Loss: 1.0681\n",
            "[LSTM] Epoch [7/10], Batch [240/892], Loss: 1.0222\n",
            "[LSTM] Epoch [7/10], Batch [250/892], Loss: 1.0852\n",
            "[LSTM] Epoch [7/10], Batch [260/892], Loss: 0.9779\n",
            "[LSTM] Epoch [7/10], Batch [270/892], Loss: 0.9858\n",
            "[LSTM] Epoch [7/10], Batch [280/892], Loss: 0.9539\n",
            "[LSTM] Epoch [7/10], Batch [290/892], Loss: 0.9742\n",
            "[LSTM] Epoch [7/10], Batch [300/892], Loss: 1.0306\n",
            "[LSTM] Epoch [7/10], Batch [310/892], Loss: 1.0041\n",
            "[LSTM] Epoch [7/10], Batch [320/892], Loss: 0.9585\n",
            "[LSTM] Epoch [7/10], Batch [330/892], Loss: 0.9567\n",
            "[LSTM] Epoch [7/10], Batch [340/892], Loss: 0.9743\n",
            "[LSTM] Epoch [7/10], Batch [350/892], Loss: 0.9679\n",
            "[LSTM] Epoch [7/10], Batch [360/892], Loss: 0.9654\n",
            "[LSTM] Epoch [7/10], Batch [370/892], Loss: 1.0284\n",
            "[LSTM] Epoch [7/10], Batch [380/892], Loss: 1.0389\n",
            "[LSTM] Epoch [7/10], Batch [390/892], Loss: 1.0199\n",
            "[LSTM] Epoch [7/10], Batch [400/892], Loss: 0.9809\n",
            "[LSTM] Epoch [7/10], Batch [410/892], Loss: 1.0491\n",
            "[LSTM] Epoch [7/10], Batch [420/892], Loss: 0.9620\n",
            "[LSTM] Epoch [7/10], Batch [430/892], Loss: 1.0004\n",
            "[LSTM] Epoch [7/10], Batch [440/892], Loss: 1.0888\n",
            "[LSTM] Epoch [7/10], Batch [450/892], Loss: 0.9691\n",
            "[LSTM] Epoch [7/10], Batch [460/892], Loss: 0.9924\n",
            "[LSTM] Epoch [7/10], Batch [470/892], Loss: 0.9515\n",
            "[LSTM] Epoch [7/10], Batch [480/892], Loss: 0.9784\n",
            "[LSTM] Epoch [7/10], Batch [490/892], Loss: 0.9953\n",
            "[LSTM] Epoch [7/10], Batch [500/892], Loss: 1.0258\n",
            "[LSTM] Epoch [7/10], Batch [510/892], Loss: 1.0784\n",
            "[LSTM] Epoch [7/10], Batch [520/892], Loss: 1.0309\n",
            "[LSTM] Epoch [7/10], Batch [530/892], Loss: 0.9238\n",
            "[LSTM] Epoch [7/10], Batch [540/892], Loss: 0.9907\n",
            "[LSTM] Epoch [7/10], Batch [550/892], Loss: 0.9624\n",
            "[LSTM] Epoch [7/10], Batch [560/892], Loss: 1.0329\n",
            "[LSTM] Epoch [7/10], Batch [570/892], Loss: 0.9244\n",
            "[LSTM] Epoch [7/10], Batch [580/892], Loss: 0.9772\n",
            "[LSTM] Epoch [7/10], Batch [590/892], Loss: 0.9761\n",
            "[LSTM] Epoch [7/10], Batch [600/892], Loss: 0.9754\n",
            "[LSTM] Epoch [7/10], Batch [610/892], Loss: 0.9597\n",
            "[LSTM] Epoch [7/10], Batch [620/892], Loss: 0.9235\n",
            "[LSTM] Epoch [7/10], Batch [630/892], Loss: 0.9587\n",
            "[LSTM] Epoch [7/10], Batch [640/892], Loss: 0.9105\n",
            "[LSTM] Epoch [7/10], Batch [650/892], Loss: 0.9204\n",
            "[LSTM] Epoch [7/10], Batch [660/892], Loss: 0.9715\n",
            "[LSTM] Epoch [7/10], Batch [670/892], Loss: 0.9907\n",
            "[LSTM] Epoch [7/10], Batch [680/892], Loss: 1.0212\n",
            "[LSTM] Epoch [7/10], Batch [690/892], Loss: 1.0765\n",
            "[LSTM] Epoch [7/10], Batch [700/892], Loss: 1.0565\n",
            "[LSTM] Epoch [7/10], Batch [710/892], Loss: 0.9359\n",
            "[LSTM] Epoch [7/10], Batch [720/892], Loss: 1.0375\n",
            "[LSTM] Epoch [7/10], Batch [730/892], Loss: 0.9954\n",
            "[LSTM] Epoch [7/10], Batch [740/892], Loss: 0.9422\n",
            "[LSTM] Epoch [7/10], Batch [750/892], Loss: 0.9849\n",
            "[LSTM] Epoch [7/10], Batch [760/892], Loss: 0.9519\n",
            "[LSTM] Epoch [7/10], Batch [770/892], Loss: 1.0002\n",
            "[LSTM] Epoch [7/10], Batch [780/892], Loss: 0.9327\n",
            "[LSTM] Epoch [7/10], Batch [790/892], Loss: 0.9160\n",
            "[LSTM] Epoch [7/10], Batch [800/892], Loss: 1.0022\n",
            "[LSTM] Epoch [7/10], Batch [810/892], Loss: 1.0237\n",
            "[LSTM] Epoch [7/10], Batch [820/892], Loss: 1.0017\n",
            "[LSTM] Epoch [7/10], Batch [830/892], Loss: 0.9407\n",
            "[LSTM] Epoch [7/10], Batch [840/892], Loss: 1.0066\n",
            "[LSTM] Epoch [7/10], Batch [850/892], Loss: 0.9278\n",
            "[LSTM] Epoch [7/10], Batch [860/892], Loss: 0.9506\n",
            "[LSTM] Epoch [7/10], Batch [870/892], Loss: 0.9950\n",
            "[LSTM] Epoch [7/10], Batch [880/892], Loss: 0.9921\n",
            "[LSTM] Epoch [7/10], Batch [890/892], Loss: 0.9600\n",
            "[LSTM] Epoch [7/10] completed, Average Loss: 0.9877, Time: 282.41s\n",
            "[LSTM] Epoch [8/10], Batch [10/892], Loss: 0.9540\n",
            "[LSTM] Epoch [8/10], Batch [20/892], Loss: 1.0426\n",
            "[LSTM] Epoch [8/10], Batch [30/892], Loss: 0.8949\n",
            "[LSTM] Epoch [8/10], Batch [40/892], Loss: 0.9974\n",
            "[LSTM] Epoch [8/10], Batch [50/892], Loss: 1.0221\n",
            "[LSTM] Epoch [8/10], Batch [60/892], Loss: 0.9086\n",
            "[LSTM] Epoch [8/10], Batch [70/892], Loss: 0.8964\n",
            "[LSTM] Epoch [8/10], Batch [80/892], Loss: 0.9891\n",
            "[LSTM] Epoch [8/10], Batch [90/892], Loss: 0.9902\n",
            "[LSTM] Epoch [8/10], Batch [100/892], Loss: 1.0116\n",
            "[LSTM] Epoch [8/10], Batch [110/892], Loss: 0.9194\n",
            "[LSTM] Epoch [8/10], Batch [120/892], Loss: 0.8997\n",
            "[LSTM] Epoch [8/10], Batch [130/892], Loss: 1.0063\n",
            "[LSTM] Epoch [8/10], Batch [140/892], Loss: 0.9200\n",
            "[LSTM] Epoch [8/10], Batch [150/892], Loss: 0.9776\n",
            "[LSTM] Epoch [8/10], Batch [160/892], Loss: 0.9812\n",
            "[LSTM] Epoch [8/10], Batch [170/892], Loss: 0.9207\n",
            "[LSTM] Epoch [8/10], Batch [180/892], Loss: 0.9595\n",
            "[LSTM] Epoch [8/10], Batch [190/892], Loss: 0.9341\n",
            "[LSTM] Epoch [8/10], Batch [200/892], Loss: 0.9522\n",
            "[LSTM] Epoch [8/10], Batch [210/892], Loss: 0.9710\n",
            "[LSTM] Epoch [8/10], Batch [220/892], Loss: 0.9268\n",
            "[LSTM] Epoch [8/10], Batch [230/892], Loss: 0.9495\n",
            "[LSTM] Epoch [8/10], Batch [240/892], Loss: 0.8848\n",
            "[LSTM] Epoch [8/10], Batch [250/892], Loss: 0.9996\n",
            "[LSTM] Epoch [8/10], Batch [260/892], Loss: 0.9814\n",
            "[LSTM] Epoch [8/10], Batch [270/892], Loss: 1.0387\n",
            "[LSTM] Epoch [8/10], Batch [280/892], Loss: 0.9697\n",
            "[LSTM] Epoch [8/10], Batch [290/892], Loss: 0.9419\n",
            "[LSTM] Epoch [8/10], Batch [300/892], Loss: 0.9525\n",
            "[LSTM] Epoch [8/10], Batch [310/892], Loss: 0.9862\n",
            "[LSTM] Epoch [8/10], Batch [320/892], Loss: 0.8778\n",
            "[LSTM] Epoch [8/10], Batch [330/892], Loss: 0.9757\n",
            "[LSTM] Epoch [8/10], Batch [340/892], Loss: 0.9049\n",
            "[LSTM] Epoch [8/10], Batch [350/892], Loss: 0.9471\n",
            "[LSTM] Epoch [8/10], Batch [360/892], Loss: 1.0210\n",
            "[LSTM] Epoch [8/10], Batch [370/892], Loss: 0.9749\n",
            "[LSTM] Epoch [8/10], Batch [380/892], Loss: 0.9800\n",
            "[LSTM] Epoch [8/10], Batch [390/892], Loss: 0.9398\n",
            "[LSTM] Epoch [8/10], Batch [400/892], Loss: 0.9516\n",
            "[LSTM] Epoch [8/10], Batch [410/892], Loss: 0.9576\n",
            "[LSTM] Epoch [8/10], Batch [420/892], Loss: 0.9331\n",
            "[LSTM] Epoch [8/10], Batch [430/892], Loss: 1.0328\n",
            "[LSTM] Epoch [8/10], Batch [440/892], Loss: 0.9920\n",
            "[LSTM] Epoch [8/10], Batch [450/892], Loss: 0.9700\n",
            "[LSTM] Epoch [8/10], Batch [460/892], Loss: 0.9135\n",
            "[LSTM] Epoch [8/10], Batch [470/892], Loss: 0.9337\n",
            "[LSTM] Epoch [8/10], Batch [480/892], Loss: 0.9592\n",
            "[LSTM] Epoch [8/10], Batch [490/892], Loss: 0.9187\n",
            "[LSTM] Epoch [8/10], Batch [500/892], Loss: 0.9330\n",
            "[LSTM] Epoch [8/10], Batch [510/892], Loss: 0.9847\n",
            "[LSTM] Epoch [8/10], Batch [520/892], Loss: 0.9111\n",
            "[LSTM] Epoch [8/10], Batch [530/892], Loss: 0.8653\n",
            "[LSTM] Epoch [8/10], Batch [540/892], Loss: 0.8778\n",
            "[LSTM] Epoch [8/10], Batch [550/892], Loss: 0.9260\n",
            "[LSTM] Epoch [8/10], Batch [560/892], Loss: 0.9930\n",
            "[LSTM] Epoch [8/10], Batch [570/892], Loss: 0.9518\n",
            "[LSTM] Epoch [8/10], Batch [580/892], Loss: 0.8718\n",
            "[LSTM] Epoch [8/10], Batch [590/892], Loss: 0.9622\n",
            "[LSTM] Epoch [8/10], Batch [600/892], Loss: 0.8861\n",
            "[LSTM] Epoch [8/10], Batch [610/892], Loss: 0.9903\n",
            "[LSTM] Epoch [8/10], Batch [620/892], Loss: 0.9727\n",
            "[LSTM] Epoch [8/10], Batch [630/892], Loss: 0.8855\n",
            "[LSTM] Epoch [8/10], Batch [640/892], Loss: 0.9595\n",
            "[LSTM] Epoch [8/10], Batch [650/892], Loss: 0.8732\n",
            "[LSTM] Epoch [8/10], Batch [660/892], Loss: 0.8895\n",
            "[LSTM] Epoch [8/10], Batch [670/892], Loss: 0.8595\n",
            "[LSTM] Epoch [8/10], Batch [680/892], Loss: 0.8978\n",
            "[LSTM] Epoch [8/10], Batch [690/892], Loss: 0.9070\n",
            "[LSTM] Epoch [8/10], Batch [700/892], Loss: 0.9743\n",
            "[LSTM] Epoch [8/10], Batch [710/892], Loss: 0.8873\n",
            "[LSTM] Epoch [8/10], Batch [720/892], Loss: 0.9466\n",
            "[LSTM] Epoch [8/10], Batch [730/892], Loss: 0.9194\n",
            "[LSTM] Epoch [8/10], Batch [740/892], Loss: 0.9820\n",
            "[LSTM] Epoch [8/10], Batch [750/892], Loss: 0.9336\n",
            "[LSTM] Epoch [8/10], Batch [760/892], Loss: 0.9338\n",
            "[LSTM] Epoch [8/10], Batch [770/892], Loss: 0.9570\n",
            "[LSTM] Epoch [8/10], Batch [780/892], Loss: 0.9762\n",
            "[LSTM] Epoch [8/10], Batch [790/892], Loss: 0.9172\n",
            "[LSTM] Epoch [8/10], Batch [800/892], Loss: 0.9460\n",
            "[LSTM] Epoch [8/10], Batch [810/892], Loss: 0.9107\n",
            "[LSTM] Epoch [8/10], Batch [820/892], Loss: 0.9715\n",
            "[LSTM] Epoch [8/10], Batch [830/892], Loss: 0.8896\n",
            "[LSTM] Epoch [8/10], Batch [840/892], Loss: 0.9392\n",
            "[LSTM] Epoch [8/10], Batch [850/892], Loss: 0.9572\n",
            "[LSTM] Epoch [8/10], Batch [860/892], Loss: 0.8502\n",
            "[LSTM] Epoch [8/10], Batch [870/892], Loss: 0.9506\n",
            "[LSTM] Epoch [8/10], Batch [880/892], Loss: 0.8624\n",
            "[LSTM] Epoch [8/10], Batch [890/892], Loss: 0.9543\n",
            "[LSTM] Epoch [8/10] completed, Average Loss: 0.9445, Time: 280.54s\n",
            "[LSTM] Epoch [9/10], Batch [10/892], Loss: 0.8669\n",
            "[LSTM] Epoch [9/10], Batch [20/892], Loss: 1.0372\n",
            "[LSTM] Epoch [9/10], Batch [30/892], Loss: 0.9510\n",
            "[LSTM] Epoch [9/10], Batch [40/892], Loss: 0.9543\n",
            "[LSTM] Epoch [9/10], Batch [50/892], Loss: 0.9851\n",
            "[LSTM] Epoch [9/10], Batch [60/892], Loss: 0.9535\n",
            "[LSTM] Epoch [9/10], Batch [70/892], Loss: 0.9305\n",
            "[LSTM] Epoch [9/10], Batch [80/892], Loss: 0.8885\n",
            "[LSTM] Epoch [9/10], Batch [90/892], Loss: 0.8160\n",
            "[LSTM] Epoch [9/10], Batch [100/892], Loss: 0.9510\n",
            "[LSTM] Epoch [9/10], Batch [110/892], Loss: 0.8559\n",
            "[LSTM] Epoch [9/10], Batch [120/892], Loss: 0.9298\n",
            "[LSTM] Epoch [9/10], Batch [130/892], Loss: 0.9162\n",
            "[LSTM] Epoch [9/10], Batch [140/892], Loss: 0.8900\n",
            "[LSTM] Epoch [9/10], Batch [150/892], Loss: 0.9163\n",
            "[LSTM] Epoch [9/10], Batch [160/892], Loss: 0.8619\n",
            "[LSTM] Epoch [9/10], Batch [170/892], Loss: 0.9309\n",
            "[LSTM] Epoch [9/10], Batch [180/892], Loss: 0.8861\n",
            "[LSTM] Epoch [9/10], Batch [190/892], Loss: 0.8743\n",
            "[LSTM] Epoch [9/10], Batch [200/892], Loss: 0.9048\n",
            "[LSTM] Epoch [9/10], Batch [210/892], Loss: 0.8831\n",
            "[LSTM] Epoch [9/10], Batch [220/892], Loss: 0.9781\n",
            "[LSTM] Epoch [9/10], Batch [230/892], Loss: 0.8682\n",
            "[LSTM] Epoch [9/10], Batch [240/892], Loss: 0.9524\n",
            "[LSTM] Epoch [9/10], Batch [250/892], Loss: 0.9388\n",
            "[LSTM] Epoch [9/10], Batch [260/892], Loss: 0.9166\n",
            "[LSTM] Epoch [9/10], Batch [270/892], Loss: 0.8505\n",
            "[LSTM] Epoch [9/10], Batch [280/892], Loss: 0.9415\n",
            "[LSTM] Epoch [9/10], Batch [290/892], Loss: 0.9191\n",
            "[LSTM] Epoch [9/10], Batch [300/892], Loss: 0.9319\n",
            "[LSTM] Epoch [9/10], Batch [310/892], Loss: 0.8531\n",
            "[LSTM] Epoch [9/10], Batch [320/892], Loss: 0.8483\n",
            "[LSTM] Epoch [9/10], Batch [330/892], Loss: 0.9201\n",
            "[LSTM] Epoch [9/10], Batch [340/892], Loss: 0.9568\n",
            "[LSTM] Epoch [9/10], Batch [350/892], Loss: 0.9350\n",
            "[LSTM] Epoch [9/10], Batch [360/892], Loss: 0.8366\n",
            "[LSTM] Epoch [9/10], Batch [370/892], Loss: 0.8868\n",
            "[LSTM] Epoch [9/10], Batch [380/892], Loss: 0.9101\n",
            "[LSTM] Epoch [9/10], Batch [390/892], Loss: 0.9101\n",
            "[LSTM] Epoch [9/10], Batch [400/892], Loss: 0.9127\n",
            "[LSTM] Epoch [9/10], Batch [410/892], Loss: 0.8548\n",
            "[LSTM] Epoch [9/10], Batch [420/892], Loss: 0.8618\n",
            "[LSTM] Epoch [9/10], Batch [430/892], Loss: 0.8942\n",
            "[LSTM] Epoch [9/10], Batch [440/892], Loss: 0.8838\n",
            "[LSTM] Epoch [9/10], Batch [450/892], Loss: 0.9264\n",
            "[LSTM] Epoch [9/10], Batch [460/892], Loss: 0.9023\n",
            "[LSTM] Epoch [9/10], Batch [470/892], Loss: 0.8692\n",
            "[LSTM] Epoch [9/10], Batch [480/892], Loss: 0.8692\n",
            "[LSTM] Epoch [9/10], Batch [490/892], Loss: 0.8416\n",
            "[LSTM] Epoch [9/10], Batch [500/892], Loss: 0.8657\n",
            "[LSTM] Epoch [9/10], Batch [510/892], Loss: 0.9102\n",
            "[LSTM] Epoch [9/10], Batch [520/892], Loss: 0.8986\n",
            "[LSTM] Epoch [9/10], Batch [530/892], Loss: 0.8628\n",
            "[LSTM] Epoch [9/10], Batch [540/892], Loss: 0.8600\n",
            "[LSTM] Epoch [9/10], Batch [550/892], Loss: 0.9142\n",
            "[LSTM] Epoch [9/10], Batch [560/892], Loss: 0.8486\n",
            "[LSTM] Epoch [9/10], Batch [570/892], Loss: 0.9176\n",
            "[LSTM] Epoch [9/10], Batch [580/892], Loss: 0.8637\n",
            "[LSTM] Epoch [9/10], Batch [590/892], Loss: 0.9015\n",
            "[LSTM] Epoch [9/10], Batch [600/892], Loss: 0.8739\n",
            "[LSTM] Epoch [9/10], Batch [610/892], Loss: 0.8801\n",
            "[LSTM] Epoch [9/10], Batch [620/892], Loss: 0.8965\n",
            "[LSTM] Epoch [9/10], Batch [630/892], Loss: 0.9418\n",
            "[LSTM] Epoch [9/10], Batch [640/892], Loss: 0.9223\n",
            "[LSTM] Epoch [9/10], Batch [650/892], Loss: 0.9515\n",
            "[LSTM] Epoch [9/10], Batch [660/892], Loss: 0.9498\n",
            "[LSTM] Epoch [9/10], Batch [670/892], Loss: 0.9015\n",
            "[LSTM] Epoch [9/10], Batch [680/892], Loss: 0.9188\n",
            "[LSTM] Epoch [9/10], Batch [690/892], Loss: 0.8497\n",
            "[LSTM] Epoch [9/10], Batch [700/892], Loss: 0.8018\n",
            "[LSTM] Epoch [9/10], Batch [710/892], Loss: 0.8588\n",
            "[LSTM] Epoch [9/10], Batch [720/892], Loss: 0.8746\n",
            "[LSTM] Epoch [9/10], Batch [730/892], Loss: 0.8875\n",
            "[LSTM] Epoch [9/10], Batch [740/892], Loss: 0.9358\n",
            "[LSTM] Epoch [9/10], Batch [750/892], Loss: 0.9659\n",
            "[LSTM] Epoch [9/10], Batch [760/892], Loss: 0.8540\n",
            "[LSTM] Epoch [9/10], Batch [770/892], Loss: 0.9035\n",
            "[LSTM] Epoch [9/10], Batch [780/892], Loss: 0.8803\n",
            "[LSTM] Epoch [9/10], Batch [790/892], Loss: 0.8632\n",
            "[LSTM] Epoch [9/10], Batch [800/892], Loss: 0.8971\n",
            "[LSTM] Epoch [9/10], Batch [810/892], Loss: 0.9016\n",
            "[LSTM] Epoch [9/10], Batch [820/892], Loss: 0.8977\n",
            "[LSTM] Epoch [9/10], Batch [830/892], Loss: 0.8995\n",
            "[LSTM] Epoch [9/10], Batch [840/892], Loss: 0.8658\n",
            "[LSTM] Epoch [9/10], Batch [850/892], Loss: 0.8751\n",
            "[LSTM] Epoch [9/10], Batch [860/892], Loss: 0.9261\n",
            "[LSTM] Epoch [9/10], Batch [870/892], Loss: 0.9099\n",
            "[LSTM] Epoch [9/10], Batch [880/892], Loss: 0.8470\n",
            "[LSTM] Epoch [9/10], Batch [890/892], Loss: 0.8586\n",
            "[LSTM] Epoch [9/10] completed, Average Loss: 0.9071, Time: 281.15s\n",
            "[LSTM] Epoch [10/10], Batch [10/892], Loss: 0.8619\n",
            "[LSTM] Epoch [10/10], Batch [20/892], Loss: 0.8891\n",
            "[LSTM] Epoch [10/10], Batch [30/892], Loss: 0.9316\n",
            "[LSTM] Epoch [10/10], Batch [40/892], Loss: 0.8413\n",
            "[LSTM] Epoch [10/10], Batch [50/892], Loss: 0.8443\n",
            "[LSTM] Epoch [10/10], Batch [60/892], Loss: 0.8188\n",
            "[LSTM] Epoch [10/10], Batch [70/892], Loss: 0.8425\n",
            "[LSTM] Epoch [10/10], Batch [80/892], Loss: 0.8776\n",
            "[LSTM] Epoch [10/10], Batch [90/892], Loss: 0.8493\n",
            "[LSTM] Epoch [10/10], Batch [100/892], Loss: 1.0253\n",
            "[LSTM] Epoch [10/10], Batch [110/892], Loss: 0.9570\n",
            "[LSTM] Epoch [10/10], Batch [120/892], Loss: 0.8202\n",
            "[LSTM] Epoch [10/10], Batch [130/892], Loss: 0.8636\n",
            "[LSTM] Epoch [10/10], Batch [140/892], Loss: 0.8768\n",
            "[LSTM] Epoch [10/10], Batch [150/892], Loss: 0.8654\n",
            "[LSTM] Epoch [10/10], Batch [160/892], Loss: 0.8876\n",
            "[LSTM] Epoch [10/10], Batch [170/892], Loss: 0.8716\n",
            "[LSTM] Epoch [10/10], Batch [180/892], Loss: 0.9360\n",
            "[LSTM] Epoch [10/10], Batch [190/892], Loss: 0.8516\n",
            "[LSTM] Epoch [10/10], Batch [200/892], Loss: 0.8770\n",
            "[LSTM] Epoch [10/10], Batch [210/892], Loss: 0.8951\n",
            "[LSTM] Epoch [10/10], Batch [220/892], Loss: 0.8775\n",
            "[LSTM] Epoch [10/10], Batch [230/892], Loss: 0.9432\n",
            "[LSTM] Epoch [10/10], Batch [240/892], Loss: 0.9338\n",
            "[LSTM] Epoch [10/10], Batch [250/892], Loss: 0.8898\n",
            "[LSTM] Epoch [10/10], Batch [260/892], Loss: 0.8891\n",
            "[LSTM] Epoch [10/10], Batch [270/892], Loss: 0.8392\n",
            "[LSTM] Epoch [10/10], Batch [280/892], Loss: 0.8937\n",
            "[LSTM] Epoch [10/10], Batch [290/892], Loss: 0.8688\n",
            "[LSTM] Epoch [10/10], Batch [300/892], Loss: 0.8826\n",
            "[LSTM] Epoch [10/10], Batch [310/892], Loss: 0.8232\n",
            "[LSTM] Epoch [10/10], Batch [320/892], Loss: 0.8582\n",
            "[LSTM] Epoch [10/10], Batch [330/892], Loss: 0.8392\n",
            "[LSTM] Epoch [10/10], Batch [340/892], Loss: 0.8987\n",
            "[LSTM] Epoch [10/10], Batch [350/892], Loss: 0.8266\n",
            "[LSTM] Epoch [10/10], Batch [360/892], Loss: 0.8381\n",
            "[LSTM] Epoch [10/10], Batch [370/892], Loss: 0.8831\n",
            "[LSTM] Epoch [10/10], Batch [380/892], Loss: 0.9644\n",
            "[LSTM] Epoch [10/10], Batch [390/892], Loss: 0.8745\n",
            "[LSTM] Epoch [10/10], Batch [400/892], Loss: 0.8657\n",
            "[LSTM] Epoch [10/10], Batch [410/892], Loss: 0.9063\n",
            "[LSTM] Epoch [10/10], Batch [420/892], Loss: 0.8472\n",
            "[LSTM] Epoch [10/10], Batch [430/892], Loss: 0.8877\n",
            "[LSTM] Epoch [10/10], Batch [440/892], Loss: 0.8828\n",
            "[LSTM] Epoch [10/10], Batch [450/892], Loss: 0.8574\n",
            "[LSTM] Epoch [10/10], Batch [460/892], Loss: 0.7928\n",
            "[LSTM] Epoch [10/10], Batch [470/892], Loss: 0.8666\n",
            "[LSTM] Epoch [10/10], Batch [480/892], Loss: 0.8978\n",
            "[LSTM] Epoch [10/10], Batch [490/892], Loss: 0.8287\n",
            "[LSTM] Epoch [10/10], Batch [500/892], Loss: 0.8636\n",
            "[LSTM] Epoch [10/10], Batch [510/892], Loss: 0.8325\n",
            "[LSTM] Epoch [10/10], Batch [520/892], Loss: 0.8478\n",
            "[LSTM] Epoch [10/10], Batch [530/892], Loss: 0.8760\n",
            "[LSTM] Epoch [10/10], Batch [540/892], Loss: 0.8831\n",
            "[LSTM] Epoch [10/10], Batch [550/892], Loss: 0.8439\n",
            "[LSTM] Epoch [10/10], Batch [560/892], Loss: 0.8056\n",
            "[LSTM] Epoch [10/10], Batch [570/892], Loss: 0.8872\n",
            "[LSTM] Epoch [10/10], Batch [580/892], Loss: 0.9085\n",
            "[LSTM] Epoch [10/10], Batch [590/892], Loss: 0.8011\n",
            "[LSTM] Epoch [10/10], Batch [600/892], Loss: 0.8923\n",
            "[LSTM] Epoch [10/10], Batch [610/892], Loss: 0.8625\n",
            "[LSTM] Epoch [10/10], Batch [620/892], Loss: 0.9020\n",
            "[LSTM] Epoch [10/10], Batch [630/892], Loss: 0.8717\n",
            "[LSTM] Epoch [10/10], Batch [640/892], Loss: 0.9881\n",
            "[LSTM] Epoch [10/10], Batch [650/892], Loss: 0.9308\n",
            "[LSTM] Epoch [10/10], Batch [660/892], Loss: 0.8659\n",
            "[LSTM] Epoch [10/10], Batch [670/892], Loss: 0.8528\n",
            "[LSTM] Epoch [10/10], Batch [680/892], Loss: 0.8274\n",
            "[LSTM] Epoch [10/10], Batch [690/892], Loss: 0.8748\n",
            "[LSTM] Epoch [10/10], Batch [700/892], Loss: 0.8044\n",
            "[LSTM] Epoch [10/10], Batch [710/892], Loss: 0.8663\n",
            "[LSTM] Epoch [10/10], Batch [720/892], Loss: 0.9371\n",
            "[LSTM] Epoch [10/10], Batch [730/892], Loss: 0.8124\n",
            "[LSTM] Epoch [10/10], Batch [740/892], Loss: 0.9109\n",
            "[LSTM] Epoch [10/10], Batch [750/892], Loss: 0.9011\n",
            "[LSTM] Epoch [10/10], Batch [760/892], Loss: 0.8731\n",
            "[LSTM] Epoch [10/10], Batch [770/892], Loss: 0.7669\n",
            "[LSTM] Epoch [10/10], Batch [780/892], Loss: 0.8673\n",
            "[LSTM] Epoch [10/10], Batch [790/892], Loss: 0.9202\n",
            "[LSTM] Epoch [10/10], Batch [800/892], Loss: 0.8201\n",
            "[LSTM] Epoch [10/10], Batch [810/892], Loss: 0.8829\n",
            "[LSTM] Epoch [10/10], Batch [820/892], Loss: 0.8375\n",
            "[LSTM] Epoch [10/10], Batch [830/892], Loss: 0.9579\n",
            "[LSTM] Epoch [10/10], Batch [840/892], Loss: 0.8491\n",
            "[LSTM] Epoch [10/10], Batch [850/892], Loss: 0.9013\n",
            "[LSTM] Epoch [10/10], Batch [860/892], Loss: 0.8735\n",
            "[LSTM] Epoch [10/10], Batch [870/892], Loss: 0.8040\n",
            "[LSTM] Epoch [10/10], Batch [880/892], Loss: 0.8594\n",
            "[LSTM] Epoch [10/10], Batch [890/892], Loss: 0.8900\n",
            "[LSTM] Epoch [10/10] completed, Average Loss: 0.8715, Time: 280.49s\n"
          ]
        }
      ],
      "source": [
        "if dataloader is not None:\n",
        "    model_lstm = CNNRNNASR(\n",
        "        input_size=N_MELS,\n",
        "        vocab_size=vocab_size,\n",
        "        rnn_type='LSTM',\n",
        "        bidirectional=False,\n",
        "        conv_channels=[64, 128, 256],\n",
        "        kernel_sizes=[3, 3, 3],\n",
        "    ).to(device)\n",
        "    \n",
        "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "    optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    metrics_lstm = train_model(model_lstm, dataloader, criterion, optimizer_lstm, \n",
        "                                num_epochs=NUM_EPOCHS, model_name=\"LSTM\")\n",
        "    \n",
        "    results_lstm = compute_metrics(metrics_lstm, model_name=\"LSTM\")\n",
        "    all_results['LSTM'] = results_lstm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Entraînement GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[GRU] Epoch [1/10], Batch [10/892], Loss: 3.0414\n",
            "[GRU] Epoch [1/10], Batch [20/892], Loss: 2.8967\n",
            "[GRU] Epoch [1/10], Batch [30/892], Loss: 2.8796\n",
            "[GRU] Epoch [1/10], Batch [40/892], Loss: 2.8790\n",
            "[GRU] Epoch [1/10], Batch [50/892], Loss: 2.8664\n",
            "[GRU] Epoch [1/10], Batch [60/892], Loss: 2.8763\n",
            "[GRU] Epoch [1/10], Batch [70/892], Loss: 2.8664\n",
            "[GRU] Epoch [1/10], Batch [80/892], Loss: 2.8819\n",
            "[GRU] Epoch [1/10], Batch [90/892], Loss: 2.8532\n",
            "[GRU] Epoch [1/10], Batch [100/892], Loss: 2.8340\n",
            "[GRU] Epoch [1/10], Batch [110/892], Loss: 2.7861\n",
            "[GRU] Epoch [1/10], Batch [120/892], Loss: 2.7305\n",
            "[GRU] Epoch [1/10], Batch [130/892], Loss: 2.6966\n",
            "[GRU] Epoch [1/10], Batch [140/892], Loss: 2.6431\n",
            "[GRU] Epoch [1/10], Batch [150/892], Loss: 2.5765\n",
            "[GRU] Epoch [1/10], Batch [160/892], Loss: 2.5398\n",
            "[GRU] Epoch [1/10], Batch [170/892], Loss: 2.4276\n",
            "[GRU] Epoch [1/10], Batch [180/892], Loss: 2.3891\n",
            "[GRU] Epoch [1/10], Batch [190/892], Loss: 2.2862\n",
            "[GRU] Epoch [1/10], Batch [200/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [210/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [220/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [230/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [240/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [250/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [260/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [270/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [280/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [290/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [300/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [310/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [320/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [330/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [340/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [350/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [360/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [370/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [380/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [390/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [400/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [410/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [420/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [430/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [440/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [450/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [460/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [470/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [480/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [490/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [500/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [510/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [520/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [530/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [540/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [550/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [560/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [570/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [580/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [590/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [600/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [610/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [620/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [630/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [640/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [650/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [660/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [670/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [680/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [690/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [700/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [710/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [720/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [730/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [740/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [750/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [760/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [770/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [780/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [790/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [800/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [810/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [820/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [830/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [840/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [850/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [860/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [870/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [880/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10], Batch [890/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [1/10] completed, Average Loss: 2.9235, Time: 265.25s, Skipped batches: 698\n",
            "[GRU] Epoch [2/10], Batch [10/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [20/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [30/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [40/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [50/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [60/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [70/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [80/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [90/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [100/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [110/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [120/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [130/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [140/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [150/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [160/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [170/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [180/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [190/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [200/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [210/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [220/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [230/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [240/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [250/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [260/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [270/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [280/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [290/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [300/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [310/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [320/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [330/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [340/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [350/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [360/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [370/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [380/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [390/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [400/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [410/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [420/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [430/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [440/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [450/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [460/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [470/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [480/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [490/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [500/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [510/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [520/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [530/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [540/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [550/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [560/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [570/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [580/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [590/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [600/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [610/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [620/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [630/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [640/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [650/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [660/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [670/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [680/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [690/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [700/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [710/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [720/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [730/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [740/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [750/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [760/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [770/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [780/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [790/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [800/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [810/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [820/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [830/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [840/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [850/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [860/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [870/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [880/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10], Batch [890/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [2/10] completed, Average Loss: inf, Time: 196.51s, Skipped batches: 892\n",
            "[GRU] Epoch [3/10], Batch [10/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [20/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [30/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [40/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [50/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [60/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [70/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [80/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [90/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [100/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [110/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [120/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [130/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [140/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [150/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [160/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [170/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [180/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [190/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [200/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [210/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [220/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [230/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [240/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [250/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [260/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [270/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [280/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [290/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [300/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [310/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [320/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [330/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [340/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [350/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [360/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [370/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [380/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [390/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [400/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [410/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [420/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [430/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [440/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [450/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [460/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [470/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [480/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [490/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [500/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [510/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [520/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [530/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [540/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [550/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [560/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [570/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [580/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [590/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [600/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [610/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [620/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [630/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [640/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [650/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [660/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [670/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [680/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [690/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [700/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [710/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [720/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [730/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [740/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [750/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [760/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [770/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [780/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [790/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [800/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [810/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [820/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [830/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [840/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [850/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [860/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [870/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [880/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10], Batch [890/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [3/10] completed, Average Loss: inf, Time: 182.80s, Skipped batches: 892\n",
            "[GRU] Epoch [4/10], Batch [10/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [20/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [30/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [40/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [50/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [60/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [70/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [80/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [90/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [100/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [110/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [120/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [130/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [140/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [150/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [160/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [170/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [180/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [190/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [200/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [210/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [220/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [230/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [240/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [250/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [260/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [270/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [280/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [290/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [300/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [310/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [320/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [330/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [340/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [350/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [360/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [370/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [380/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [390/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [400/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [410/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [420/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [430/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [440/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [450/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [460/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [470/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [480/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [490/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [500/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [510/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [520/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [530/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [540/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [550/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [560/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [570/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [580/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [590/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [600/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [610/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [620/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [630/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [640/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [650/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [660/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [670/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [680/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [690/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [700/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [710/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [720/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [730/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [740/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [750/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [760/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [770/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [780/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [790/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [800/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [810/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [820/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [830/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [840/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [850/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [860/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [870/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [880/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10], Batch [890/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [4/10] completed, Average Loss: inf, Time: 182.42s, Skipped batches: 892\n",
            "[GRU] Epoch [5/10], Batch [10/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [20/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [30/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [40/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [50/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [60/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [70/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [80/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [90/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [100/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [110/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [120/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [130/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [140/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [150/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [160/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [170/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [180/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [190/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [200/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [210/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [220/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [230/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [240/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [250/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [260/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [270/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [280/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [290/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [300/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [310/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [320/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [330/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [340/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [350/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [360/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [370/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [380/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [390/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [400/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [410/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [420/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [430/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [440/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [450/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [460/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [470/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [480/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [490/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [500/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [510/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [520/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [530/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [540/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [550/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [560/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [570/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [580/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [590/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [600/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [610/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [620/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [630/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [640/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [650/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [660/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [670/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [680/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [690/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [700/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [710/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [720/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [730/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [740/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [750/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [760/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [770/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [780/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [790/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [800/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [810/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [820/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [830/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [840/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [850/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [860/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [870/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [880/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10], Batch [890/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [5/10] completed, Average Loss: inf, Time: 183.01s, Skipped batches: 892\n",
            "[GRU] Epoch [6/10], Batch [10/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [20/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [30/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [40/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [50/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [60/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [70/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [80/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [90/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [100/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [110/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [120/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [130/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [140/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [150/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [160/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [170/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [180/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [190/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [200/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [210/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [220/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [230/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [240/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [250/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [260/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [270/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [280/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [290/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [300/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [310/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [320/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [330/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [340/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [350/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [360/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [370/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [380/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [390/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [400/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [410/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [420/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [430/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [440/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [450/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [460/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [470/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [480/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [490/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [500/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [510/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [520/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [530/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [540/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [550/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [560/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [570/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [580/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [590/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [600/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [610/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [620/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [630/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [640/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [650/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [660/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [670/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [680/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [690/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [700/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [710/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [720/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [730/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [740/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [750/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [760/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [770/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [780/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [790/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [800/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [810/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [820/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [830/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [840/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [850/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [860/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [870/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [880/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10], Batch [890/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [6/10] completed, Average Loss: inf, Time: 181.52s, Skipped batches: 892\n",
            "[GRU] Epoch [7/10], Batch [10/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [20/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [30/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [40/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [50/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [60/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [70/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [80/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [90/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [100/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [110/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [120/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [130/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [140/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [150/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [160/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [170/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [180/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [190/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [200/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [210/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [220/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [230/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [240/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [250/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [260/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [270/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [280/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [290/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [300/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [310/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [320/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [330/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [340/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [350/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [360/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [370/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [380/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [390/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [400/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [410/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [420/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [430/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [440/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [450/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [460/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [470/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [480/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [490/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [500/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [510/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [520/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [530/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [540/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [550/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [560/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [570/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [580/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [590/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [600/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [610/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [620/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [630/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [640/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [650/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [660/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [670/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [680/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [690/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [700/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [710/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [720/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [730/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [740/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [750/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [760/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [770/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [780/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [790/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [800/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [810/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [820/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [830/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [840/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [850/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [860/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [870/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [880/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10], Batch [890/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [7/10] completed, Average Loss: inf, Time: 182.23s, Skipped batches: 892\n",
            "[GRU] Epoch [8/10], Batch [10/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [20/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [30/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [40/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [50/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [60/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [70/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [80/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [90/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [100/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [110/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [120/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [130/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [140/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [150/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [160/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [170/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [180/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [190/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [200/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [210/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [220/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [230/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [240/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [250/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [260/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [270/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [280/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [290/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [300/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [310/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [320/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [330/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [340/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [350/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [360/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [370/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [380/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [390/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [400/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [410/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [420/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [430/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [440/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [450/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [460/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [470/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [480/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [490/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [500/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [510/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [520/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [530/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [540/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [550/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [560/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [570/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [580/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [590/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [600/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [610/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [620/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [630/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [640/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [650/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [660/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [670/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [680/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [690/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [700/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [710/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [720/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [730/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [740/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [750/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [760/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [770/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [780/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [790/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [800/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [810/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [820/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [830/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [840/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [850/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [860/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [870/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [880/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10], Batch [890/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [8/10] completed, Average Loss: inf, Time: 180.99s, Skipped batches: 892\n",
            "[GRU] Epoch [9/10], Batch [10/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [20/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [30/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [40/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [50/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [60/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [70/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [80/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [90/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [100/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [110/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [120/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [130/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [140/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [150/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [160/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [170/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [180/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [190/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [200/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [210/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [220/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [230/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [240/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [250/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [260/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [270/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [280/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [290/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [300/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [310/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [320/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [330/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [340/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [350/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [360/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [370/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [380/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [390/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [400/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [410/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [420/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [430/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [440/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [450/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [460/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [470/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [480/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [490/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [500/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [510/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [520/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [530/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [540/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [550/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [560/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [570/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [580/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [590/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [600/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [610/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [620/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [630/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [640/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [650/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [660/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [670/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [680/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [690/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [700/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [710/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [720/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [730/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [740/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [750/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [760/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [770/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [780/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [790/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [800/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [810/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [820/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [830/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [840/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [850/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [860/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [870/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [880/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10], Batch [890/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [9/10] completed, Average Loss: inf, Time: 185.73s, Skipped batches: 892\n",
            "[GRU] Epoch [10/10], Batch [10/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [20/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [30/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [40/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [50/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [60/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [70/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [80/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [90/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [100/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [110/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [120/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [130/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [140/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [150/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [160/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [170/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [180/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [190/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [200/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [210/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [220/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [230/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [240/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [250/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [260/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [270/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [280/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [290/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [300/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [310/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [320/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [330/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [340/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [350/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [360/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [370/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [380/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [390/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [400/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [410/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [420/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [430/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [440/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [450/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [460/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [470/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [480/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [490/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [500/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [510/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [520/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [530/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [540/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [550/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [560/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [570/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [580/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [590/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [600/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [610/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [620/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [630/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [640/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [650/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [660/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [670/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [680/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [690/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [700/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [710/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [720/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [730/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [740/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [750/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [760/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [770/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [780/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [790/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [800/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [810/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [820/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [830/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [840/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [850/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [860/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [870/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [880/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10], Batch [890/892], Loss: nan (SKIPPED - invalid loss)\n",
            "[GRU] Epoch [10/10] completed, Average Loss: inf, Time: 184.39s, Skipped batches: 892\n"
          ]
        }
      ],
      "source": [
        "if dataloader is not None:\n",
        "    model_gru = CNNRNNASR(\n",
        "        input_size=N_MELS,\n",
        "        vocab_size=vocab_size,\n",
        "        rnn_type='GRU',\n",
        "        bidirectional=False,\n",
        "        conv_channels=[64, 128, 256],\n",
        "        kernel_sizes=[3, 3, 3],\n",
        "    ).to(device)\n",
        "    \n",
        "    optimizer_gru = optim.Adam(model_gru.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    metrics_gru = train_model(model_gru, dataloader, criterion, optimizer_gru, \n",
        "                              num_epochs=NUM_EPOCHS, model_name=\"GRU\")\n",
        "    \n",
        "    results_gru = compute_metrics(metrics_gru, model_name=\"GRU\")\n",
        "    all_results['GRU'] = results_gru"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Comparaison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle          Perte finale    Temps total (s)    Variance CTC    Écart-type CTC     Long. moy. séq.   \n",
            "LSTM            0.8715          2805.27            0.4317          0.6570             1269.6            \n",
            "GRU             inf             1924.85            2.1226          1.4569             1269.6            \n"
          ]
        }
      ],
      "source": [
        "headers = ['Modèle', 'Perte finale', 'Temps total (s)', \n",
        "           'Variance CTC', 'Écart-type CTC', 'Long. moy. séq.']\n",
        "print(f\"{headers[0]:<15} {headers[1]:<15} {headers[2]:<18} \"\n",
        "      f\"{headers[3]:<15} {headers[4]:<18} {headers[5]:<18}\")\n",
        "\n",
        "for model_name in ['LSTM', 'GRU']:\n",
        "    if model_name in all_results:\n",
        "        results = all_results[model_name]\n",
        "        \n",
        "        perte = results['performance']['perte_finale']\n",
        "        perte_str = f\"{perte:.4f}\" if perte is not None else \"N/A\"\n",
        "        \n",
        "        temps_total = results['vitesse']['temps_total']\n",
        "        temps_total_str = f\"{temps_total:.2f}\" if temps_total is not None else \"N/A\"\n",
        "        \n",
        "        variance = results['stabilite_ctc']['variance_globale']\n",
        "        variance_str = f\"{variance:.4f}\" if variance is not None else \"N/A\"\n",
        "        \n",
        "        std = results['stabilite_ctc']['ecart_type_global']\n",
        "        std_str = f\"{std:.4f}\" if std is not None else \"N/A\"\n",
        "        \n",
        "        long_moy = results['capacite_temporelle']['longueur_moyenne']\n",
        "        long_moy_str = f\"{long_moy:.1f}\" if long_moy is not None else \"N/A\"\n",
        "        \n",
        "        print(f\"{model_name:<15} {perte_str:<15} {temps_total_str:<18} \"\n",
        "              f\"{variance_str:<15} {std_str:<18} {long_moy_str:<18}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On remarque des temps d'entrainements beaucoup plus long qu'avec un simple MLP pour une loss similaire\\\n",
        "Pour la capacite temporelle, c'est quasi similaire entre GRU et LSTM\\\n",
        "Au niveau de la stabilite CTC, GRU semble etre plus stable ce qui veut dire qu'il a un entrainement plus previsible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partie 4 — Transformer ASR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture Transformer avec Self-Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        \n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerASR(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        vocab_size,\n",
        "        d_model=256,\n",
        "        nhead=8,\n",
        "        num_layers=4,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1,\n",
        "        max_len=5000,\n",
        "    ):\n",
        "        super(TransformerASR, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.input_projection = nn.Linear(input_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_len, dropout=dropout)\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation='relu'\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        \n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "        \n",
        "    def forward(self, x, mel_lengths=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        \n",
        "        x = self.input_projection(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        \n",
        "        src_key_padding_mask = None\n",
        "        if mel_lengths is not None:\n",
        "            max_len = x.size(1)\n",
        "            src_key_padding_mask = torch.arange(max_len, device=x.device).expand(\n",
        "                batch_size, max_len\n",
        "            ) >= mel_lengths.unsqueeze(1)\n",
        "        \n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        x = self.output_projection(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fonction d'entraînement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_with_masks(model, dataloader, criterion, optimizer, num_epochs=NUM_EPOCHS, model_name=\"Model\", max_grad_norm=1.0):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    batch_losses = []\n",
        "    sequence_lengths = []\n",
        "    epoch_times = []\n",
        "    \n",
        "    total_start_time = time.time()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "        skipped_batches = 0\n",
        "        \n",
        "        for batch_idx, (melspectrograms, texts, mel_lengths, text_lengths) in enumerate(dataloader):\n",
        "            melspectrograms = melspectrograms.to(device)\n",
        "            texts = texts.to(device)\n",
        "            mel_lengths = mel_lengths.to(device)\n",
        "            text_lengths = text_lengths.to(device)\n",
        "            \n",
        "            if (mel_lengths <= 0).any() or (text_lengths <= 0).any():\n",
        "                skipped_batches += 1\n",
        "                continue\n",
        "            \n",
        "            if (mel_lengths < text_lengths).any():\n",
        "                skipped_batches += 1\n",
        "                continue\n",
        "            \n",
        "            sequence_lengths.extend(mel_lengths.cpu().numpy().tolist())\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            try:\n",
        "                import inspect\n",
        "                sig = inspect.signature(model.forward)\n",
        "                if 'mel_lengths' in sig.parameters:\n",
        "                    logits = model(melspectrograms, mel_lengths=mel_lengths)\n",
        "                else:\n",
        "                    logits = model(melspectrograms)\n",
        "            except:\n",
        "                try:\n",
        "                    logits = model(melspectrograms, mel_lengths=mel_lengths)\n",
        "                except TypeError:\n",
        "                    logits = model(melspectrograms)\n",
        "            \n",
        "            log_probs = nn.functional.log_softmax(logits, dim=2)\n",
        "            log_probs = log_probs.transpose(0, 1)\n",
        "            \n",
        "            loss = criterion(log_probs, texts, mel_lengths, text_lengths)\n",
        "            \n",
        "            if not torch.isfinite(loss) or loss.item() > 1000:\n",
        "                skipped_batches += 1\n",
        "                if (batch_idx + 1) % 10 == 0:\n",
        "                    print(f'[{model_name}] Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f} (SKIPPED - invalid loss)')\n",
        "                continue\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            loss_value = loss.item()\n",
        "            epoch_loss += loss_value\n",
        "            num_batches += 1\n",
        "            batch_losses.append(loss_value)\n",
        "            \n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f'[{model_name}] Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss_value:.4f}')\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        epoch_times.append(epoch_time)\n",
        "        avg_loss = epoch_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f'[{model_name}] Epoch [{epoch+1}/{num_epochs}] completed, Average Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s, Skipped batches: {skipped_batches}')\n",
        "    \n",
        "    total_time = time.time() - total_start_time\n",
        "    \n",
        "    metrics = {\n",
        "        'train_losses': train_losses,\n",
        "        'batch_losses': batch_losses,\n",
        "        'sequence_lengths': sequence_lengths,\n",
        "        'epoch_times': epoch_times,\n",
        "        'total_time': total_time,\n",
        "        'final_loss': train_losses[-1] if train_losses else None,\n",
        "    }\n",
        "    \n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Entraînement Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Transformer] Epoch [1/1], Batch [10/892], Loss: 4.9424\n",
            "[Transformer] Epoch [1/1], Batch [20/892], Loss: 4.1104\n",
            "[Transformer] Epoch [1/1], Batch [30/892], Loss: 4.0325\n",
            "[Transformer] Epoch [1/1], Batch [40/892], Loss: 4.0148\n",
            "[Transformer] Epoch [1/1], Batch [50/892], Loss: 3.9338\n",
            "[Transformer] Epoch [1/1], Batch [60/892], Loss: 3.8390\n",
            "[Transformer] Epoch [1/1], Batch [70/892], Loss: 3.7779\n",
            "[Transformer] Epoch [1/1], Batch [80/892], Loss: 3.8060\n",
            "[Transformer] Epoch [1/1], Batch [90/892], Loss: 4.6524\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m criterion = nn.CTCLoss(blank=\u001b[32m0\u001b[39m, reduction=\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m, zero_infinity=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m optimizer_transformer = optim.Adam(model_transformer.parameters(), lr=LEARNING_RATE)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m metrics_transformer = \u001b[43mtrain_model_with_masks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_transformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTransformer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m results_transformer = compute_metrics(metrics_transformer, model_name=\u001b[33m\"\u001b[39m\u001b[33mTransformer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m all_results[\u001b[33m'\u001b[39m\u001b[33mTransformer\u001b[39m\u001b[33m'\u001b[39m] = results_transformer\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mtrain_model_with_masks\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, num_epochs, model_name, max_grad_norm)\u001b[39m\n\u001b[32m     64\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\u001b[32m     66\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m loss_value = \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m epoch_loss += loss_value\n\u001b[32m     70\u001b[39m num_batches += \u001b[32m1\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "if dataloader is not None:\n",
        "    model_transformer = TransformerASR(\n",
        "        input_size=N_MELS,\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=256,\n",
        "        nhead=8,\n",
        "        num_layers=4,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=DROPOUT,\n",
        "    ).to(device)\n",
        "    \n",
        "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "    optimizer_transformer = optim.Adam(model_transformer.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    metrics_transformer = train_model_with_masks(\n",
        "        model_transformer, \n",
        "        dataloader, \n",
        "        criterion, \n",
        "        optimizer_transformer,\n",
        "        num_epochs=1, \n",
        "        model_name=\"Transformer\"\n",
        "    )\n",
        "    \n",
        "    results_transformer = compute_metrics(metrics_transformer, model_name=\"Transformer\")\n",
        "    all_results['Transformer'] = results_transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Comparaison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle          Perte finale    Temps total (s)    Variance CTC    Écart-type CTC     Long. moy. séq.   \n",
            "LSTM            0.8715          2805.27            0.4317          0.6570             1269.6            \n",
            "GRU             inf             1924.85            2.1226          1.4569             1269.6            \n"
          ]
        }
      ],
      "source": [
        "headers = ['Modèle', 'Perte finale', 'Temps total (s)', \n",
        "           'Variance CTC', 'Écart-type CTC', 'Long. moy. séq.']\n",
        "print(f\"{headers[0]:<15} {headers[1]:<15} {headers[2]:<18} \"\n",
        "      f\"{headers[3]:<15} {headers[4]:<18} {headers[5]:<18}\")\n",
        "\n",
        "for model_name in ['LSTM', 'GRU', 'Transformer']:\n",
        "    if model_name in all_results:\n",
        "        results = all_results[model_name]\n",
        "        \n",
        "        perte = results['performance']['perte_finale']\n",
        "        perte_str = f\"{perte:.4f}\" if perte is not None else \"N/A\"\n",
        "        \n",
        "        temps_total = results['vitesse']['temps_total']\n",
        "        temps_total_str = f\"{temps_total:.2f}\" if temps_total is not None else \"N/A\"\n",
        "        \n",
        "        variance = results['stabilite_ctc']['variance_globale']\n",
        "        variance_str = f\"{variance:.4f}\" if variance is not None else \"N/A\"\n",
        "        \n",
        "        std = results['stabilite_ctc']['ecart_type_global']\n",
        "        std_str = f\"{std:.4f}\" if std is not None else \"N/A\"\n",
        "        \n",
        "        long_moy = results['capacite_temporelle']['longueur_moyenne']\n",
        "        long_moy_str = f\"{long_moy:.1f}\" if long_moy is not None else \"N/A\"\n",
        "        \n",
        "        print(f\"{model_name:<15} {perte_str:<15} {temps_total_str:<18} \"\n",
        "              f\"{variance_str:<15} {std_str:<18} {long_moy_str:<18}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "for model_name in ['LSTM', 'GRU', 'Transformer']:\n",
        "    if model_name in all_results:\n",
        "        losses = all_results[model_name]['performance']['pertes_par_epoch']\n",
        "        axes[0].plot(losses, label=model_name, linewidth=2, marker='o', markersize=4)\n",
        "\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Comparaison des Courbes de Convergence')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "model_names = []\n",
        "std_values = []\n",
        "for model_name in ['LSTM', 'GRU', 'Transformer']:\n",
        "    if model_name in all_results:\n",
        "        model_names.append(model_name)\n",
        "        std_values.append(all_results[model_name]['stabilite_ctc']['ecart_type_global'])\n",
        "\n",
        "axes[1].bar(model_names, std_values, alpha=0.7, color=['blue', 'green', 'red'])\n",
        "axes[1].set_ylabel('Écart-type CTC')\n",
        "axes[1].set_title('Stabilité de l\\'Entraînement (Écart-type)')\n",
        "axes[1].grid(True, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
